{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hsswkwk/turbo-chainsaw/blob/feature-add-image-classification/%E7%94%BB%E5%83%8F%E5%88%86%E9%A1%9E.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 画像分類（Image Classification）\n",
        "入力した画像情報を「形」や「模様」、「色」など特徴量に分解し、入力情報から対象物を判定できるように対応付けを行う"
      ],
      "metadata": {
        "id": "wm_anjAZ6g32"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN（Convolutional Neural Network）\n",
        "畳み込みニューラルネットワーク（CNNまたはConvNet）は、畳み込みを使用しているニューラルネットワークの総称"
      ],
      "metadata": {
        "id": "ZEG5KVeo6_gB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "\n",
        "# CIFAR-10データセットの読み込み\n",
        "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
        "\n",
        "# データの前処理（画素値を0-1の範囲に正規化）\n",
        "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
        "\n",
        "# CNNモデルの構築\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(10))\n",
        "\n",
        "# モデルのコンパイル\n",
        "# optimizer: 最適化アルゴリズム, loss: 損失関数, metrics: 評価指標\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# モデルの学習\n",
        "history = model.fit(train_images, train_labels, epochs=10,\n",
        "                    validation_data=(test_images, test_labels))\n",
        "\n",
        "# モデルの評価\n",
        "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
        "print(f'\\nTest accuracy: {test_acc}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGnPN0oe6ynR",
        "outputId": "a4094635-34de-4231-f8d6-70c8067a597a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 47ms/step - accuracy: 0.3569 - loss: 1.7421 - val_accuracy: 0.5606 - val_loss: 1.2570\n",
            "Epoch 2/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 46ms/step - accuracy: 0.5721 - loss: 1.1904 - val_accuracy: 0.5864 - val_loss: 1.1666\n",
            "Epoch 3/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 48ms/step - accuracy: 0.6427 - loss: 1.0116 - val_accuracy: 0.6461 - val_loss: 1.0078\n",
            "Epoch 4/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 46ms/step - accuracy: 0.6841 - loss: 0.9036 - val_accuracy: 0.6638 - val_loss: 0.9597\n",
            "Epoch 5/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 46ms/step - accuracy: 0.7105 - loss: 0.8265 - val_accuracy: 0.6734 - val_loss: 0.9533\n",
            "Epoch 6/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 47ms/step - accuracy: 0.7344 - loss: 0.7600 - val_accuracy: 0.6871 - val_loss: 0.9083\n",
            "Epoch 7/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 47ms/step - accuracy: 0.7581 - loss: 0.6959 - val_accuracy: 0.7047 - val_loss: 0.8732\n",
            "Epoch 8/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 46ms/step - accuracy: 0.7738 - loss: 0.6500 - val_accuracy: 0.7151 - val_loss: 0.8498\n",
            "Epoch 9/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 45ms/step - accuracy: 0.7876 - loss: 0.6114 - val_accuracy: 0.7176 - val_loss: 0.8442\n",
            "Epoch 10/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 46ms/step - accuracy: 0.7998 - loss: 0.5646 - val_accuracy: 0.7066 - val_loss: 0.9097\n",
            "313/313 - 3s - 11ms/step - accuracy: 0.7066 - loss: 0.9097\n",
            "\n",
            "Test accuracy: 0.70660001039505\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ResNet\n",
        "残差ニューラルネットワーク（ResNet）は、ウェイト層が層入力を参照して残差関数を学習する深層学習モデル"
      ],
      "metadata": {
        "id": "U2q4lT1R7bVZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models, applications\n",
        "\n",
        "# ResNet50モデルの読み込み（ImageNetで事前学習済み）\n",
        "# ImageNet:\n",
        "# 物体認識ソフトウェアの研究において、ソフトウェアの最適化や性能の評価等に\n",
        "# 用いるために設計された、大規模な画像データセット\n",
        "resnet_model = applications.ResNet50(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n",
        "\n",
        "# CIFAR-10データセットの読み込み\n",
        "(x_train, y_train), (x_test, y_test) = datasets.cifar10.load_data()\n",
        "\n",
        "# データの前処理\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes=10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes=10)\n",
        "\n",
        "# ResNet50の出力層をCIFAR-10用に変更\n",
        "# 事前学習済みのResNet50モデルに、CIFAR-10のクラス数に対応する全結合層を追加\n",
        "model = models.Sequential()\n",
        "model.add(resnet_model)\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "# モデルのコンパイル\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# モデルの学習\n",
        "# バッチサイズ: モデルが一度に処理するデータの量\n",
        "# エポック数: データセット全体を何回繰り返して学習するか\n",
        "model.fit(x_train, y_train, batch_size=256, epochs=10, validation_data=(x_test, y_test))\n",
        "\n",
        "# モデルの評価\n",
        "loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(f'Test Accuracy: {accuracy}')"
      ],
      "metadata": {
        "id": "Ogb5YYhJ7pRL",
        "outputId": "42fa15f5-3490-444a-d11b-6f695d0b6438",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 0us/step\n",
            "Epoch 1/10\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 259ms/step - accuracy: 0.5010 - loss: 1.6574 - val_accuracy: 0.1000 - val_loss: 5.7507\n",
            "Epoch 2/10\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 91ms/step - accuracy: 0.7555 - loss: 0.7280 - val_accuracy: 0.1000 - val_loss: 3.3375\n",
            "Epoch 3/10\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 94ms/step - accuracy: 0.8054 - loss: 0.6059 - val_accuracy: 0.1174 - val_loss: 3.8823\n",
            "Epoch 4/10\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 92ms/step - accuracy: 0.7335 - loss: 0.7874 - val_accuracy: 0.3736 - val_loss: 2.2272\n",
            "Epoch 5/10\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 92ms/step - accuracy: 0.8656 - loss: 0.3919 - val_accuracy: 0.7206 - val_loss: 0.8817\n",
            "Epoch 6/10\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 90ms/step - accuracy: 0.9092 - loss: 0.2681 - val_accuracy: 0.7605 - val_loss: 0.8308\n",
            "Epoch 7/10\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 90ms/step - accuracy: 0.9149 - loss: 0.2818 - val_accuracy: 0.1000 - val_loss: 92771.0625\n",
            "Epoch 8/10\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 93ms/step - accuracy: 0.5443 - loss: 1.3890 - val_accuracy: 0.3589 - val_loss: 2.3131\n",
            "Epoch 9/10\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 89ms/step - accuracy: 0.6669 - loss: 1.0756 - val_accuracy: 0.6138 - val_loss: 1.1355\n",
            "Epoch 10/10\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 93ms/step - accuracy: 0.7327 - loss: 0.8033 - val_accuracy: 0.1998 - val_loss: 13.9136\n",
            "Test Accuracy: 0.19979999959468842\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VGG16\n",
        "ImageNetと呼ばれる大規模画像データセットで学習された16層(畳み込み13層、フル結合3層)からなる畳み込みニューラルネットワーク(CNN)"
      ],
      "metadata": {
        "id": "WIGoeNWQ7qB_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models, applications\n",
        "\n",
        "# VGG16モデルの読み込み（ImageNetで事前学習済み）\n",
        "vgg16_model = applications.VGG16(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n",
        "\n",
        "# CIFAR-10データセットの読み込み\n",
        "(x_train, y_train), (x_test, y_test) = datasets.cifar10.load_data()\n",
        "\n",
        "# データの前処理\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes=10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes=10)\n",
        "\n",
        "# VGG16の出力層をCIFAR-10用に変更\n",
        "model = models.Sequential()\n",
        "model.add(vgg16_model)\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "\n",
        "# モデルのコンパイル\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# モデルの学習\n",
        "model.fit(x_train, y_train, batch_size=128, epochs=10, validation_data=(x_test, y_test))\n",
        "\n",
        "# モデルの評価\n",
        "loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(f'Test Accuracy: {accuracy}')"
      ],
      "metadata": {
        "id": "kUzzG6l076Mt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Efficient Net\n",
        "複合スケーリング手法を用いて、ネットワークの幅、深さ、解像度を同時に最適化し、より少ないパラメータ数で高い精度を持つCNN"
      ],
      "metadata": {
        "id": "BKgq6KRn760G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models, applications\n",
        "\n",
        "# EfficientNetB0モデルの読み込み（ImageNetで事前学習済み）\n",
        "efficientnet_model = applications.EfficientNetB0(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n",
        "\n",
        "# CIFAR-10データセットの読み込み\n",
        "(x_train, y_train), (x_test, y_test) = datasets.cifar10.load_data()\n",
        "\n",
        "# データの前処理\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes=10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes=10)\n",
        "\n",
        "# EfficientNetB0の出力層をCIFAR-10用に変更\n",
        "model = models.Sequential()\n",
        "model.add(efficientnet_model)\n",
        "model.add(layers.GlobalAveragePooling2D())  # GlobalAveragePooling2Dを追加\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "# モデルのコンパイル\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# モデルの学習\n",
        "model.fit(x_train, y_train, batch_size=128, epochs=10, validation_data=(x_test, y_test))\n",
        "\n",
        "# モデルの評価\n",
        "loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(f'Test Accuracy: {accuracy}')"
      ],
      "metadata": {
        "id": "NEpyzPWV8wAu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}