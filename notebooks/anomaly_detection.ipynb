{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNkR2aTwnZcVXpecpnIaO8j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hsswkwk/turbo-chainsaw/blob/feature-add-anomaly-detection/notebooks/anomaly_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 異常検知（Anomaly detection）\n",
        "標準的な状態や想定から逸脱しているデータや事象を特定する技術\n"
      ],
      "metadata": {
        "id": "PaP90VRh7wqm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "6SBjNkeLS_is"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 正規分布に従うデータの異常検知\n",
        "### 例\n",
        "製品の温度をセンサーで監視している場合の異常検知\n",
        "<br>\n",
        "<br>\n",
        "### 手法\n",
        "\n",
        "#### 3$\\sigma$法\n",
        "データが正規分布に従うと仮定し、平均値から標準偏差の3倍以上離れた値を異常値とみなす方法\n",
        "<br>\n",
        "#### マハラノビス・タグチ法\n",
        "データの各次元間の相関を考慮した距離尺度を用いて、平均値から離れた値を異常値とみなす方法\n",
        "<br>\n",
        "#### ホテリングの$T^2$法\n",
        "マハラノビス距離を拡張した手法で、データの平均値からのずれを検定統計量として用いて異常値を検出する方法\n",
        "<br>\n",
        "#### 密度比推定\n",
        "正常データと異常データの確率密度比を推定することで異常を検知する手法\n",
        "<br>"
      ],
      "metadata": {
        "id": "1dE3Gampih0k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.spatial.distance import mahalanobis\n",
        "from scipy.stats import f\n",
        "from sklearn.neighbors import KernelDensity\n",
        "\n",
        "\n",
        "### 3σ法\n",
        "# threshold: データの平均値から標準偏差の何倍離れたら異常値とみなすか\n",
        "def three_sigma_method(data, threshold=3):\n",
        "  mean = np.mean(data)\n",
        "  std = np.std(data)\n",
        "  lower_bound = mean - threshold * std\n",
        "  upper_bound = mean + threshold * std\n",
        "  if data.ndim == 1:\n",
        "    anomaly_indices = np.where((data < lower_bound) | (data > upper_bound))[0]\n",
        "  elif data.ndim >= 2:\n",
        "    anomaly_indices = np.where(np.any((data < lower_bound) | (data > upper_bound), axis=1))[0]\n",
        "  return data[anomaly_indices]\n",
        "\n",
        "### マハラノビス距離\n",
        "# マハラノビス距離: データの相関を考慮した距離尺度\n",
        "# threshold_percentile: マハラノビス距離の分布におけるパーセンタイル値を超える\n",
        "#                       距離を持つデータを異常値とみなす\n",
        "def mahalanobis_distance_method(data, threshold_percentile=99):\n",
        "  mean = np.mean(data, axis=0)\n",
        "  if data.ndim == 1:\n",
        "    std = np.std(data)\n",
        "\n",
        "    # 標準偏差が0の場合、ゼロ除算を避けるために微小な値を加算\n",
        "    if std == 0:\n",
        "      std = 1e-6\n",
        "\n",
        "    distances = np.abs((data - mean) / std)\n",
        "\n",
        "    # 異常値とみなす閾値を設定\n",
        "    threshold = np.percentile(distances, threshold_percentile)\n",
        "\n",
        "    # 閾値を超えるデータを異常値として検出\n",
        "    anomalies = data[distances > threshold]\n",
        "  elif data.ndim >= 2:\n",
        "    cov = np.cov(data, rowvar=False)\n",
        "    inv_cov = np.linalg.inv(cov)\n",
        "\n",
        "    distances = [mahalanobis(x, mean, inv_cov) for x in data]\n",
        "\n",
        "    # 異常値とみなす閾値を設定\n",
        "    threshold = np.percentile(distances, threshold_percentile)\n",
        "\n",
        "    # 閾値を超えるデータを異常値として検出\n",
        "    anomaly_indices = np.where(np.array(distances) > threshold)[0]\n",
        "    anomalies = data[anomaly_indices]\n",
        "  return anomalies\n",
        "\n",
        "### ホテリングのT^2法\n",
        "def hotelling_t2_method(data):\n",
        "  mean = np.mean(data, axis=0)\n",
        "  if data.ndim == 1:\n",
        "    var = np.var(data)\n",
        "    t2_values = [(x - mean)**2 / var for x in data]\n",
        "    # f.ppf: F分布のパーセント点関数\n",
        "    threshold = f.ppf(0.95, 1, len(data) - 1)  # 異常値とみなす閾値を設定\n",
        "    anomalies = [data[i] for i, t2 in enumerate(t2_values) if t2 > threshold]\n",
        "  elif data.ndim >= 2:\n",
        "    mean = np.mean(data, axis=0)\n",
        "    cov = np.cov(data, rowvar=False)\n",
        "    inv_cov = np.linalg.inv(cov)\n",
        "    t2_values = [mahalanobis(x, mean, inv_cov)**2 for x in data]\n",
        "    threshold = f.ppf(0.95, data.shape[1], data.shape[0] - data.shape[1] - 1)  # 異常値とみなす閾値を設定\n",
        "    anomaly_indices = np.where(np.array(t2_values) > threshold)[0]\n",
        "    anomalies = data[anomaly_indices]\n",
        "  return anomalies\n",
        "\n",
        "\n",
        "# 正常データ生成\n",
        "normal_data = np.random.normal(loc=25, scale=2, size=100)\n",
        "\n",
        "# 異常データ生成\n",
        "anomaly_data = np.random.normal(loc=25, scale=2, size=100)\n",
        "# 異常値の追加\n",
        "anomaly_indices = [10, 20, 30]  # 異常値のインデックス\n",
        "anomaly_values = [35, 15, 32]  # 異常値\n",
        "anomaly_data[anomaly_indices] = anomaly_values\n",
        "\n",
        "# 結果の出力\n",
        "anomalies = three_sigma_method(anomaly_data)\n",
        "print(\"================== 3σ法 =================\")\n",
        "print(\"異常値:\", anomalies)\n",
        "print(\"\\n\")\n",
        "\n",
        "anomalies = mahalanobis_distance_method(anomaly_data)\n",
        "print(\"===== マハラノビス距離による異常検知 =====\")\n",
        "print(\"異常値:\", anomalies)\n",
        "print(\"\\n\")\n",
        "\n",
        "anomalies = hotelling_t2_method(anomaly_data)\n",
        "print(\"===== ホテリングのT^2法による異常検知 ====\")\n",
        "print(\"異常値:\", anomalies)\n",
        "print(\"\\n\")\n"
      ],
      "metadata": {
        "id": "VydvSi0Ejy_Z",
        "outputId": "ef7768f0-5b10-4201-c565-b3b96fd627a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================== 3σ法 =================\n",
            "異常値: [35. 15.]\n",
            "\n",
            "\n",
            "===== マハラノビス距離による異常検知 =====\n",
            "異常値: [35.]\n",
            "\n",
            "\n",
            "===== ホテリングのT^2法による異常検知 ====\n",
            "異常値: [np.float64(35.0), np.float64(15.0), np.float64(32.0)]\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 非正規データの異常検知\n",
        "### 例\n",
        "ウェブサイトへのアクセス数の異常検知\n",
        "<br>\n",
        "<br>\n",
        "### 手法\n",
        "\n",
        "#### ガンマ分布の当てはめ\n",
        "データがガンマ分布に従うと仮定し、その分布から大きく外れた値を異常値とみなす方法。データがガンマ分布に従う場合、高い精度で異常を検出できるが、従わない場合、精度が低下する。\n",
        "<br>\n",
        "\n",
        "#### カイ二乗分布への当てはめ\n",
        "データがカイ二乗分布に従うと仮定し、その分布から大きく外れた値を異常値とみなす方法。主に、特徴量の値が正の値を取り、歪んだ分布をしている場合に有効。<br>\n",
        "<br>\n",
        "\n",
        "#### $k$近傍法\n",
        "各データポイントからk番目に近いデータポイントまでの距離を計算し、距離が大きいデータポイントを異常値と判定するアルゴリズム。$k$はデータセットのサイズの平方根よりも小さい奇数にすると良い。\n",
        "<br>\n",
        "\n",
        "#### $k$ means法\n",
        "データを複数のクラスタに分割し、どのクラスタにも属さないデータポイントを異常値と判定するアルゴリズム。$k$を推定する手法としてエルボー法やシルエット分析などがある。\n",
        "<br>\n",
        "\n",
        "#### 混合ガウス分布モデル（Gaussian Mixture Model, GMM）\n",
        "データが複数のガウス分布（正規分布）の混合で表現できると仮定し、低確率なデータ点を異常値とみなす方法。\n",
        "<br>\n",
        "\n",
        "#### One-Class SVM\n",
        "正常データのみを用いて、正常データの領域を学習するアルゴリズム。 学習した領域から外れたデータは異常値と判定する。\n",
        "<br>\n",
        "\n",
        "#### [密度比推定](#scrollTo=1dE3Gampih0k&line=1&uniqifier=1)\n",
        "<br>\n",
        "\n",
        "#### 孤立フォレスト（Isolation Forest）\n",
        "データポイントをランダムに分割していくことで、異常値を孤立させるアルゴリズム。\n",
        "<br>\n",
        "\n",
        "#### Local Outlier Factor (LOF)\n",
        "データポイントの局所的な密度を計算し、密度が低いデータポイントを異常値と判定するアルゴリズム。\n",
        "<br>\n",
        "\n"
      ],
      "metadata": {
        "id": "FMOqmjNLiot5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.stats as stats\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "\n",
        "### ガンマ分布の当てはめ\n",
        "def fitting_gamma_distribution(data):\n",
        "  # データにガンマ分布を当てはめる\n",
        "  shape, loc, scale = stats.gamma.fit(data)\n",
        "\n",
        "  # 異常度を計算する\n",
        "  anomaly_scores = 1 / stats.gamma.pdf(data, shape, loc, scale)\n",
        "\n",
        "  # 閾値を設定する\n",
        "  threshold = np.mean(anomaly_scores) + 3 * np.std(anomaly_scores)\n",
        "\n",
        "  # 異常値を検出する\n",
        "  anomalies = data[anomaly_scores > threshold]\n",
        "\n",
        "  return anomalies\n",
        "\n",
        "\n",
        "### k近傍法\n",
        "def k_nearest_neighbors(data, k, threshold_percentile):\n",
        "  data = data.reshape(-1, 1)\n",
        "\n",
        "  # k近傍の計算\n",
        "  knn = NearestNeighbors(n_neighbors=k)\n",
        "  knn.fit(data)\n",
        "  distances, indices = knn.kneighbors(data)\n",
        "\n",
        "  # 異常度の算出\n",
        "  anomaly_score = distances[:, -1]\n",
        "  threshold = np.percentile(anomaly_score, threshold_percentile)\n",
        "\n",
        "  # 異常値の判定\n",
        "  anomaly_index = np.where(anomaly_score > threshold)\n",
        "\n",
        "  # 異常値を検出する\n",
        "  anomalies = data[anomaly_index]\n",
        "\n",
        "  return anomalies\n",
        "\n",
        "\n",
        "### 孤立フォレスト\n",
        "def isolation_forest(data, threshold_percentile):\n",
        "  data = data.reshape(-1, 1)\n",
        "\n",
        "  # モデルの学習\n",
        "  model = IsolationForest()\n",
        "  model.fit(data)\n",
        "\n",
        "  # 異常度の算出\n",
        "  anomaly_score = model.decision_function(data)\n",
        "  threshold = np.percentile(anomaly_score, threshold_percentile)\n",
        "\n",
        "  # 異常値の判定\n",
        "  anomaly_index = np.where(anomaly_score < threshold)\n",
        "\n",
        "  # 異常値を検出する\n",
        "  anomalies = data[anomaly_index]\n",
        "\n",
        "  return anomalies\n",
        "\n",
        "\n",
        "def power_law_distribution(alpha, xmin, xmax, size):\n",
        "    \"\"\"べき乗則に従う乱数を生成する関数\n",
        "\n",
        "    Args:\n",
        "        alpha (float): べき乗則の指数\n",
        "        xmin (float): 最小値\n",
        "        xmax (float): 最大値\n",
        "        size (int): 生成する乱数の数\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: べき乗則に従う乱数\n",
        "    \"\"\"\n",
        "\n",
        "    u = np.random.uniform(size=size)\n",
        "    return (xmin ** (1 - alpha) + (xmax ** (1 - alpha) - xmin ** (1 - alpha)) * u) ** (1 / (1 - alpha))\n",
        "\n",
        "\n",
        "# 正常データの生成\n",
        "normal_data = power_law_distribution(alpha=2, xmin=1, xmax=100, size=1000)\n",
        "\n",
        "# 異常データの生成\n",
        "outliers = np.random.uniform(low=2*normal_data.max(), high=3*normal_data.max(), size=10)\n",
        "anomaly_data = np.concatenate([normal_data, outliers])\n",
        "\n",
        "all_data = np.concatenate([normal_data, anomaly_data])\n",
        "\n",
        "#### ガンマ分布の当てはめ\n",
        "anomalies = fitting_gamma_distribution(all_data)\n",
        "print(\"============ ガンマ分布の当てはめ ==========\")\n",
        "print(\"異常値:\", anomalies)\n",
        "print(\"\\n\")\n",
        "\n",
        "#### k近傍法\n",
        "anomalies = k_nearest_neighbors(all_data, 5, 99.5)\n",
        "print(\"================== k近傍法 =================\")\n",
        "print(\"異常値:\", anomalies)\n",
        "print(\"\\n\")\n",
        "\n",
        "#### 孤立フォレスト (Isolation Forest)\n",
        "anomalies = isolation_forest(all_data, 0.5)\n",
        "print(\"=============== 孤立フォレスト ==============\")\n",
        "print(\"異常値:\", anomalies)\n",
        "print(\"\\n\")\n"
      ],
      "metadata": {
        "id": "aDeywrg2lfYP",
        "outputId": "d3c79b52-2bbc-45d2-c8b9-f86e10b71876",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============ ガンマ分布の当てはめ ==========\n",
            "異常値: [264.16588693 233.77545697 256.14674318 237.30077276 248.1279328 ]\n",
            "\n",
            "\n",
            "================== k近傍法 =================\n",
            "異常値: [[264.16588693]\n",
            " [233.77545697]\n",
            " [220.09087023]\n",
            " [256.14674318]\n",
            " [209.84534072]\n",
            " [192.14706073]\n",
            " [220.01920791]\n",
            " [237.30077276]\n",
            " [191.7240541 ]\n",
            " [248.1279328 ]]\n",
            "\n",
            "\n",
            "=============== 孤立フォレスト ==============\n",
            "異常値: [[264.16588693]\n",
            " [233.77545697]\n",
            " [220.09087023]\n",
            " [256.14674318]\n",
            " [209.84534072]\n",
            " [192.14706073]\n",
            " [220.01920791]\n",
            " [237.30077276]\n",
            " [191.7240541 ]\n",
            " [248.1279328 ]]\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 不要次元のある次元データの異常検知\n",
        "### 例\n",
        "顧客の購買データを用いた異常検知\n",
        "<br>\n",
        "<br>\n",
        "### 次元削減を用いる手法\n",
        "\n",
        "#### 主成分分析（PCA）\n",
        "データの分散を最大化するように新しい軸を定義し、その軸にデータを射影することで次元を削減する手法。異常なデータ点は、主成分空間において、正常なデータ点から離れた位置に配置される傾向がある。\n",
        "<br>\n",
        "\n",
        "#### 確率的主成分分析（Probabilistic PCA）\n",
        "PCAを確率モデルとして拡張した手法。データにノイズが含まれる場合に、よりロバストな結果が得られる。（＝データにノイズや外れ値が含まれていても、分析結果が大きく影響を受けにくい）\n",
        "<br>\n",
        "\n",
        "#### カーネル主成分分析（Kernel PCA）\n",
        "非線形な関係を持つデータに対して、カーネル関数を使用して高次元空間に写像し、その空間でPCAを行うことで次元を削減する手法。\n",
        "<br>\n",
        "\n",
        "#### 因子分析（Factor Analysis）\n",
        "観測変数の背後にある潜在変数を推定し、それらを用いて次元を削減する手法。\n",
        "<br>\n",
        "\n",
        "#### 独立成分分析（ICA）\n",
        "観測変数を、統計的に独立な成分に分解することで次元を削減する手法。\n",
        "<br>\n",
        "\n",
        "#### t-distributed Stochastic Neighbor Embedding (t-SNE)\n",
        "高次元データを低次元空間に埋め込む際に、データの局所的な構造を保持する様に設計された手法。異常検知では、正常なデータ点と異常なデータ点が、低次元空間において明確に分離されることが期待する。\n",
        "<br>\n",
        "\n",
        "#### Uniform Manifold Approximation and Projection (UMAP)\n",
        "高次元データを低次元空間に埋め込む際に、データのトポロジカルな構造を保持する様に設計された手法です。t-SNEと同様に、異常検知では、正常なデータ点と異常なデータ点が、低次元空間において明確に分離されることを期待する。\n",
        "<br>\n",
        "\n",
        "### 特徴量選択を用いる手法\n",
        "#### フィルター法\n",
        "各特徴量と目的変数の間の相関や相互情報量などを用いて、重要度の低い特徴量を除去する手法。\n",
        "<br>\n",
        "\n",
        "#### ラッパー法\n",
        "特徴量のサブセットを選択し、そのサブセットを用いて学習したモデルの性能を評価することで、最適な特徴量を選択する手法。\n",
        "<br>\n",
        "\n",
        "#### 埋め込み法\n",
        "モデルの学習過程で特徴量選択を行う手法。LASSO回帰や決定木などが用いられる。\n",
        "<br>\n",
        "\n",
        "### その他の手法\n",
        "#### [One-Class SVM](#scrollTo=FMOqmjNLiot5&line=33&uniqifier=1)\n",
        "<br>\n",
        "\n",
        "#### [孤立フォレスト（Isolation Forest）](#scrollTo=FMOqmjNLiot5&line=33&uniqifier=1)\n",
        "<br>\n",
        "\n",
        "#### [Local Outlier Factor (LOF)](#scrollTo=FMOqmjNLiot5&line=33&uniqifier=1)\n",
        "<br>\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "V4fWrW7RivcU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "### One-Class SVM\n",
        "def one_class_svm(data, threshold_percentile):\n",
        "  data = data.reshape(-1, 1)\n",
        "\n",
        "  scaler = StandardScaler()\n",
        "  scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "  # モデルの学習\n",
        "  #param_grid = {'nu': [0.1, 0.2, 0.3], 'gamma': ['scale', 'auto']}\n",
        "  model = OneClassSVM(nu=0.1, gamma='scale')\n",
        "  model.fit(scaled_data)\n",
        "\n",
        "  # 異常度の算出\n",
        "  anomaly_score = model.decision_function(scaled_data)\n",
        "  threshold = np.percentile(anomaly_score, threshold_percentile)\n",
        "\n",
        "  # 異常値の判定\n",
        "  anomaly_index = np.where(anomaly_score < threshold)\n",
        "\n",
        "  # 異常値を検出する\n",
        "  anomalies = data[anomaly_index]\n",
        "\n",
        "  return anomalies\n",
        "\n",
        "\n",
        "### Local Outlier Factor (LOF)\n",
        "def local_outlier_factor(data):\n",
        "  data = data.reshape(-1, 1)\n",
        "\n",
        "  # モデルの学習\n",
        "  model = LocalOutlierFactor()\n",
        "  anomaly_score = model.fit_predict(data)\n",
        "\n",
        "  # 異常値の判定\n",
        "  anomaly_index = np.where(anomaly_score == -1)\n",
        "\n",
        "  # 異常値を検出する\n",
        "  anomalies = data[anomaly_index]\n",
        "\n",
        "  return anomalies\n",
        "\n",
        "\n",
        "#### 正常データの生成\n",
        "# 顧客ID、年齢、性別、購入金額、購入頻度、不要な次元（ランダムな数値）を生成\n",
        "num_customers = 100  # 顧客数\n",
        "customer_ids = np.arange(1, num_customers + 1)\n",
        "ages = np.random.randint(20, 60, size=num_customers)\n",
        "genders = np.random.choice(['Male', 'Female'], size=num_customers)\n",
        "purchase_amounts = np.random.randint(100, 10000, size=num_customers)\n",
        "purchase_frequencies = np.random.randint(1, 10, size=num_customers)\n",
        "unnecessary_dimension = np.random.rand(num_customers)\n",
        "\n",
        "# データフレームを作成\n",
        "normal_data = pd.DataFrame({\n",
        "  'CustomerID': customer_ids,\n",
        "  'Age': ages,\n",
        "  'Gender': genders,\n",
        "  'PurchaseAmount': purchase_amounts,\n",
        "  'PurchaseFrequency': purchase_frequencies,\n",
        "  'UnnecessaryDimension': unnecessary_dimension  # 不要な次元\n",
        "})\n",
        "\n",
        "#### 異常データの生成\n",
        "anomaly_data = normal_data.copy()\n",
        "\n",
        "# 異常値を挿入するインデックスをランダムに選択\n",
        "# 例：顧客の10%を異常値とする\n",
        "anomaly_indices = np.random.choice(anomaly_data.index, size=int(num_customers * 0.1), replace=False)\n",
        "\n",
        "# 選択したインデックスのデータに異常値を代入\n",
        "# 例：PurchaseAmountを極端に大きくする\n",
        "anomaly_data.loc[anomaly_indices, 'PurchaseAmount'] = anomaly_data.loc[anomaly_indices, 'PurchaseAmount'] * np.random.uniform(100, 200, size=len(anomaly_indices)).astype(np.int64)\n",
        "\n",
        "all_data = pd.concat([normal_data, anomaly_data], ignore_index=True)\n",
        "numerical_data = all_data[['Age', 'PurchaseAmount', 'PurchaseFrequency', 'UnnecessaryDimension']]\n",
        "\n",
        "#### One-Class SVM\n",
        "anomalies = one_class_svm(numerical_data.values, 1)\n",
        "print(\"=============== One-Class SVM ==============\")\n",
        "print(\"異常値:\", anomalies)\n",
        "print(\"\\n\")\n",
        "\n",
        "# #### 孤立フォレスト（Isolation Forest）\n",
        "anomalies = isolation_forest(numerical_data['PurchaseAmount'].values, 5)\n",
        "print(\"=============== 孤立フォレスト ==============\")\n",
        "print(\"異常値:\", anomalies)\n",
        "print(\"\\n\")\n",
        "\n",
        "# #### Local Outlier Factor (LOF)\n",
        "anomalies = local_outlier_factor(numerical_data['PurchaseAmount'].values)\n",
        "print(\"============ Local Outlier Factor ===========\")\n",
        "print(\"異常値:\", anomalies)\n",
        "print(\"\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "8QWDCFA6aqkS",
        "outputId": "f9f64ce4-0e8c-4d67-bc1c-3a35cb5fa890",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=============== One-Class SVM ==============\n",
            "異常値: [[ 731777.]\n",
            " [ 854700.]\n",
            " [1299750.]\n",
            " [ 474929.]\n",
            " [ 766764.]\n",
            " [1310736.]\n",
            " [1186938.]\n",
            " [1131477.]]\n",
            "\n",
            "\n",
            "=============== 孤立フォレスト ==============\n",
            "異常値: [[  77715]\n",
            " [ 731777]\n",
            " [1214640]\n",
            " [ 854700]\n",
            " [1299750]\n",
            " [ 474929]\n",
            " [ 766764]\n",
            " [1310736]\n",
            " [1186938]\n",
            " [1131477]]\n",
            "\n",
            "\n",
            "============ Local Outlier Factor ===========\n",
            "異常値: [[  77715]\n",
            " [ 731777]\n",
            " [1214640]\n",
            " [ 854700]\n",
            " [1299750]\n",
            " [ 474929]\n",
            " [ 766764]\n",
            " [1310736]\n",
            " [1186938]\n",
            " [1131477]]\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 入出力関係のあるデータの異常検知\n",
        "### 例\n",
        "製造工程における品質管理（入力データ：温度、圧力、処理時間など、出力データ：寸法、重量、強度など）\n",
        "<br>\n",
        "\n",
        "### 手法\n",
        "\n",
        "#### 線形回帰モデル\n",
        "正常データを用いて、入力と出力の関係を回帰モデルで学習し、新しいデータが入力された際に、回帰モデルの予測値と実際の出力値の差が大きい場合、異常と判定する。\n",
        "<br>\n",
        "\n",
        "#### リッジ回帰\n",
        "線形回帰モデルの一種で、過学習を防ぐために正則化項を導入したもの。\n",
        "<br>\n",
        "\n",
        "#### ガウス過程回帰\n",
        "非線形な回帰問題を解くための機械学習手法。観測データから入力と出力の関係を確率分布として学習する。この確率分布はガウス過程（任意の有限個の点における確率変数の集合が、常に多変量正規分布に従うような確率過程）で表現され、予測値だけでなく、予測値の不確実性も推定することができる。\n",
        "<br>\n",
        "\n",
        "#### 状態空間モデル\n",
        "入出力関係を状態空間モデルで表現し、観測値と予測値の差を異常度として用いる方法。\n",
        "<br>\n",
        "\n",
        "#### [One-Class SVM](#scrollTo=FMOqmjNLiot5&line=33&uniqifier=1)"
      ],
      "metadata": {
        "id": "NWg4xZztixiP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "\n",
        "### 線形回帰\n",
        "def linear_regression(input_data, output_data):\n",
        "  # 線形回帰モデルを学習\n",
        "  model = LinearRegression()\n",
        "  model.fit(input_data, output_data)\n",
        "\n",
        "  # 予測値と実際の出力値の差を計算\n",
        "  predicted_output = model.predict(input_data)\n",
        "  residuals = output_data - predicted_output\n",
        "\n",
        "  # 異常値を判定 (例：残差が大きい値を異常値とする)\n",
        "  threshold = np.mean(residuals) + 2 * np.std(residuals)  # 閾値は調整が必要\n",
        "  anomalies = input_data[np.abs(residuals) > threshold]\n",
        "\n",
        "  return anomalies\n",
        "\n",
        "\n",
        "### リッジ回帰\n",
        "def ridge_regression(input_data, output_data):\n",
        "  # リッジ回帰モデルを学習\n",
        "  model = Ridge(alpha=1.0)  # alphaは正則化の強さを指定\n",
        "  model.fit(input_data, output_data)\n",
        "\n",
        "  # 予測値と実際の出力値の差を計算\n",
        "  predicted_output = model.predict(input_data)\n",
        "  residuals = output_data - predicted_output\n",
        "\n",
        "  # 異常値を判定 (例：残差が大きい値を異常値とする)\n",
        "  threshold = np.mean(residuals) + 2 * np.std(residuals)  # 閾値は調整が必要\n",
        "  anomalies = input_data[np.abs(residuals) > threshold]\n",
        "\n",
        "  return anomalies\n",
        "\n",
        "\n",
        "def one_class_svm_io(input_data, output_data, threshold_percentile):\n",
        "  # 入力データと出力データを結合\n",
        "  data = np.concatenate([input_data, output_data.reshape(-1, 1)], axis=1)\n",
        "\n",
        "  # データをスケーリング\n",
        "  scaler = StandardScaler()\n",
        "  scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "  # One-Class SVMモデルを学習\n",
        "  model = OneClassSVM(nu=0.1, gamma='scale') # nu, gammaは必要に応じて調整\n",
        "  model.fit(scaled_data)\n",
        "\n",
        "  # 異常度を算出\n",
        "  anomaly_score = model.decision_function(scaled_data)\n",
        "  threshold = np.percentile(anomaly_score, threshold_percentile)\n",
        "\n",
        "  # 異常値の判定\n",
        "  anomaly_index = np.where(anomaly_score < threshold)\n",
        "\n",
        "  # 異常があったinput_dataを抽出\n",
        "  anomalies = input_data[anomaly_index]\n",
        "\n",
        "  return anomalies\n",
        "\n",
        "\n",
        "# 正常データの生成\n",
        "num_samples = 100\n",
        "input_data = np.random.rand(num_samples, 2) * 10  # 0-10の範囲の乱数を生成\n",
        "output_data = 2 * input_data[:, 0] + 3 * input_data[:, 1] + np.random.randn(num_samples) * 2 # y = 2x1 + 3x2 + noise\n",
        "\n",
        "# 異常データの生成\n",
        "num_anomalies = 10\n",
        "anomaly_input_data = np.random.rand(num_anomalies, 2) * 10 + 10  # 10-20の範囲の乱数を生成\n",
        "anomaly_output_data = np.random.rand(num_anomalies) * 10  # 0-10の範囲の乱数を生成\n",
        "\n",
        "# データの結合\n",
        "input_data = np.concatenate([input_data, anomaly_input_data])\n",
        "output_data = np.concatenate([output_data, anomaly_output_data])\n",
        "\n",
        "\n",
        "#### 線形回帰\n",
        "anomalies = linear_regression(input_data, output_data)\n",
        "print(\"================== 線形回帰 ================\")\n",
        "print(\"異常値:\", anomalies)\n",
        "print(\"\\n\")\n",
        "\n",
        "#### リッジ回帰\n",
        "anomalies = ridge_regression(input_data, output_data)\n",
        "print(\"================= リッジ回帰 ================\")\n",
        "print(\"異常値:\", anomalies)\n",
        "print(\"\\n\")\n",
        "\n",
        "#### One-Class SVM\n",
        "anomalies = one_class_svm_io(input_data, output_data, 1)\n",
        "print(\"=============== One-Class SVM ==============\")\n",
        "print(\"異常値:\", anomalies)\n",
        "print(\"\\n\")"
      ],
      "metadata": {
        "id": "l6EqJ9RmO4q6",
        "outputId": "f6dffb46-f7b4-403e-bcbd-67e169d7322c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================== 線形回帰 ================\n",
            "異常値: [[ 9.77397504  7.61991305]\n",
            " [ 8.49115644  8.16077453]\n",
            " [ 9.4377397   8.54573071]\n",
            " [14.28544412 18.1103912 ]]\n",
            "\n",
            "\n",
            "================= リッジ回帰 ================\n",
            "異常値: [[ 9.77397504  7.61991305]\n",
            " [ 8.49115644  8.16077453]\n",
            " [ 9.4377397   8.54573071]\n",
            " [14.28544412 18.1103912 ]]\n",
            "\n",
            "\n",
            "=============== One-Class SVM ==============\n",
            "異常値: [[0.23502969 9.23246612]\n",
            " [9.97443361 1.38012123]]\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 時系列データの異常検知\n",
        "### 例\n",
        "サーバーのCPU使用率の異常検知\n",
        "\n",
        "### 手法\n",
        "\n",
        "#### 自己回帰モデル\n",
        "時系列データの過去の値を用いて、未来の値を予測するモデル。予測値と実測値の差が大きい場合に異常と判定する。\n",
        "<br>\n",
        "\n",
        "#### 移動平均法\n",
        "一定期間のデータの平均値を計算し、その平均値から大きく外れた値を異常とみなす手法。\n",
        "<br>\n",
        "\n",
        "#### 指数平滑法\n",
        "直近のデータに大きな重みを置くことで、移動平均法よりも長期的な変動に対応できる手法。\n",
        "<br>\n",
        "\n",
        "#### ARIMAモデル\n",
        "時系列データの自己相関を考慮したモデルで、予測値と実測値の差が大きい場合、異常とみなす。\n",
        "<br>\n",
        "\n",
        "#### [$k$近傍法](#scrollTo=FMOqmjNLiot5&line=7&uniqifier=1)\n",
        "<br>\n",
        "\n",
        "#### サポートベクターマシン (SVM)\n",
        "正常データと異常データを分離する超平面を学習する手法。[One-Class SVM](#scrollTo=FMOqmjNLiot5&line=33&uniqifier=1)は正常データのみを用いる。\n",
        "<br>\n",
        "\n",
        "#### [孤立フォレスト (Isolation Forest)](#scrollTo=FMOqmjNLiot5&line=7&uniqifier=1)\n",
        "<br>\n",
        "\n",
        "#### LSTM (Long Short-Term Memory)\n",
        "時系列データや自然言語処理などのシーケンシャルデータの分析に広く用いられる深層学習モデルでRNN (Recurrent Neural Network) の一種。過去の情報を記憶し、現在の出力に反映させることができる。\n",
        "\n",
        "##### セル状態\n",
        "過去の情報を記憶するためのユニット。時系列データの処理が進むにつれて、過去の情報が蓄積されていく。\n",
        "\n",
        "##### ゲート機構\n",
        "セル状態への情報の入力、出力、保持を制御する仕組み。現在の入力情報をセル状態にどれだけ反映させるかを制御する入力ゲート、セル状態の情報を現在の出力にどれだけ反映させるかを制御する出力ゲート、セル状態の情報をどれだけ保持するかを制御する忘却ゲートからなる。\n",
        "<br>\n",
        "\n",
        "#### 特異スペクトル変換法\n",
        "時系列データの長期的な依存関係を学習できるニューラルネットワーク。正常な時系列データを学習し、学習データと大きく異なるパターンを持つデータを異常とみなす。\n",
        "<br>\n",
        "\n",
        "#### [状態空間モデル](#scrollTo=NWg4xZztixiP&line=23&uniqifier=1)\n",
        "\n",
        "#### Autoencoder\n",
        "データを低次元空間に圧縮し、復元するニューラルネットワーク。正常な時系列データを学習し、復元誤差が大きいデータを異常とみなす。\n",
        "<br>\n",
        "\n",
        "#### Variational Autoencoder (VAE)\n",
        "Autoencoderを確率モデルとして拡張した手法。\n",
        "<br>\n",
        "\n",
        "#### Generative Adversarial Networks (GANs)\n",
        " 正常な時系列データを生成する生成器と、生成されたデータが正常か異常かを判定する識別器を競合的に学習させる手法。識別器が正常と判定できないデータを異常とみなす。\n",
        " <br>"
      ],
      "metadata": {
        "id": "n1WuKOxxizq4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "\n",
        "\n",
        "### 移動平均法\n",
        "# window_size: 移動平均を計算する際に使用するデータの期間の長さ。\n",
        "#              周期性がある場合は周期の長さを設定すると異常値を検出しやすくなる。\n",
        "#              周期性がない場合は、変動が激しい時は小さく設定、変動が緩やかな時は\n",
        "#              大きく設定すると異常値を検出しやすくなる。\n",
        "#              （window_sizeを大きくすると短期的な変動の影響を受けにくくなる）\n",
        "def moving_average_method(data, window_size):\n",
        "    # 移動平均を計算\n",
        "    moving_average = np.convolve(data, np.ones(window_size) / window_size, mode='same')\n",
        "\n",
        "    # 異常値を判定\n",
        "    threshold = 2 * np.std(data - moving_average)  # 閾値は調整が必要\n",
        "    anomalies = data[np.abs(data - moving_average) > threshold]\n",
        "\n",
        "    return anomalies\n",
        "\n",
        "\n",
        "### ARIMAモデル\n",
        "# order: (p, d, q)からなるタプル\n",
        "#        p: 自己回帰(AR)モデルの次数。過去のデータポイントをいくつ参照するか。\n",
        "#        d: 階差の次数。データの定常化（トレンドや季節性の除去）のために何回差分を取るか。\n",
        "#        q: 移動平均(MA)モデルの次数。過去の予測誤差をいくつ参照するか。\n",
        "def arima_model(data, order):\n",
        "    # ARIMAモデルを学習\n",
        "    model = ARIMA(data, order=order)\n",
        "    model_fit = model.fit()\n",
        "\n",
        "    # 予測値と実測値の差を計算\n",
        "    residuals = model_fit.resid\n",
        "\n",
        "    # 異常値を判定\n",
        "    threshold = 2 * np.std(residuals)  # 閾値は調整が必要\n",
        "    anomalies = data[np.abs(residuals) > threshold]\n",
        "\n",
        "    return anomalies\n",
        "\n",
        "\n",
        "### LSTM (Long Short-Term Memory)\n",
        "# timesteps: 過去の何時点分のデータを使って予測を行うか\n",
        "def lstm_model(data, timesteps):\n",
        "    # データをLSTMモデルの入力形式に変換\n",
        "    X = []\n",
        "    y = []\n",
        "    for i in range(timesteps, len(data)):\n",
        "        X.append(data[i - timesteps:i])\n",
        "        y.append(data[i])\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "\n",
        "    # LSTMモデルを構築\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(units=50, return_sequences=True, input_shape=(timesteps, 1)))\n",
        "    model.add(LSTM(units=50))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "    # LSTMモデルを学習\n",
        "    model.fit(X, y, epochs=100, batch_size=32)\n",
        "\n",
        "    # 予測値と実測値の差を計算\n",
        "    predictions = model.predict(X)\n",
        "    residuals = y - predictions.flatten()\n",
        "\n",
        "    # 異常値を判定 (例：残差が大きい値を異常値とする)\n",
        "    threshold = 3 * np.std(residuals)  # 閾値は調整が必要\n",
        "\n",
        "    # 異常値のインデックスを取得\n",
        "    anomaly_indices = np.where(np.abs(residuals) > threshold)[0]\n",
        "\n",
        "    # 元のデータのサイズに合うようにインデックスを調整\n",
        "    anomaly_indices = anomaly_indices + timesteps\n",
        "\n",
        "    # 異常値を検出する\n",
        "    anomalies = data[anomaly_indices]\n",
        "\n",
        "    return anomalies\n",
        "\n",
        "\n",
        "# CPU使用率データの例\n",
        "cpu_usage = np.array([20, 22, 25, 23, 21, 24, 26, 28, 80, 25])  # 80 が異常値\n",
        "\n",
        "\n",
        "#### 移動平均法\n",
        "anomalies = moving_average_method(cpu_usage, window_size=3)\n",
        "print(\"================== 移動平均法 ================\")\n",
        "print(\"異常値:\", anomalies)\n",
        "print(\"\\n\")\n",
        "\n",
        "#### ARIMAモデル\n",
        "anomalies = arima_model(cpu_usage, order=(3, 0, 0))  # orderはモデルの次数を指定\n",
        "print(\"================= ARIMAモデル ================\")\n",
        "print(\"異常値:\", anomalies)\n",
        "print(\"\\n\")\n",
        "\n",
        "#### LSTM (Long Short-Term Memory)\n",
        "anomalies = lstm_model(cpu_usage, timesteps=3)  # timestepsは過去のデータを使用する期間を指定\n",
        "print(\"==================== LSTM ====================\")\n",
        "print(\"異常値:\", anomalies)\n",
        "print(\"\\n\")\n"
      ],
      "metadata": {
        "id": "Sp81D4inPF0h",
        "outputId": "36494aa9-49b7-44d2-f563-10eb88f5f741",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================== 移動平均法 ================\n",
            "異常値: [80]\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/statsmodels/tsa/statespace/sarimax.py:966: UserWarning: Non-stationary starting autoregressive parameters found. Using zeros as starting parameters.\n",
            "  warn('Non-stationary starting autoregressive parameters'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================= ARIMAモデル ================\n",
            "異常値: [80]\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9s/step - loss: 1461.7051\n",
            "Epoch 2/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - loss: 1454.8430\n",
            "Epoch 3/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 1448.1453\n",
            "Epoch 4/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 1441.5795\n",
            "Epoch 5/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 1435.0979\n",
            "Epoch 6/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 1428.6506\n",
            "Epoch 7/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1422.1963\n",
            "Epoch 8/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 1415.7020\n",
            "Epoch 9/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1409.1393\n",
            "Epoch 10/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 1402.4827\n",
            "Epoch 11/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 1395.7074\n",
            "Epoch 12/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 1388.7902\n",
            "Epoch 13/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 1381.7120\n",
            "Epoch 14/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 1374.4574\n",
            "Epoch 15/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 1367.0129\n",
            "Epoch 16/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 1359.3652\n",
            "Epoch 17/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 1351.5013\n",
            "Epoch 18/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 1343.4081\n",
            "Epoch 19/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 1335.0717\n",
            "Epoch 20/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 1326.4808\n",
            "Epoch 21/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 1317.6260\n",
            "Epoch 22/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 1308.5020\n",
            "Epoch 23/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 1299.1088\n",
            "Epoch 24/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 1289.4521\n",
            "Epoch 25/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 1279.5449\n",
            "Epoch 26/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 1269.4044\n",
            "Epoch 27/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 1259.0521\n",
            "Epoch 28/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 1248.5134\n",
            "Epoch 29/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 1237.8143\n",
            "Epoch 30/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 1226.9807\n",
            "Epoch 31/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 1216.0377\n",
            "Epoch 32/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1205.0072\n",
            "Epoch 33/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 1193.9094\n",
            "Epoch 34/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 1182.7607\n",
            "Epoch 35/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 1171.5753\n",
            "Epoch 36/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 1160.3651\n",
            "Epoch 37/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 1149.1394\n",
            "Epoch 38/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 1137.9080\n",
            "Epoch 39/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1126.6785\n",
            "Epoch 40/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 1115.4602\n",
            "Epoch 41/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 1104.2621\n",
            "Epoch 42/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 1093.0945\n",
            "Epoch 43/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 1081.9696\n",
            "Epoch 44/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 1070.9004\n",
            "Epoch 45/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 1059.9025\n",
            "Epoch 46/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 1048.9929\n",
            "Epoch 47/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1038.1913\n",
            "Epoch 48/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 1027.5194\n",
            "Epoch 49/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 1017.0009\n",
            "Epoch 50/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 1006.6613\n",
            "Epoch 51/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 996.5282\n",
            "Epoch 52/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 986.6300\n",
            "Epoch 53/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 976.9946\n",
            "Epoch 54/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 967.6490\n",
            "Epoch 55/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 958.6190\n",
            "Epoch 56/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 949.9262\n",
            "Epoch 57/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 941.5884\n",
            "Epoch 58/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 933.6188\n",
            "Epoch 59/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 926.0247\n",
            "Epoch 60/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 918.8075\n",
            "Epoch 61/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 911.9629\n",
            "Epoch 62/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 905.4814\n",
            "Epoch 63/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 899.3487\n",
            "Epoch 64/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 893.5469\n",
            "Epoch 65/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 888.0551\n",
            "Epoch 66/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 882.8504\n",
            "Epoch 67/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 877.9097\n",
            "Epoch 68/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 873.2088\n",
            "Epoch 69/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 868.7249\n",
            "Epoch 70/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 864.4360\n",
            "Epoch 71/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 860.3213\n",
            "Epoch 72/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 856.3625\n",
            "Epoch 73/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - loss: 852.5422\n",
            "Epoch 74/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 848.8454\n",
            "Epoch 75/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 845.2591\n",
            "Epoch 76/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 841.7711\n",
            "Epoch 77/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - loss: 838.3721\n",
            "Epoch 78/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 835.0530\n",
            "Epoch 79/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 831.8067\n",
            "Epoch 80/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 828.6273\n",
            "Epoch 81/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - loss: 825.5091\n",
            "Epoch 82/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 822.4474\n",
            "Epoch 83/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 819.4387\n",
            "Epoch 84/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 816.4794\n",
            "Epoch 85/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - loss: 813.5660\n",
            "Epoch 86/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - loss: 810.6962\n",
            "Epoch 87/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - loss: 807.8672\n",
            "Epoch 88/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 805.0766\n",
            "Epoch 89/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - loss: 802.3223\n",
            "Epoch 90/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 799.6026\n",
            "Epoch 91/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - loss: 796.9155\n",
            "Epoch 92/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 794.2595\n",
            "Epoch 93/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 791.6333\n",
            "Epoch 94/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 789.0356\n",
            "Epoch 95/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 786.4650\n",
            "Epoch 96/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 783.9208\n",
            "Epoch 97/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 781.4018\n",
            "Epoch 98/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 778.9070\n",
            "Epoch 99/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 776.4360\n",
            "Epoch 100/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 773.9876\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 336ms/step\n",
            "==================== LSTM ====================\n",
            "異常値: [80]\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 変数間に関係があるデータの異常検知\n",
        "### 例\n",
        "クレジットカードの不正利用検知（深夜に高額な取引が行われた場合、不正利用の可能性が高い、特定の場所で頻繁に取引が行われた場合、盗難カードの可能性が高いなど）\n",
        "\n",
        "### 手法\n",
        "#### 疎構造学習\n",
        "高次元データにおいて、変数間の関係が疎であることを仮定して、データの構造を学習する手法。正常なデータにおける変数間の関係を疎構造として学習し、その構造から外れるデータを異常とみなす。\n",
        "<br>\n",
        "\n",
        "#### ベイジアンネットワーク\n",
        "変数間の因果関係を表現したグラフィカルモデル。変数間の条件付き確率を学習し、異常な条件付き確率を持つデータを異常とみなす。\n",
        "<br>\n",
        "\n",
        "#### マルコフ確率場\n",
        "変数間の相互作用を表現したグラフィカルモデル。変数間のポテンシャル関数を学習し、異常なポテンシャル関数を持つデータを異常とみなす。\n",
        "<br>\n",
        "\n",
        "#### [k-means法](#scrollTo=FMOqmjNLiot5&line=7&uniqifier=1)\n",
        "<br>\n",
        "\n",
        "#### DBSCAN\n",
        "密度ベースのクラスタリングアルゴリズム。データポイントを、高密度領域にあるデータポイントと低密度領域にあるデータポイント（ノイズ）に分類し、低密度領域にあるデータポイントを異常とみなす。\n",
        "<br>\n",
        "\n",
        "#### [One-Class SVM](#scrollTo=FMOqmjNLiot5&line=33&uniqifier=1)\n",
        "<br>\n",
        "\n",
        "#### [孤立フォレスト (Isolation Forest)](#scrollTo=FMOqmjNLiot5&line=7&uniqifier=1)\n",
        "<br>\n",
        "\n",
        "#### [Local Outlier Factor (LOF)](#scrollTo=FMOqmjNLiot5&line=33&uniqifier=1)\n",
        "<br>"
      ],
      "metadata": {
        "id": "42XygjKgi1od"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "\n",
        "#### One-Class SVM\n",
        "def one_class_svm_relationship(data, threshold_percentile):\n",
        "  anomalies_per_column = pd.DataFrame()\n",
        "\n",
        "  for column in data.columns:\n",
        "    scaler = RobustScaler()\n",
        "    scaled_data = scaler.fit_transform(data[[column]])\n",
        "\n",
        "    # モデルの学習\n",
        "    # nu:     学習データの中で異常値と判定されるデータの割合の上限\n",
        "    #         nuを大きくすると、境界が緩やかになり、正常データを異常値と誤判定する\n",
        "    #         可能性は低くなるが、異常値を検出できない可能性も高まる\n",
        "    # kernel: モデルがデータの非線形性をどのように扱うか\n",
        "    # gamma:  カーネル関数の影響範囲\n",
        "    #         gammaの値が大きいと境界がより複雑で、データポイントに強くフィットする\n",
        "    #         ようになるが過学習のリスクが高まる\n",
        "    model = OneClassSVM(nu=0.3, kernel='rbf', gamma='scale')\n",
        "    model.fit(scaled_data)\n",
        "\n",
        "    # 異常度の算出\n",
        "    anomaly_score = model.decision_function(scaled_data)\n",
        "    threshold = np.percentile(anomaly_score, threshold_percentile)\n",
        "\n",
        "    # 異常値の判定\n",
        "    anomaly_index = np.where(anomaly_score < threshold)[0]\n",
        "\n",
        "    # 異常値を検出する\n",
        "    anomalies_per_column[column] = data.iloc[anomaly_index][column]\n",
        "\n",
        "  return anomalies_per_column\n",
        "\n",
        "\n",
        "#### 孤立フォレスト (Isolation Forest)\n",
        "# def isolation_forest_(transaction_data):\n",
        "#   # Isolation Forestモデルを学習\n",
        "#   model = IsolationForest()\n",
        "#   model.fit(transaction_data)\n",
        "\n",
        "#   # 異常度を算出\n",
        "#   anomaly_scores = model.decision_function(transaction_data)\n",
        "\n",
        "#   # 異常値を判定 (例：異常度が低い値を不正利用と判定)\n",
        "#   fraudulent_transactions = [transaction_data[i] for i, score in enumerate(anomaly_scores) if score < -0.35]  # 閾値は調整が必要\n",
        "\n",
        "#   return fraudulent_transactions\n",
        "\n",
        "#### Local Outlier Factor (LOF)\n",
        "# def local_outlier_factor_(transaction_data):\n",
        "#   # Local Outlier Factorモデルを学習\n",
        "#   model = LocalOutlierFactor(n_neighbors=20)  # n_neighborsはkの値を指定\n",
        "#   anomaly_scores = model.fit_predict(transaction_data)\n",
        "\n",
        "#   # 異常値を判定 (例：異常度が-1の値を不正利用と判定)\n",
        "#   fraudulent_transactions = [transaction_data[i] for i, score in enumerate(anomaly_scores) if score == -1]\n",
        "\n",
        "#   return fraudulent_transactions\n",
        "\n",
        "# 取引データの例\n",
        "transaction_data = pd.DataFrame({\n",
        "    'amount':   [100, 120, 130, 110, 125, 115, 135, 105, 10000, 120],\n",
        "    'time':     [ 10,  12,  13,  11,  12,  11,  13,  10,   20,  12],\n",
        "    'location': [  1,   2,   1,   3,   2,   1,   3,   2,    5,   1]\n",
        "})\n",
        "\n",
        "#### One-Class SVM\n",
        "anomalies = one_class_svm_relationship(transaction_data, 10)\n",
        "print(\"================ One-Class SVM ===============\")\n",
        "print(\"異常値:\", anomalies['amount'])\n",
        "print(\"\\n\")\n",
        "\n",
        "#### 孤立フォレスト\n",
        "# anomalies = isolation_forest_(transaction_data)\n",
        "# print(\"=============== 孤立フォレスト ===============\")\n",
        "# print(\"異常値:\", anomalies)\n",
        "# print(\"\\n\")\n",
        "\n",
        "#### Local Outlier Factor\n",
        "# anomalies = local_outlier_factor_(transaction_data)\n",
        "# print(\"============ Local Outlier Factor ============\")\n",
        "# print(\"異常値:\", anomalies)\n",
        "# print(\"\\n\")\n"
      ],
      "metadata": {
        "id": "UbHIYYDNPGap",
        "outputId": "c4ab3748-986a-4a22-e526-f94bb5d2ad07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   amount  time  location\n",
            "0     100    10         1\n",
            "1     120    12         2\n",
            "2     130    13         1\n",
            "3     110    11         3\n",
            "4     125    12         2\n",
            "5     115    11         1\n",
            "6     135    13         3\n",
            "7     105    10         2\n",
            "8   10000    20         5\n",
            "9     120    12         1\n",
            "================ One-Class SVM ===============\n",
            "異常値: 8    10000\n",
            "Name: amount, dtype: int64\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}