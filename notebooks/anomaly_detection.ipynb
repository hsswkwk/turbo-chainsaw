{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyODMVawDv5nt3d5EaJTzaIx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hsswkwk/turbo-chainsaw/blob/feature-add-anomaly-detection/notebooks/anomaly_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 異常検知（Anomaly detection）\n",
        "標準的な状態や想定から逸脱しているデータや事象を特定する技術\n"
      ],
      "metadata": {
        "id": "PaP90VRh7wqm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 正規分布に従うデータの異常検知\n",
        "### 例\n",
        "製品の温度をセンサーで監視している場合の異常検知\n",
        "<br>\n",
        "<br>\n",
        "### 手法\n",
        "\n",
        "#### 3$\\sigma$法\n",
        "データが正規分布に従うと仮定し、平均値から標準偏差の3倍以上離れた値を異常値とみなす方法\n",
        "<br>\n",
        "#### マハラノビス・タグチ法\n",
        "データの各次元間の相関を考慮した距離尺度を用いて、平均値から離れた値を異常値とみなす方法\n",
        "<br>\n",
        "#### ホテリングの$T^2$法\n",
        "マハラノビス距離を拡張した手法で、データの平均値からのずれを検定統計量として用いて異常値を検出する方法\n",
        "<br>\n",
        "#### 密度比推定\n",
        "正常データと異常データの確率密度比を推定することで異常を検知する手法\n",
        "<br>"
      ],
      "metadata": {
        "id": "1dE3Gampih0k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.spatial.distance import mahalanobis\n",
        "from scipy.stats import f\n",
        "from sklearn.neighbors import KernelDensity\n",
        "\n",
        "\n",
        "### 3σ法\n",
        "# threshold: データの平均値から標準偏差の何倍離れたら異常値とみなすか\n",
        "def three_sigma_method(data, threshold=3):\n",
        "  mean = np.mean(data)\n",
        "  std = np.std(data)\n",
        "  lower_bound = mean - threshold * std\n",
        "  upper_bound = mean + threshold * std\n",
        "  if data.ndim == 1:\n",
        "    anomaly_indices = np.where((data < lower_bound) | (data > upper_bound))[0]\n",
        "  elif data.ndim >= 2:\n",
        "    anomaly_indices = np.where(np.any((data < lower_bound) | (data > upper_bound), axis=1))[0]\n",
        "  return data[anomaly_indices]\n",
        "\n",
        "### マハラノビス距離\n",
        "# マハラノビス距離: データの相関を考慮した距離尺度\n",
        "# threshold_percentile: マハラノビス距離の分布におけるパーセンタイル値を超える\n",
        "#                       距離を持つデータを異常値とみなす\n",
        "def mahalanobis_distance_method(data, threshold_percentile=99):\n",
        "  mean = np.mean(data, axis=0)\n",
        "  if data.ndim == 1:\n",
        "    std = np.std(data)\n",
        "\n",
        "    # 標準偏差が0の場合、ゼロ除算を避けるために微小な値を加算\n",
        "    if std == 0:\n",
        "      std = 1e-6\n",
        "\n",
        "    distances = np.abs((data - mean) / std)\n",
        "\n",
        "    # 異常値とみなす閾値を設定\n",
        "    threshold = np.percentile(distances, threshold_percentile)\n",
        "\n",
        "    # 閾値を超えるデータを異常値として検出\n",
        "    anomalies = data[distances > threshold]\n",
        "  elif data.ndim >= 2:\n",
        "    cov = np.cov(data, rowvar=False)\n",
        "    inv_cov = np.linalg.inv(cov)\n",
        "\n",
        "    distances = [mahalanobis(x, mean, inv_cov) for x in data]\n",
        "\n",
        "    # 異常値とみなす閾値を設定\n",
        "    threshold = np.percentile(distances, threshold_percentile)\n",
        "\n",
        "    # 閾値を超えるデータを異常値として検出\n",
        "    anomaly_indices = np.where(np.array(distances) > threshold)[0]\n",
        "    anomalies = data[anomaly_indices]\n",
        "  return anomalies\n",
        "\n",
        "### ホテリングのT^2法\n",
        "def hotelling_t2_method(data):\n",
        "  mean = np.mean(data, axis=0)\n",
        "  if data.ndim == 1:\n",
        "    var = np.var(data)\n",
        "    t2_values = [(x - mean)**2 / var for x in data]\n",
        "    # f.ppf: F分布のパーセント点関数\n",
        "    threshold = f.ppf(0.95, 1, len(data) - 1)  # 異常値とみなす閾値を設定\n",
        "    anomalies = [data[i] for i, t2 in enumerate(t2_values) if t2 > threshold]\n",
        "  elif data.ndim >= 2:\n",
        "    mean = np.mean(data, axis=0)\n",
        "    cov = np.cov(data, rowvar=False)\n",
        "    inv_cov = np.linalg.inv(cov)\n",
        "    t2_values = [mahalanobis(x, mean, inv_cov)**2 for x in data]\n",
        "    threshold = f.ppf(0.95, data.shape[1], data.shape[0] - data.shape[1] - 1)  # 異常値とみなす閾値を設定\n",
        "    anomaly_indices = np.where(np.array(t2_values) > threshold)[0]\n",
        "    anomalies = data[anomaly_indices]\n",
        "  return anomalies\n",
        "\n",
        "### KLIEPによる密度比推定\n",
        "# bandwidth: カーネル密度推定で使用されるカーネルの幅を制御するパラメータ\n",
        "#            大きいほど推定される確率密度関数は滑らかになる\n",
        "def kliep(normal_data, anomaly_data, bandwidth=0.1):\n",
        "  if normal_data.ndim == 1:\n",
        "    normal_data = normal_data.reshape(-1, 1)\n",
        "  if anomaly_data.ndim == 1:\n",
        "    anomaly_data = anomaly_data.reshape(-1, 1)\n",
        "  # カーネル密度推定でデータから確率密度関数を推定する\n",
        "  kde_normal = KernelDensity(bandwidth=bandwidth).fit(normal_data)\n",
        "  kde_anomaly = KernelDensity(bandwidth=bandwidth).fit(anomaly_data)\n",
        "  density_ratio = np.exp(kde_normal.score_samples(anomaly_data) - kde_anomaly.score_samples(anomaly_data))\n",
        "\n",
        "  # 1e-6 を加算してゼロ除算を防ぐ\n",
        "  density_ratio = np.maximum(density_ratio, 1e-6)\n",
        "\n",
        "  # 異常度の算出\n",
        "  anomaly_score = -np.log(density_ratio)\n",
        "\n",
        "  # 閾値の設定 (例: 異常度の95%点)\n",
        "  threshold = np.percentile(anomaly_score, 95)\n",
        "\n",
        "  # 異常検知\n",
        "  anomalies = anomaly_data[anomaly_score > threshold]\n",
        "  return anomalies\n",
        "\n",
        "\n",
        "print(\"1次元データの場合\")\n",
        "# 正常データ生成\n",
        "normal_data = np.random.normal(loc=25, scale=2, size=100)\n",
        "\n",
        "# 異常データ生成\n",
        "anomaly_data = np.random.normal(loc=25, scale=2, size=100)\n",
        "# 異常値の追加\n",
        "anomaly_indices = [10, 20, 30]  # 異常値のインデックス\n",
        "anomaly_values = [35, 15, 32]  # 異常値\n",
        "anomaly_data[anomaly_indices] = anomaly_values\n",
        "\n",
        "# 異常検知の実行\n",
        "anomalies_three_sigma_method = three_sigma_method(anomaly_data)\n",
        "anomalies_mahalanobis_distance_method = mahalanobis_distance_method(anomaly_data)\n",
        "anomalies_hotelling_t2_method = hotelling_t2_method(anomaly_data)\n",
        "anomalies_kliep = kliep(normal_data, anomaly_data)\n",
        "\n",
        "# 結果の出力\n",
        "print(\"================== 3σ法 =================\")\n",
        "print(\"異常値:\", anomalies_three_sigma_method)\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"===== マハラノビス距離による異常検知 =====\")\n",
        "print(\"異常値:\", anomalies_mahalanobis_distance_method)\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"===== ホテリングのT^2法による異常検知 ====\")\n",
        "print(\"異常値:\", anomalies_hotelling_t2_method)\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"======== 密度比推定による異常検知 ========\")\n",
        "print(\"異常値:\", anomalies_kliep)\n",
        "print(\"\\n\")\n",
        "\n",
        "\n",
        "\n",
        "print(\"2次元データの場合\")\n",
        "\n",
        "# 正常データ生成\n",
        "normal_data2d = np.random.multivariate_normal(mean=[0, 0], cov=[[1, 0.5], [0.5, 1]], size=100)\n",
        "\n",
        "# 異常データ生成\n",
        "anomaly_data2d = np.random.multivariate_normal(mean=[0, 0], cov=[[1, 0.5], [0.5, 1]], size=100)\n",
        "# 異常値の追加\n",
        "anomaly_indices = [10, 20, 30]  # 異常値のインデックス\n",
        "anomaly_values = [[3, 3], [-3, -3], [3, -3]]  # 異常値\n",
        "anomaly_data2d[anomaly_indices] = anomaly_values\n",
        "\n",
        "# 異常検知の実行\n",
        "anomalies_three_sigma_method_2d = three_sigma_method(anomaly_data2d, threshold=2)\n",
        "anomalies_mahalanobis_distance_method_2d = mahalanobis_distance_method(anomaly_data2d)\n",
        "anomalies_hotelling_t2_method_2d = hotelling_t2_method(anomaly_data2d)\n",
        "anomalies_kliep_2d = kliep(normal_data2d, anomaly_data2d, bandwidth=0.5)\n",
        "\n",
        "# 結果の出力\n",
        "print(\"================== 3σ法 =================\")\n",
        "print(\"異常値:\", anomalies_three_sigma_method_2d)\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"===== マハラノビス距離による異常検知 =====\")\n",
        "print(\"異常値:\", anomalies_mahalanobis_distance_method_2d)\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"===== ホテリングのT^2法による異常検知 ====\")\n",
        "print(\"異常値:\", anomalies_hotelling_t2_method_2d)\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"======== 密度比推定による異常検知 ========\")\n",
        "print(\"異常値:\", anomalies_kliep_2d)\n",
        "print(\"\\n\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VydvSi0Ejy_Z",
        "outputId": "02da6d21-dcde-47e9-9ff1-967bb907101c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1次元データの場合\n",
            "================== 3σ法 =================\n",
            "異常値: [35. 15.]\n",
            "\n",
            "\n",
            "===== マハラノビス距離による異常検知 =====\n",
            "異常値: [15.]\n",
            "\n",
            "\n",
            "===== ホテリングのT^2法による異常検知 ====\n",
            "異常値: [np.float64(35.0), np.float64(15.0), np.float64(32.0)]\n",
            "\n",
            "\n",
            "======== 密度比推定による異常検知 ========\n",
            "異常値: [[35.        ]\n",
            " [15.        ]\n",
            " [32.        ]\n",
            " [20.99479616]\n",
            " [20.89919911]]\n",
            "\n",
            "\n",
            "2次元データの場合\n",
            "================== 3σ法 =================\n",
            "異常値: [[ 2.13848135  2.45500825]\n",
            " [ 3.          3.        ]\n",
            " [-3.         -3.        ]\n",
            " [ 3.         -3.        ]\n",
            " [ 2.71516196  1.84843097]\n",
            " [ 0.98278229  3.05596083]\n",
            " [ 2.37145929  1.40984141]]\n",
            "\n",
            "\n",
            "===== マハラノビス距離による異常検知 =====\n",
            "異常値: [[ 3. -3.]]\n",
            "\n",
            "\n",
            "===== ホテリングのT^2法による異常検知 ====\n",
            "異常値: [[ 2.13848135  2.45500825]\n",
            " [ 3.          3.        ]\n",
            " [-3.         -3.        ]\n",
            " [-2.21439899 -0.31184462]\n",
            " [ 0.06048322 -1.88858858]\n",
            " [ 3.         -3.        ]\n",
            " [ 2.71516196  1.84843097]\n",
            " [-2.04126395  0.25096997]\n",
            " [-2.16449548 -1.45698025]\n",
            " [ 0.12847085  2.13699689]\n",
            " [ 1.30552164 -1.098316  ]\n",
            " [ 0.06310474 -1.86091794]\n",
            " [ 0.98278229  3.05596083]\n",
            " [ 2.37145929  1.40984141]]\n",
            "\n",
            "\n",
            "======== 密度比推定による異常検知 ========\n",
            "異常値: [[ 3.          3.        ]\n",
            " [-3.         -3.        ]\n",
            " [ 0.06048322 -1.88858858]\n",
            " [ 3.         -3.        ]\n",
            " [ 0.98278229  3.05596083]]\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 非正規データの異常検知\n",
        "### 例\n",
        "ウェブサイトへのアクセス数の異常検知\n",
        "<br>\n",
        "<br>\n",
        "### 手法\n",
        "\n",
        "#### ガンマ分布の当てはめ\n",
        "データがガンマ分布に従うと仮定し、その分布から大きく外れた値を異常値とみなす方法。データがガンマ分布に従う場合、高い精度で異常を検出できるが、従わない場合、精度が低下する。\n",
        "<br>\n",
        "\n",
        "#### カイ二乗分布への当てはめ\n",
        "データがカイ二乗分布に従うと仮定し、その分布から大きく外れた値を異常値とみなす方法。主に、特徴量の値が正の値を取り、歪んだ分布をしている場合に有効。<br>\n",
        "<br>\n",
        "\n",
        "#### $k$近傍法\n",
        "各データポイントからk番目に近いデータポイントまでの距離を計算し、距離が大きいデータポイントを異常値と判定するアルゴリズム\n",
        "<br>\n",
        "\n",
        "#### $k$ means法\n",
        "データを複数のクラスタに分割し、どのクラスタにも属さないデータポイントを異常値と判定するアルゴリズム。\n",
        "<br>\n",
        "\n",
        "#### 混合ガウス分布モデル（Gaussian Mixture Model, GMM）\n",
        "データが複数のガウス分布（正規分布）の混合で表現できると仮定し、低確率なデータ点を異常値とみなす方法。\n",
        "<br>\n",
        "\n",
        "#### One-Class SVM\n",
        "正常データのみを用いて、正常データの領域を学習するアルゴリズム。 学習した領域から外れたデータは異常値と判定する。\n",
        "<br>\n",
        "\n",
        "#### [密度比推定](#scrollTo=1dE3Gampih0k&line=1&uniqifier=1)\n",
        "<br>\n",
        "\n",
        "#### 孤立フォレスト（Isolation Forest）\n",
        "データポイントをランダムに分割していくことで、異常値を孤立させるアルゴリズム。\n",
        "<br>\n",
        "\n",
        "#### Local Outlier Factor (LOF)\n",
        "データポイントの局所的な密度を計算し、密度が低いデータポイントを異常値と判定するアルゴリズム。\n",
        "<br>\n",
        "\n"
      ],
      "metadata": {
        "id": "FMOqmjNLiot5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "\n",
        "\n",
        "def fitting_gamma_distribution(data):\n",
        "  # データにガンマ分布を当てはめる\n",
        "  shape, loc, scale = stats.gamma.fit(data)\n",
        "\n",
        "  # 異常度を計算する\n",
        "  anomaly_scores = 1 / stats.gamma.pdf(data, shape, loc, scale)\n",
        "\n",
        "  # 閾値を設定する\n",
        "  threshold = np.mean(anomaly_scores) + 3 * np.std(anomaly_scores)\n",
        "\n",
        "  # 異常値を検出する\n",
        "  anomalies = data[anomaly_scores > threshold]\n",
        "\n",
        "  return anomalies\n",
        "\n",
        "\n",
        "def fitting_chi_square_distribution(data):\n",
        "  # データにカイ二乗分布を当てはめる\n",
        "  df, loc, scale = stats.chi2.fit(data)\n",
        "\n",
        "  # 異常度を計算する (確率密度関数の逆数を使用)\n",
        "  anomaly_scores = 1 / stats.chi2.pdf(data, df, loc, scale)\n",
        "\n",
        "  # 閾値を設定する (上位5%点を閾値にする)\n",
        "  threshold = stats.chi2.ppf(0.95, df, loc, scale)\n",
        "\n",
        "  # 異常値を検出する\n",
        "  anomalies = data[anomaly_scores > threshold]\n",
        "\n",
        "  return anomalies\n",
        "\n",
        "\n",
        "def k_nearest_neighbors(data, k):\n",
        "  # k近傍の計算\n",
        "  knn = NearestNeighbors(n_neighbors=k)\n",
        "  knn.fit(data)\n",
        "  distances, indices = knn.kneighbors(data)\n",
        "\n",
        "  # 異常度の算出\n",
        "  anomaly_score = distances[:, -1]\n",
        "\n",
        "  # 異常値の判定\n",
        "  anomaly_index = np.where(anomaly_score > threshold)\n",
        "\n",
        "  # 異常値を検出する\n",
        "  anomalies = data[anomaly_index]\n",
        "\n",
        "  return anomalies\n",
        "\n",
        "\n",
        "def k_means(data, k):\n",
        "  # クラスタリングの実行\n",
        "  kmeans = KMeans(n_clusters=k)\n",
        "  kmeans.fit(data)\n",
        "  labels = kmeans.labels_\n",
        "\n",
        "  # 異常値の判定\n",
        "  anomaly_index = np.where(labels == -1)\n",
        "\n",
        "  # 異常値を検出する\n",
        "  anomalies = data[anomaly_index]\n",
        "\n",
        "  return anomalies\n",
        "\n",
        "\n",
        "def gaussian_mixture_model(data):\n",
        "  # GMMモデルの構築 (コンポーネント数は3)\n",
        "  gmm = GaussianMixture(n_components=3)\n",
        "\n",
        "  # データを用いてモデルを学習\n",
        "  gmm.fit(data)\n",
        "\n",
        "  # 各データ点の確率密度を計算\n",
        "  densities = gmm.score_samples(data)\n",
        "  # 異常度は確率密度の負の対数で表されることが多い\n",
        "  anomaly_scores = -densities\n",
        "\n",
        "  # 閾値を設定 (例：確率密度の下位5%点を閾値にする)\n",
        "  threshold = np.percentile(anomaly_scores, 5)\n",
        "\n",
        "  # 異常値を検出\n",
        "  anomalies = data[anomaly_scores > threshold]\n",
        "\n",
        "  return anomalies\n",
        "\n",
        "\n",
        "def one_class_svm(data):\n",
        "  # モデルの学習\n",
        "  model = OneClassSVM()\n",
        "  model.fit(data)\n",
        "\n",
        "  # 異常度の算出\n",
        "  anomaly_score = model.decision_function(data)\n",
        "\n",
        "  # 異常値の判定\n",
        "  anomaly_index = np.where(anomaly_score < 0)\n",
        "\n",
        "  # 異常値を検出する\n",
        "  anomalies = data[anomaly_index]\n",
        "\n",
        "  return anomalies\n",
        "\n",
        "\n",
        "def isolation_forest(data):\n",
        "  # モデルの学習\n",
        "  model = IsolationForest()\n",
        "  model.fit(data)\n",
        "\n",
        "  # 異常度の算出\n",
        "  anomaly_score = model.decision_function(data)\n",
        "\n",
        "  # 異常値の判定\n",
        "  anomaly_index = np.where(anomaly_score < 0)\n",
        "\n",
        "  # 異常値を検出する\n",
        "  anomalies = data[anomaly_index]\n",
        "\n",
        "  return anomalies\n",
        "\n",
        "\n",
        "def local_outlier_factor(data):\n",
        "  # モデルの学習\n",
        "  model = LocalOutlierFactor()\n",
        "  anomaly_score = model.fit_predict(data)\n",
        "\n",
        "  # 異常値の判定\n",
        "  anomaly_index = np.where(anomaly_score == -1)\n",
        "\n",
        "  # 異常値を検出する\n",
        "  anomalies = data[anomaly_index]\n",
        "\n",
        "  return anomalies\n",
        "\n",
        "\n",
        "def power_law_distribution(alpha, xmin, xmax, size):\n",
        "    \"\"\"べき乗則に従う乱数を生成する関数\n",
        "\n",
        "    Args:\n",
        "        alpha (float): べき乗則の指数\n",
        "        xmin (float): 最小値\n",
        "        xmax (float): 最大値\n",
        "        size (int): 生成する乱数の数\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: べき乗則に従う乱数\n",
        "    \"\"\"\n",
        "\n",
        "    u = np.random.uniform(size=size)\n",
        "    return (xmin ** (1 - alpha) + (xmax ** (1 - alpha) - xmin ** (1 - alpha)) * u) ** (1 / (1 - alpha))\n",
        "\n",
        "\n",
        "# 正常データの生成\n",
        "normal_data = power_law_distribution(alpha=2, xmin=1, xmax=100, size=1000)\n",
        "\n",
        "# 異常データの生成\n",
        "outliers = np.random.uniform(low=2*normal_data.max(), high=3*normal_data.max(), size=10)\n",
        "anomaly_data = np.concatenate([normal_data, outliers])\n",
        "\n",
        "all_data = np.concatenate([normal_data, anomaly_data])\n",
        "\n",
        "#### ガンマ分布の当てはめ\n",
        "anomalies = fitting_gamma_distribution(all_data)\n",
        "print(\"============ ガンマ分布の当てはめ ==========\")\n",
        "print(\"異常値:\", anomalies)\n",
        "print(\"\\n\")\n",
        "\n",
        "#### カイ二乗分布への当てはめ\n",
        "anomalies = fitting_chi_square_distribution(all_data)\n",
        "print(\"========= カイ二乗分布への当てはめ =========\")\n",
        "print(\"異常値:\", anomalies)\n",
        "print(\"\\n\")\n",
        "\n",
        "#### k近傍法\n",
        "anomalies = k_nearest_neighbors(all_data, k)\n",
        "print(\"================== k近傍法 =================\")\n",
        "print(\"異常値:\", anomalies)\n",
        "print(\"\\n\")\n",
        "\n",
        "#### k means法\n",
        "anomalies = k_means(all_data, k)\n",
        "print(\"================= k means法 ================\")\n",
        "print(\"異常値:\", anomalies)\n",
        "print(\"\\n\")\n",
        "\n",
        "#### 混合ガウス分布モデル（Gaussian Mixture Model, GMM）\n",
        "anomalies = gaussian_mixture_model(all_data)\n",
        "print(\"============ 混合ガウス分布モデル ==========\")\n",
        "print(\"異常値:\", anomalies)\n",
        "print(\"\\n\")\n",
        "\n",
        "#### One-Class SVM\n",
        "anomalies = isolation_forest(all_data)\n",
        "print(\"=============== One-Class SVM ==============\")\n",
        "print(\"異常値:\", anomalies)\n",
        "print(\"\\n\")\n",
        "\n",
        "#### 密度比推定\n",
        "anomalies = kliep(normal_data, anomaly_data)\n",
        "print(\"================ 密度比推定 ================\")\n",
        "print(\"異常値:\", anomalies)\n",
        "print(\"\\n\")\n",
        "\n",
        "#### 孤立フォレスト (Isolation Forest)\n",
        "anomalies = isolation_forest(all_data)\n",
        "print(\"=============== 孤立フォレスト ==============\")\n",
        "print(\"異常値:\", anomalies)\n",
        "print(\"\\n\")\n",
        "\n",
        "#### Local Outlier Factor (LOF)\n",
        "anomalies = local_outlier_factor(all_data)\n",
        "print(\"============ Local Outlier Factor ===========\")\n",
        "print(\"異常値:\", anomalies)\n",
        "print(\"\\n\")\n"
      ],
      "metadata": {
        "id": "aDeywrg2lfYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 不要次元のある次元データの異常検知\n",
        "### 例\n",
        "顧客の購買データを用いた異常検知\n",
        "<br>\n",
        "<br>\n",
        "### 次元削減を用いる手法\n",
        "\n",
        "#### 主成分分析（PCA）\n",
        "データの分散を最大化するように新しい軸を定義し、その軸にデータを射影することで次元を削減する手法。異常なデータ点は、主成分空間において、正常なデータ点から離れた位置に配置される傾向がある。\n",
        "<br>\n",
        "\n",
        "#### 確率的主成分分析（Probabilistic PCA）\n",
        "PCAを確率モデルとして拡張した手法。データにノイズが含まれる場合に、よりロバストな結果が得られる。（＝データにノイズや外れ値が含まれていても、分析結果が大きく影響を受けにくい）\n",
        "<br>\n",
        "\n",
        "#### カーネル主成分分析（Kernel PCA）\n",
        "非線形な関係を持つデータに対して、カーネル関数を使用して高次元空間に写像し、その空間でPCAを行うことで次元を削減する手法。\n",
        "<br>\n",
        "\n",
        "#### 因子分析（Factor Analysis）\n",
        "観測変数の背後にある潜在変数を推定し、それらを用いて次元を削減する手法。\n",
        "<br>\n",
        "\n",
        "#### 独立成分分析（ICA）\n",
        "観測変数を、統計的に独立な成分に分解することで次元を削減する手法。\n",
        "<br>\n",
        "\n",
        "#### t-distributed Stochastic Neighbor Embedding (t-SNE)\n",
        "高次元データを低次元空間に埋め込む際に、データの局所的な構造を保持する様に設計された手法。異常検知では、正常なデータ点と異常なデータ点が、低次元空間において明確に分離されることが期待する。\n",
        "<br>\n",
        "\n",
        "#### Uniform Manifold Approximation and Projection (UMAP)\n",
        "高次元データを低次元空間に埋め込む際に、データのトポロジカルな構造を保持する様に設計された手法です。t-SNEと同様に、異常検知では、正常なデータ点と異常なデータ点が、低次元空間において明確に分離されることを期待する。\n",
        "<br>\n",
        "\n",
        "### 特徴量選択を用いる手法\n",
        "#### フィルター法\n",
        "各特徴量と目的変数の間の相関や相互情報量などを用いて、重要度の低い特徴量を除去する手法。\n",
        "<br>\n",
        "\n",
        "#### ラッパー法\n",
        "特徴量のサブセットを選択し、そのサブセットを用いて学習したモデルの性能を評価することで、最適な特徴量を選択する手法。\n",
        "<br>\n",
        "\n",
        "#### 埋め込み法\n",
        "モデルの学習過程で特徴量選択を行う手法。LASSO回帰や決定木などが用いられる。\n",
        "<br>\n",
        "\n",
        "### その他の手法\n",
        "#### [One-Class SVM](#scrollTo=FMOqmjNLiot5&line=33&uniqifier=1)\n",
        "正常なデータのみを用いて、正常データの領域を学習し、その領域から外れたデータを異常と判定する手法。\n",
        "<br>\n",
        "\n",
        "#### [孤立フォレスト（Isolation Forest）](#scrollTo=FMOqmjNLiot5&line=33&uniqifier=1)\n",
        "<br>\n",
        "\n",
        "#### [Local Outlier Factor (LOF)](#scrollTo=FMOqmjNLiot5&line=33&uniqifier=1)\n",
        "<br>\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "V4fWrW7RivcU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.decomposition import KernelPCA\n",
        "\n",
        "\n",
        "#### 正常データの生成\n",
        "# 顧客ID、年齢、性別、購入金額、購入頻度、不要な次元（ランダムな数値）を生成\n",
        "num_customers = 100  # 顧客数\n",
        "customer_ids = np.arange(1, num_customers + 1)\n",
        "ages = np.random.randint(20, 60, size=num_customers)\n",
        "genders = np.random.choice(['Male', 'Female'], size=num_customers)\n",
        "purchase_amounts = np.random.randint(100, 10000, size=num_customers)\n",
        "purchase_frequencies = np.random.randint(1, 10, size=num_customers)\n",
        "unnecessary_dimension = np.random.rand(num_customers)\n",
        "\n",
        "# データフレームを作成\n",
        "normal_data = pd.DataFrame({\n",
        "  'CustomerID': customer_ids,\n",
        "  'Age': ages,\n",
        "  'Gender': genders,\n",
        "  'PurchaseAmount': purchase_amounts,\n",
        "  'PurchaseFrequency': purchase_frequencies,\n",
        "  'UnnecessaryDimension': unnecessary_dimension  # 不要な次元\n",
        "})\n",
        "\n",
        "#### 異常データの生成\n",
        "anomaly_data = normal_data.copy()\n",
        "\n",
        "# 異常値を挿入するインデックスをランダムに選択\n",
        "# 例：顧客の10%を異常値とする\n",
        "anomaly_indices = np.random.choice(anomaly_data.index, size=int(num_customers * 0.1), replace=False)\n",
        "\n",
        "# 選択したインデックスのデータに異常値を代入\n",
        "# 例：PurchaseAmountを極端に大きくする\n",
        "anomaly_data.loc[anomaly_indices, 'PurchaseAmount'] = anomaly_data.loc[anomaly_indices, 'PurchaseAmount'] * np.random.uniform(100, 200, size=len(anomaly_indices))\n",
        "\n",
        "\n",
        "def pca(normal_data, anomaly_data):\n",
        "  # 不要な次元を除外\n",
        "  features = ['Age', 'PurchaseAmount', 'PurchaseFrequency', 'UnnecessaryDimension']  # Genderはカテゴリカルデータなので除外\n",
        "  X_normal = normal_data[features].values\n",
        "  X_anomaly = anomaly_data[features].values\n",
        "\n",
        "  # データを標準化\n",
        "  scaler = StandardScaler()\n",
        "  X_normal_scaled = scaler.fit_transform(X_normal)\n",
        "  X_anomaly_scaled = scaler.transform(X_anomaly)\n",
        "\n",
        "  # PCAモデルを学習\n",
        "  pca = PCA(n_components=2)  # 次元数を2に削減\n",
        "  pca.fit(X_normal_scaled)  # 正常データでモデルを学習\n",
        "\n",
        "  # 異常度を計算\n",
        "  normal_scores = pca.transform(X_normal_scaled)\n",
        "  anomaly_scores = pca.transform(X_anomaly_scaled)\n",
        "\n",
        "  # 再構成誤差を異常度として使用\n",
        "  reconstruction_error_normal = np.sum(np.square(X_normal_scaled - pca.inverse_transform(normal_scores)), axis=1)\n",
        "  reconstruction_error_anomaly = np.sum(np.square(X_anomaly_scaled - pca.inverse_transform(anomaly_scores)), axis=1)\n",
        "\n",
        "  # 閾値を設定 (例: 再構成誤差の95%点)\n",
        "  threshold = np.percentile(reconstruction_error_normal, 95)\n",
        "\n",
        "  # 異常を検出\n",
        "  anomalies = anomaly_data[reconstruction_error_anomaly > threshold]\n",
        "\n",
        "  return anomalies\n",
        "\n",
        "\n",
        "def probabilistic_pca(normal_data, anomaly_data):\n",
        "  # 不要な次元を除外\n",
        "  features = ['Age', 'PurchaseAmount', 'PurchaseFrequency', 'UnnecessaryDimension']  # Genderはカテゴリカルデータなので除外\n",
        "  X_normal = normal_data[features].values\n",
        "  X_anomaly = anomaly_data[features].values\n",
        "\n",
        "  # データを標準化\n",
        "  scaler = StandardScaler()\n",
        "  X_normal_scaled = scaler.fit_transform(X_normal)\n",
        "  X_anomaly_scaled = scaler.transform(X_anomaly)\n",
        "\n",
        "  # 確率的PCAモデルを学習\n",
        "  ppca = PCA(n_components=2, svd_solver='full')  # 次元数を2に削減, svd_solver='full'で確率的PCAを指定\n",
        "  ppca.fit(X_normal_scaled)  # 正常データでモデルを学習\n",
        "\n",
        "  # 異常度を計算 (平均対数尤度)\n",
        "  anomaly_scores = -ppca.score_samples(X_anomaly_scaled)\n",
        "\n",
        "  # 閾値を設定 (例: 平均対数尤度の5%点)\n",
        "  threshold = np.percentile(anomaly_scores, 5)\n",
        "\n",
        "  # 異常を検出\n",
        "  anomalies = anomaly_data[anomaly_scores < threshold]  # 平均対数尤度が低いデータが異常\n",
        "\n",
        "  return anomalies\n",
        "\n",
        "\n",
        "def kernel_pca(normal_data, anomaly_data):\n",
        "  # 不要な次元を除外\n",
        "  features = ['Age', 'PurchaseAmount', 'PurchaseFrequency', 'UnnecessaryDimension']  # Genderはカテゴリカルデータなので除外\n",
        "  X_normal = normal_data[features].values\n",
        "  X_anomaly = anomaly_data[features].values\n",
        "\n",
        "  # データを標準化\n",
        "  scaler = StandardScaler()\n",
        "  X_normal_scaled = scaler.fit_transform(X_normal)\n",
        "  X_anomaly_scaled = scaler.transform(X_anomaly)\n",
        "\n",
        "  # Kernel PCAモデルを学習\n",
        "  kpca = KernelPCA(n_components=2, kernel='rbf')  # 次元数を2に削減, kernel='rbf'でRBFカーネルを指定\n",
        "  kpca.fit(X_normal_scaled)  # 正常データでモデルを学習\n",
        "\n",
        "  # 異常度を計算 (再構成誤差)\n",
        "  normal_scores = kpca.transform(X_normal_scaled)\n",
        "  anomaly_scores = kpca.transform(X_anomaly_scaled)\n",
        "\n",
        "  # 再構成誤差を計算 (近似)\n",
        "  reconstruction_error_normal = np.sum(np.square(X_normal_scaled - kpca.inverse_transform(normal_scores)), axis=1)\n",
        "  reconstruction_error_anomaly = np.sum(np.square(X_anomaly_scaled - kpca.inverse_transform(anomaly_scores)), axis=1)\n",
        "\n",
        "  # 閾値を設定 (例: 再構成誤差の95%点)\n",
        "  threshold = np.percentile(reconstruction_error_normal, 95)\n",
        "\n",
        "  # 異常を検出\n",
        "  anomalies = anomaly_data[reconstruction_error_anomaly > threshold]  # 再構成誤差が閾値を超えるデータが異常\n",
        "\n",
        "  return anomalies\n",
        "\n",
        "\n",
        "#### 主成分分析（PCA）\n",
        "anomalies = pca(normal_data, anomaly_data)\n",
        "print(\"============ 主成分分析 ===========\")\n",
        "print(\"異常値:\", anomalies)\n",
        "print(\"\\n\")\n",
        "\n",
        "#### 確率的主成分分析（Probabilistic PCA）\n",
        "anomalies = probabilistic_pca(normal_data, anomaly_data)\n",
        "print(\"========= 確率的主成分分析 ========\")\n",
        "print(\"異常値:\", anomalies)\n",
        "print(\"\\n\")\n",
        "\n",
        "#### カーネル主成分分析（Kernel PCA）\n",
        "anomalies = kernel_pca(normal_data, anomaly_data)\n",
        "print(\"======== カーネル主成分分析 ========\")\n",
        "print(\"異常値:\", anomalies)\n",
        "print(\"\\n\")\n",
        "\n",
        "#### 因子分析（Factor Analysis）\n",
        "\n",
        "#### 独立成分分析（ICA）\n",
        "\n",
        "#### t-distributed Stochastic Neighbor Embedding (t-SNE)\n",
        "\n",
        "#### Uniform Manifold Approximation and Projection (UMAP)\n",
        "\n",
        "#### フィルター法\n",
        "\n",
        "#### ラッパー法\n",
        "\n",
        "#### 埋め込み法\n",
        "\n",
        "#### One-Class SVM\n",
        "\n",
        "#### 孤立フォレスト（Isolation Forest）\n",
        "\n",
        "#### Local Outlier Factor (LOF)\n",
        "\n"
      ],
      "metadata": {
        "id": "8QWDCFA6aqkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 入出力関係のあるデータの異常検知\n",
        "### 例\n",
        "製造工程における品質管理\n"
      ],
      "metadata": {
        "id": "NWg4xZztixiP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 時系列データの異常検知\n",
        "### 例\n",
        "サーバーのCPU使用率の異常検知\n"
      ],
      "metadata": {
        "id": "n1WuKOxxizq4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 変数間に関係があるデータの異常検知\n",
        "### 例\n",
        "クレジットカードの不正利用検知\n"
      ],
      "metadata": {
        "id": "42XygjKgi1od"
      }
    }
  ]
}