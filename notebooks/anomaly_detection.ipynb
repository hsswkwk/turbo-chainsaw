{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPXSKE2PEUeSkZUA4+P/PRH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hsswkwk/turbo-chainsaw/blob/feature-add-anomaly-detection/notebooks/anomaly_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 異常検知（Anomaly detection）\n",
        "標準的な状態や想定から逸脱しているデータや事象を特定する技術\n"
      ],
      "metadata": {
        "id": "PaP90VRh7wqm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 正規分布に従うデータの異常検知\n",
        "### 例\n",
        "製品の温度をセンサーで監視している場合の異常検知\n",
        "<br>\n",
        "<br>\n",
        "### 手法\n",
        "\n",
        "#### 3$\\sigma$法\n",
        "データが正規分布に従うと仮定し、平均値から標準偏差の3倍以上離れた値を異常値とみなす方法\n",
        "<br>\n",
        "#### マハラノビス・タグチ法\n",
        "データの各次元間の相関を考慮した距離尺度を用いて、平均値から離れた値を異常値とみなす方法\n",
        "<br>\n",
        "#### ホテリングの$T^2$法\n",
        "マハラノビス距離を拡張した手法で、データの平均値からのずれを検定統計量として用いて異常値を検出する方法\n",
        "<br>\n",
        "#### 密度比推定\n",
        "正常データと異常データの確率密度比を推定することで異常を検知する手法\n",
        "<br>"
      ],
      "metadata": {
        "id": "1dE3Gampih0k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.spatial.distance import mahalanobis\n",
        "from scipy.stats import f\n",
        "from sklearn.neighbors import KernelDensity\n",
        "\n",
        "\n",
        "### 3σ法\n",
        "# threshold: データの平均値から標準偏差の何倍離れたら異常値とみなすか\n",
        "def three_sigma_method(data, threshold=3):\n",
        "  mean = np.mean(data)\n",
        "  std = np.std(data)\n",
        "  lower_bound = mean - threshold * std\n",
        "  upper_bound = mean + threshold * std\n",
        "  if data.ndim == 1:\n",
        "    anomaly_indices = np.where((data < lower_bound) | (data > upper_bound))[0]\n",
        "  elif data.ndim >= 2:\n",
        "    anomaly_indices = np.where(np.any((data < lower_bound) | (data > upper_bound), axis=1))[0]\n",
        "  return data[anomaly_indices]\n",
        "\n",
        "### マハラノビス距離\n",
        "# マハラノビス距離: データの相関を考慮した距離尺度\n",
        "# threshold_percentile: マハラノビス距離の分布におけるパーセンタイル値を超える\n",
        "#                       距離を持つデータを異常値とみなす\n",
        "def mahalanobis_distance_method(data, threshold_percentile=99):\n",
        "  mean = np.mean(data, axis=0)\n",
        "  if data.ndim == 1:\n",
        "    std = np.std(data)\n",
        "\n",
        "    # 標準偏差が0の場合、ゼロ除算を避けるために微小な値を加算\n",
        "    if std == 0:\n",
        "      std = 1e-6\n",
        "\n",
        "    distances = np.abs((data - mean) / std)\n",
        "\n",
        "    # 異常値とみなす閾値を設定\n",
        "    threshold = np.percentile(distances, threshold_percentile)\n",
        "\n",
        "    # 閾値を超えるデータを異常値として検出\n",
        "    anomalies = data[distances > threshold]\n",
        "  elif data.ndim >= 2:\n",
        "    cov = np.cov(data, rowvar=False)\n",
        "    inv_cov = np.linalg.inv(cov)\n",
        "\n",
        "    distances = [mahalanobis(x, mean, inv_cov) for x in data]\n",
        "\n",
        "    # 異常値とみなす閾値を設定\n",
        "    threshold = np.percentile(distances, threshold_percentile)\n",
        "\n",
        "    # 閾値を超えるデータを異常値として検出\n",
        "    anomaly_indices = np.where(np.array(distances) > threshold)[0]\n",
        "    anomalies = data[anomaly_indices]\n",
        "  return anomalies\n",
        "\n",
        "### ホテリングのT^2法\n",
        "def hotelling_t2_method(data):\n",
        "  mean = np.mean(data, axis=0)\n",
        "  if data.ndim == 1:\n",
        "    var = np.var(data)\n",
        "    t2_values = [(x - mean)**2 / var for x in data]\n",
        "    # f.ppf: F分布のパーセント点関数\n",
        "    threshold = f.ppf(0.95, 1, len(data) - 1)  # 異常値とみなす閾値を設定\n",
        "    anomalies = [data[i] for i, t2 in enumerate(t2_values) if t2 > threshold]\n",
        "  elif data.ndim >= 2:\n",
        "    mean = np.mean(data, axis=0)\n",
        "    cov = np.cov(data, rowvar=False)\n",
        "    inv_cov = np.linalg.inv(cov)\n",
        "    t2_values = [mahalanobis(x, mean, inv_cov)**2 for x in data]\n",
        "    threshold = f.ppf(0.95, data.shape[1], data.shape[0] - data.shape[1] - 1)  # 異常値とみなす閾値を設定\n",
        "    anomaly_indices = np.where(np.array(t2_values) > threshold)[0]\n",
        "    anomalies = data[anomaly_indices]\n",
        "  return anomalies\n",
        "\n",
        "### KLIEPによる密度比推定\n",
        "# bandwidth: カーネル密度推定で使用されるカーネルの幅を制御するパラメータ\n",
        "#            大きいほど推定される確率密度関数は滑らかになる\n",
        "def kliep(normal_data, anomaly_data, bandwidth=0.1):\n",
        "  if normal_data.ndim == 1:\n",
        "    normal_data = normal_data.reshape(-1, 1)\n",
        "  if anomaly_data.ndim == 1:\n",
        "    anomaly_data = anomaly_data.reshape(-1, 1)\n",
        "  # カーネル密度推定でデータから確率密度関数を推定する\n",
        "  kde_normal = KernelDensity(bandwidth=bandwidth).fit(normal_data)\n",
        "  kde_anomaly = KernelDensity(bandwidth=bandwidth).fit(anomaly_data)\n",
        "  density_ratio = np.exp(kde_normal.score_samples(anomaly_data) - kde_anomaly.score_samples(anomaly_data))\n",
        "\n",
        "  # 1e-6 を加算してゼロ除算を防ぐ\n",
        "  density_ratio = np.maximum(density_ratio, 1e-6)\n",
        "\n",
        "  # 異常度の算出\n",
        "  anomaly_score = -np.log(density_ratio)\n",
        "\n",
        "  # 閾値の設定 (例: 異常度の95%点)\n",
        "  threshold = np.percentile(anomaly_score, 95)\n",
        "\n",
        "  # 異常検知\n",
        "  anomalies = anomaly_data[anomaly_score > threshold]\n",
        "  return anomalies\n",
        "\n",
        "\n",
        "print(\"1次元データの場合\")\n",
        "# 正常データ生成\n",
        "normal_data = np.random.normal(loc=25, scale=2, size=100)\n",
        "\n",
        "# 異常データ生成\n",
        "anomaly_data = np.random.normal(loc=25, scale=2, size=100)\n",
        "# 異常値の追加\n",
        "anomaly_indices = [10, 20, 30]  # 異常値のインデックス\n",
        "anomaly_values = [35, 15, 32]  # 異常値\n",
        "anomaly_data[anomaly_indices] = anomaly_values\n",
        "\n",
        "# 異常検知の実行\n",
        "anomalies_three_sigma_method = three_sigma_method(anomaly_data)\n",
        "anomalies_mahalanobis_distance_method = mahalanobis_distance_method(anomaly_data)\n",
        "anomalies_hotelling_t2_method = hotelling_t2_method(anomaly_data)\n",
        "anomalies_kliep = kliep(normal_data, anomaly_data)\n",
        "\n",
        "# 結果の出力\n",
        "print(\"================== 3σ法 =================\")\n",
        "print(\"異常値:\", anomalies_three_sigma_method)\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"===== マハラノビス距離による異常検知 =====\")\n",
        "print(\"異常値:\", anomalies_mahalanobis_distance_method)\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"===== ホテリングのT^2法による異常検知 ====\")\n",
        "print(\"異常値:\", anomalies_hotelling_t2_method)\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"======== 密度比推定による異常検知 ========\")\n",
        "print(\"異常値:\", anomalies_kliep)\n",
        "print(\"\\n\")\n",
        "\n",
        "\n",
        "\n",
        "print(\"2次元データの場合\")\n",
        "\n",
        "# 正常データ生成\n",
        "normal_data2d = np.random.multivariate_normal(mean=[0, 0], cov=[[1, 0.5], [0.5, 1]], size=100)\n",
        "\n",
        "# 異常データ生成\n",
        "anomaly_data2d = np.random.multivariate_normal(mean=[0, 0], cov=[[1, 0.5], [0.5, 1]], size=100)\n",
        "# 異常値の追加\n",
        "anomaly_indices = [10, 20, 30]  # 異常値のインデックス\n",
        "anomaly_values = [[3, 3], [-3, -3], [3, -3]]  # 異常値\n",
        "anomaly_data2d[anomaly_indices] = anomaly_values\n",
        "\n",
        "# 異常検知の実行\n",
        "anomalies_three_sigma_method_2d = three_sigma_method(anomaly_data2d, threshold=2)\n",
        "anomalies_mahalanobis_distance_method_2d = mahalanobis_distance_method(anomaly_data2d)\n",
        "anomalies_hotelling_t2_method_2d = hotelling_t2_method(anomaly_data2d)\n",
        "anomalies_kliep_2d = kliep(normal_data2d, anomaly_data2d, bandwidth=0.5)\n",
        "\n",
        "# 結果の出力\n",
        "print(\"================== 3σ法 =================\")\n",
        "print(\"異常値:\", anomalies_three_sigma_method_2d)\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"===== マハラノビス距離による異常検知 =====\")\n",
        "print(\"異常値:\", anomalies_mahalanobis_distance_method_2d)\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"===== ホテリングのT^2法による異常検知 ====\")\n",
        "print(\"異常値:\", anomalies_hotelling_t2_method_2d)\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"======== 密度比推定による異常検知 ========\")\n",
        "print(\"異常値:\", anomalies_kliep_2d)\n",
        "print(\"\\n\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VydvSi0Ejy_Z",
        "outputId": "02da6d21-dcde-47e9-9ff1-967bb907101c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1次元データの場合\n",
            "================== 3σ法 =================\n",
            "異常値: [35. 15.]\n",
            "\n",
            "\n",
            "===== マハラノビス距離による異常検知 =====\n",
            "異常値: [15.]\n",
            "\n",
            "\n",
            "===== ホテリングのT^2法による異常検知 ====\n",
            "異常値: [np.float64(35.0), np.float64(15.0), np.float64(32.0)]\n",
            "\n",
            "\n",
            "======== 密度比推定による異常検知 ========\n",
            "異常値: [[35.        ]\n",
            " [15.        ]\n",
            " [32.        ]\n",
            " [20.99479616]\n",
            " [20.89919911]]\n",
            "\n",
            "\n",
            "2次元データの場合\n",
            "================== 3σ法 =================\n",
            "異常値: [[ 2.13848135  2.45500825]\n",
            " [ 3.          3.        ]\n",
            " [-3.         -3.        ]\n",
            " [ 3.         -3.        ]\n",
            " [ 2.71516196  1.84843097]\n",
            " [ 0.98278229  3.05596083]\n",
            " [ 2.37145929  1.40984141]]\n",
            "\n",
            "\n",
            "===== マハラノビス距離による異常検知 =====\n",
            "異常値: [[ 3. -3.]]\n",
            "\n",
            "\n",
            "===== ホテリングのT^2法による異常検知 ====\n",
            "異常値: [[ 2.13848135  2.45500825]\n",
            " [ 3.          3.        ]\n",
            " [-3.         -3.        ]\n",
            " [-2.21439899 -0.31184462]\n",
            " [ 0.06048322 -1.88858858]\n",
            " [ 3.         -3.        ]\n",
            " [ 2.71516196  1.84843097]\n",
            " [-2.04126395  0.25096997]\n",
            " [-2.16449548 -1.45698025]\n",
            " [ 0.12847085  2.13699689]\n",
            " [ 1.30552164 -1.098316  ]\n",
            " [ 0.06310474 -1.86091794]\n",
            " [ 0.98278229  3.05596083]\n",
            " [ 2.37145929  1.40984141]]\n",
            "\n",
            "\n",
            "======== 密度比推定による異常検知 ========\n",
            "異常値: [[ 3.          3.        ]\n",
            " [-3.         -3.        ]\n",
            " [ 0.06048322 -1.88858858]\n",
            " [ 3.         -3.        ]\n",
            " [ 0.98278229  3.05596083]]\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 非正規データの異常検知\n",
        "### 例\n",
        "ウェブサイトへのアクセス数の異常検知\n",
        "<br>\n",
        "<br>\n",
        "### 手法\n",
        "\n",
        "#### ガンマ分布の当てはめ\n",
        "データがガンマ分布に従うと仮定し、その分布から大きく外れた値を異常値とみなす方法。データがガンマ分布に従う場合、高い精度で異常を検出できるが、従わない場合、精度が低下する。\n",
        "<br>\n",
        "\n",
        "#### カイ二乗分布への当てはめ\n",
        "データがカイ二乗分布に従うと仮定し、その分布から大きく外れた値を異常値とみなす方法。主に、特徴量の値が正の値を取り、歪んだ分布をしている場合に有効。<br>\n",
        "\n",
        "#### $k$近傍法\n",
        "各データポイントからk番目に近いデータポイントまでの距離を計算し、距離が大きいデータポイントを異常値と判定するアルゴリズム\n",
        "<br>\n",
        "\n",
        "#### $k$ means法\n",
        "データを複数のクラスタに分割し、どのクラスタにも属さないデータポイントを異常値と判定するアルゴリズム。\n",
        "<br>\n",
        "\n",
        "#### 混合ガウス分布モデル（Gaussian Mixture Model, GMM）\n",
        "データが複数のガウス分布（正規分布）の混合で表現できると仮定し、低確率なデータ点を異常値とみなす方法。\n",
        "<br>\n",
        "\n",
        "#### One-Class SVM\n",
        "正常データのみを用いて、正常データの領域を学習するアルゴリズム。 学習した領域から外れたデータは異常値と判定する。\n",
        "<br>\n",
        "\n",
        "#### 密度比推定\n",
        "正常データと異常データの確率密度比を推定することで異常を検知する手法。\n",
        "<br>\n",
        "\n",
        "#### 孤立フォレスト\n",
        "データポイントをランダムに分割していくことで、異常値を孤立させるアルゴリズム。\n",
        "<br>\n",
        "\n",
        "#### Local Outlier Factor (LOF)\n",
        "データポイントの局所的な密度を計算し、密度が低いデータポイントを異常値と判定するアルゴリズム。\n",
        "<br>\n",
        "\n"
      ],
      "metadata": {
        "id": "FMOqmjNLiot5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "\n",
        "\n",
        "def fitting_gamma_distribution(data):\n",
        "  # データにガンマ分布を当てはめる\n",
        "  shape, loc, scale = stats.gamma.fit(data)\n",
        "\n",
        "  # 異常度を計算する\n",
        "  anomaly_scores = 1 / stats.gamma.pdf(data, shape, loc, scale)\n",
        "\n",
        "  # 閾値を設定する\n",
        "  threshold = np.mean(anomaly_scores) + 3 * np.std(anomaly_scores)\n",
        "\n",
        "  # 異常値を検出する\n",
        "  anomalies = data[anomaly_scores > threshold]\n",
        "\n",
        "  return anomalies\n",
        "\n",
        "\n",
        "def fitting_chi_square_distribution(data):\n",
        "  # データにカイ二乗分布を当てはめる\n",
        "  df, loc, scale = stats.chi2.fit(data)\n",
        "\n",
        "  # 異常度を計算する (確率密度関数の逆数を使用)\n",
        "  anomaly_scores = 1 / stats.chi2.pdf(data, df, loc, scale)\n",
        "\n",
        "  # 閾値を設定する (上位5%点を閾値にする)\n",
        "  threshold = stats.chi2.ppf(0.95, df, loc, scale)\n",
        "\n",
        "  # 異常値を検出する\n",
        "  anomalies = data[anomaly_scores > threshold]\n",
        "\n",
        "  return anomalies\n",
        "\n",
        "\n",
        "def k_nearest_neighbors(data, k):\n",
        "  # k近傍の計算\n",
        "  knn = NearestNeighbors(n_neighbors=k)\n",
        "  knn.fit(data)\n",
        "  distances, indices = knn.kneighbors(data)\n",
        "\n",
        "  # 異常度の算出\n",
        "  anomaly_score = distances[:, -1]\n",
        "\n",
        "  # 異常値の判定\n",
        "  anomaly_index = np.where(anomaly_score > threshold)\n",
        "\n",
        "  # 異常値を検出する\n",
        "  anomalies = data[anomaly_index]\n",
        "\n",
        "  return anomalies\n",
        "\n",
        "\n",
        "def k_means(data, k):\n",
        "  # クラスタリングの実行\n",
        "  kmeans = KMeans(n_clusters=k)\n",
        "  kmeans.fit(data)\n",
        "  labels = kmeans.labels_\n",
        "\n",
        "  # 異常値の判定\n",
        "  anomaly_index = np.where(labels == -1)\n",
        "\n",
        "  # 異常値を検出する\n",
        "  anomalies = data[anomaly_index]\n",
        "\n",
        "  return anomalies\n",
        "\n",
        "\n",
        "def gaussian_mixture_model(data):\n",
        "  # GMMモデルの構築 (コンポーネント数は3)\n",
        "  gmm = GaussianMixture(n_components=3)\n",
        "\n",
        "  # データを用いてモデルを学習\n",
        "  gmm.fit(data)\n",
        "\n",
        "  # 各データ点の確率密度を計算\n",
        "  densities = gmm.score_samples(data)\n",
        "  # 異常度は確率密度の負の対数で表されることが多い\n",
        "  anomaly_scores = -densities\n",
        "\n",
        "  # 閾値を設定 (例：確率密度の下位5%点を閾値にする)\n",
        "  threshold = np.percentile(anomaly_scores, 5)\n",
        "\n",
        "  # 異常値を検出\n",
        "  anomalies = data[anomaly_scores > threshold]\n",
        "\n",
        "  return anomalies\n",
        "\n",
        "\n",
        "def one_class_svm(data):\n",
        "  # モデルの学習\n",
        "  model = OneClassSVM()\n",
        "  model.fit(data)\n",
        "\n",
        "  # 異常度の算出\n",
        "  anomaly_score = model.decision_function(data)\n",
        "\n",
        "  # 異常値の判定\n",
        "  anomaly_index = np.where(anomaly_score < 0)\n",
        "\n",
        "  # 異常値を検出する\n",
        "  anomalies = data[anomaly_index]\n",
        "\n",
        "  return anomalies\n",
        "\n",
        "\n",
        "def isolation_forest(data):\n",
        "  # モデルの学習\n",
        "  model = IsolationForest()\n",
        "  model.fit(data)\n",
        "\n",
        "  # 異常度の算出\n",
        "  anomaly_score = model.decision_function(data)\n",
        "\n",
        "  # 異常値の判定\n",
        "  anomaly_index = np.where(anomaly_score < 0)\n",
        "\n",
        "  # 異常値を検出する\n",
        "  anomalies = data[anomaly_index]\n",
        "\n",
        "  return anomalies\n",
        "\n",
        "\n",
        "def local_outlier_factor(data):\n",
        "  # モデルの学習\n",
        "  model = LocalOutlierFactor()\n",
        "  anomaly_score = model.fit_predict(data)\n",
        "\n",
        "  # 異常値の判定\n",
        "  anomaly_index = np.where(anomaly_score == -1)\n",
        "\n",
        "  # 異常値を検出する\n",
        "  anomalies = data[anomaly_index]\n",
        "\n",
        "  return anomalies\n",
        "\n",
        "\n",
        "def power_law_distribution(alpha, xmin, xmax, size):\n",
        "    \"\"\"べき乗則に従う乱数を生成する関数\n",
        "\n",
        "    Args:\n",
        "        alpha (float): べき乗則の指数\n",
        "        xmin (float): 最小値\n",
        "        xmax (float): 最大値\n",
        "        size (int): 生成する乱数の数\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: べき乗則に従う乱数\n",
        "    \"\"\"\n",
        "\n",
        "    u = np.random.uniform(size=size)\n",
        "    return (xmin ** (1 - alpha) + (xmax ** (1 - alpha) - xmin ** (1 - alpha)) * u) ** (1 / (1 - alpha))\n",
        "\n",
        "\n",
        "# 正常データの生成\n",
        "normal_data = power_law_distribution(alpha=2, xmin=1, xmax=100, size=1000)\n",
        "\n",
        "# 異常データの生成\n",
        "outliers = np.random.uniform(low=2*normal_data.max(), high=3*normal_data.max(), size=10)\n",
        "anomaly_data = np.concatenate([normal_data, outliers])\n",
        "\n",
        "all_data = np.concatenate([normal_data, anomaly_data])\n",
        "\n",
        "#### ガンマ分布の当てはめ\n",
        "anomalies = fitting_gamma_distribution(all_data)\n",
        "\n",
        "#### カイ二乗分布への当てはめ\n",
        "anomalies = fitting_chi_square_distribution(all_data)\n",
        "\n",
        "#### k近傍法\n",
        "anomalies = k_nearest_neighbors(all_data, k)\n",
        "\n",
        "#### k means法\n",
        "anomalies = k_means(all_data, k)\n",
        "\n",
        "#### 混合ガウス分布モデル（Gaussian Mixture Model, GMM）\n",
        "anomalies = gaussian_mixture_model(all_data)\n",
        "\n",
        "#### One-Class SVM\n",
        "anomalies = isolation_forest(all_data)\n",
        "\n",
        "#### 密度比推定\n",
        "anomalies = kliep(normal_data, anomaly_data)\n",
        "\n",
        "#### 孤立フォレスト (Isolation Forest)\n",
        "anomalies = isolation_forest(all_data)\n",
        "\n",
        "#### Local Outlier Factor (LOF)\n",
        "anomalies = local_outlier_factor(all_data)\n"
      ],
      "metadata": {
        "id": "aDeywrg2lfYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 不要次元のある次元データの異常検知\n",
        "### 例\n",
        "顧客の購買データを用いた異常検知\n"
      ],
      "metadata": {
        "id": "V4fWrW7RivcU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 入出力関係のあるデータの異常検知\n",
        "### 例\n",
        "製造工程における品質管理\n"
      ],
      "metadata": {
        "id": "NWg4xZztixiP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 時系列データの異常検知\n",
        "### 例\n",
        "サーバーのCPU使用率の異常検知\n"
      ],
      "metadata": {
        "id": "n1WuKOxxizq4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 変数間に関係があるデータの異常検知\n",
        "### 例\n",
        "クレジットカードの不正利用検知\n"
      ],
      "metadata": {
        "id": "42XygjKgi1od"
      }
    }
  ]
}