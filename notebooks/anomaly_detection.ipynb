{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNcQclc5msCpncgJaCv4H/x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hsswkwk/turbo-chainsaw/blob/feature-add-anomaly-detection/notebooks/anomaly_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 異常検知（Anomaly detection）\n",
        "標準的な状態や想定から逸脱しているデータや事象を特定する技術\n"
      ],
      "metadata": {
        "id": "PaP90VRh7wqm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 正規分布に従うデータの異常検知\n",
        "### 例\n",
        "製品の温度をセンサーで監視している場合の異常検知\n",
        "<br>\n",
        "<br>\n",
        "### 手法\n",
        "\n",
        "#### 3$\\sigma$法\n",
        "データが正規分布に従うと仮定し、平均値から標準偏差の3倍以上離れた値を異常値とみなす方法\n",
        "<br>\n",
        "#### マハラノビス・タグチ法\n",
        "データの各次元間の相関を考慮した距離尺度を用いて、平均値から離れた値を異常値とみなす方法\n",
        "<br>\n",
        "#### ホテリングの$T^2$法\n",
        "マハラノビス距離を拡張した手法で、データの平均値からのずれを検定統計量として用いて異常値を検出する方法\n",
        "<br>\n",
        "#### 密度比推定\n",
        "正常データと異常データの確率密度比を推定することで異常を検知する手法\n",
        "<br>"
      ],
      "metadata": {
        "id": "1dE3Gampih0k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.spatial.distance import mahalanobis\n",
        "from scipy.stats import f\n",
        "from sklearn.neighbors import KernelDensity\n",
        "\n",
        "\n",
        "### 3σ法\n",
        "# threshold: データの平均値から標準偏差の何倍離れたら異常値とみなすか\n",
        "def three_sigma_method(data, threshold=3):\n",
        "  mean = np.mean(data)\n",
        "  std = np.std(data)\n",
        "  lower_bound = mean - threshold * std\n",
        "  upper_bound = mean + threshold * std\n",
        "  if data.ndim == 1:\n",
        "    anomaly_indices = np.where((data < lower_bound) | (data > upper_bound))[0]\n",
        "  elif data.ndim >= 2:\n",
        "    anomaly_indices = np.where(np.any((data < lower_bound) | (data > upper_bound), axis=1))[0]\n",
        "  return data[anomaly_indices]\n",
        "\n",
        "### マハラノビス距離\n",
        "# マハラノビス距離: データの相関を考慮した距離尺度\n",
        "# threshold_percentile: マハラノビス距離の分布におけるパーセンタイル値を超える\n",
        "#                       距離を持つデータを異常値とみなす\n",
        "def mahalanobis_distance_method(data, threshold_percentile=99):\n",
        "  mean = np.mean(data, axis=0)\n",
        "  if data.ndim == 1:\n",
        "    std = np.std(data)\n",
        "\n",
        "    # 標準偏差が0の場合、ゼロ除算を避けるために微小な値を加算\n",
        "    if std == 0:\n",
        "      std = 1e-6\n",
        "\n",
        "    distances = np.abs((data - mean) / std)\n",
        "\n",
        "    # 異常値とみなす閾値を設定\n",
        "    threshold = np.percentile(distances, threshold_percentile)\n",
        "\n",
        "    # 閾値を超えるデータを異常値として検出\n",
        "    anomalies = data[distances > threshold]\n",
        "  elif data.ndim >= 2:\n",
        "    cov = np.cov(data, rowvar=False)\n",
        "    inv_cov = np.linalg.inv(cov)\n",
        "\n",
        "    distances = [mahalanobis(x, mean, inv_cov) for x in data]\n",
        "\n",
        "    # 異常値とみなす閾値を設定\n",
        "    threshold = np.percentile(distances, threshold_percentile)\n",
        "\n",
        "    # 閾値を超えるデータを異常値として検出\n",
        "    anomaly_indices = np.where(np.array(distances) > threshold)[0]\n",
        "    anomalies = data[anomaly_indices]\n",
        "  return anomalies\n",
        "\n",
        "### ホテリングのT^2法\n",
        "def hotelling_t2_method(data):\n",
        "  mean = np.mean(data, axis=0)\n",
        "  if data.ndim == 1:\n",
        "    var = np.var(data)\n",
        "    t2_values = [(x - mean)**2 / var for x in data]\n",
        "    # f.ppf: F分布のパーセント点関数\n",
        "    threshold = f.ppf(0.95, 1, len(data) - 1)  # 異常値とみなす閾値を設定\n",
        "    anomalies = [data[i] for i, t2 in enumerate(t2_values) if t2 > threshold]\n",
        "  elif data.ndim >= 2:\n",
        "    mean = np.mean(data, axis=0)\n",
        "    cov = np.cov(data, rowvar=False)\n",
        "    inv_cov = np.linalg.inv(cov)\n",
        "    t2_values = [mahalanobis(x, mean, inv_cov)**2 for x in data]\n",
        "    threshold = f.ppf(0.95, data.shape[1], data.shape[0] - data.shape[1] - 1)  # 異常値とみなす閾値を設定\n",
        "    anomaly_indices = np.where(np.array(t2_values) > threshold)[0]\n",
        "    anomalies = data[anomaly_indices]\n",
        "  return anomalies\n",
        "\n",
        "### KLIEPによる密度比推定\n",
        "# bandwidth: カーネル密度推定で使用されるカーネルの幅を制御するパラメータ\n",
        "#            大きいほど推定される確率密度関数は滑らかになる\n",
        "def kliep(normal_data, anomaly_data, bandwidth=0.1):\n",
        "  if normal_data.ndim == 1:\n",
        "    normal_data = normal_data.reshape(-1, 1)\n",
        "  if anomaly_data.ndim == 1:\n",
        "    anomaly_data = anomaly_data.reshape(-1, 1)\n",
        "  # カーネル密度推定でデータから確率密度関数を推定する\n",
        "  kde_normal = KernelDensity(bandwidth=bandwidth).fit(normal_data)\n",
        "  kde_anomaly = KernelDensity(bandwidth=bandwidth).fit(anomaly_data)\n",
        "  density_ratio = np.exp(kde_normal.score_samples(anomaly_data) - kde_anomaly.score_samples(anomaly_data))\n",
        "\n",
        "  # 1e-6 を加算してゼロ除算を防ぐ\n",
        "  density_ratio = np.maximum(density_ratio, 1e-6)\n",
        "\n",
        "  # 異常度の算出\n",
        "  anomaly_score = -np.log(density_ratio)\n",
        "\n",
        "  # 閾値の設定 (例: 異常度の95%点)\n",
        "  threshold = np.percentile(anomaly_score, 95)\n",
        "\n",
        "  # 異常検知\n",
        "  anomalies = anomaly_data[anomaly_score > threshold]\n",
        "  return anomalies\n",
        "\n",
        "\n",
        "print(\"1次元データの場合\")\n",
        "# 正常データ生成\n",
        "normal_data = np.random.normal(loc=25, scale=2, size=100)\n",
        "\n",
        "# 異常データ生成\n",
        "anomaly_data = np.random.normal(loc=25, scale=2, size=100)\n",
        "# 異常値の追加\n",
        "anomaly_indices = [10, 20, 30]  # 異常値のインデックス\n",
        "anomaly_values = [35, 15, 32]  # 異常値\n",
        "anomaly_data[anomaly_indices] = anomaly_values\n",
        "\n",
        "# 異常検知の実行\n",
        "anomalies_three_sigma_method = three_sigma_method(anomaly_data)\n",
        "anomalies_mahalanobis_distance_method = mahalanobis_distance_method(anomaly_data)\n",
        "anomalies_hotelling_t2_method = hotelling_t2_method(anomaly_data)\n",
        "anomalies_kliep = kliep(normal_data, anomaly_data)\n",
        "\n",
        "# 結果の出力\n",
        "print(\"================== 3σ法 =================\")\n",
        "print(\"異常値:\", anomalies_three_sigma_method)\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"===== マハラノビス距離による異常検知 =====\")\n",
        "print(\"異常値:\", anomalies_mahalanobis_distance_method)\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"===== ホテリングのT^2法による異常検知 ====\")\n",
        "print(\"異常値:\", anomalies_hotelling_t2_method)\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"======== 密度比推定による異常検知 ========\")\n",
        "print(\"異常値:\", anomalies_kliep)\n",
        "print(\"\\n\")\n",
        "\n",
        "\n",
        "\n",
        "print(\"2次元データの場合\")\n",
        "\n",
        "# 正常データ生成\n",
        "normal_data2d = np.random.multivariate_normal(mean=[0, 0], cov=[[1, 0.5], [0.5, 1]], size=100)\n",
        "\n",
        "# 異常データ生成\n",
        "anomaly_data2d = np.random.multivariate_normal(mean=[0, 0], cov=[[1, 0.5], [0.5, 1]], size=100)\n",
        "# 異常値の追加\n",
        "anomaly_indices = [10, 20, 30]  # 異常値のインデックス\n",
        "anomaly_values = [[3, 3], [-3, -3], [3, -3]]  # 異常値\n",
        "anomaly_data2d[anomaly_indices] = anomaly_values\n",
        "\n",
        "# 異常検知の実行\n",
        "anomalies_three_sigma_method_2d = three_sigma_method(anomaly_data2d, threshold=2)\n",
        "anomalies_mahalanobis_distance_method_2d = mahalanobis_distance_method(anomaly_data2d)\n",
        "anomalies_hotelling_t2_method_2d = hotelling_t2_method(anomaly_data2d)\n",
        "anomalies_kliep_2d = kliep(normal_data2d, anomaly_data2d, bandwidth=0.5)\n",
        "\n",
        "# 結果の出力\n",
        "print(\"================== 3σ法 =================\")\n",
        "print(\"異常値:\", anomalies_three_sigma_method_2d)\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"===== マハラノビス距離による異常検知 =====\")\n",
        "print(\"異常値:\", anomalies_mahalanobis_distance_method_2d)\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"===== ホテリングのT^2法による異常検知 ====\")\n",
        "print(\"異常値:\", anomalies_hotelling_t2_method_2d)\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"======== 密度比推定による異常検知 ========\")\n",
        "print(\"異常値:\", anomalies_kliep_2d)\n",
        "print(\"\\n\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VydvSi0Ejy_Z",
        "outputId": "5544390a-62ac-42e1-daf0-ae443fbf8b17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1次元データの場合\n",
            "================== 3σ法 =================\n",
            "異常値: [35. 15.]\n",
            "\n",
            "\n",
            "===== マハラノビス距離による異常検知 =====\n",
            "異常値: [15.]\n",
            "\n",
            "\n",
            "===== ホテリングのT^2法による異常検知 ====\n",
            "異常値: [np.float64(35.0), np.float64(15.0), np.float64(32.0), np.float64(30.447814911192673)]\n",
            "\n",
            "\n",
            "======== 密度比推定による異常検知 ========\n",
            "異常値: [[35.        ]\n",
            " [15.        ]\n",
            " [32.        ]\n",
            " [29.19206427]\n",
            " [29.39143387]]\n",
            "\n",
            "\n",
            "2次元データの場合\n",
            "================== 3σ法 =================\n",
            "異常値: [[ 3.          3.        ]\n",
            " [-3.         -3.        ]\n",
            " [ 3.         -3.        ]\n",
            " [ 2.32858487  0.49744657]\n",
            " [ 2.32936571  1.22094773]\n",
            " [ 0.12111887 -2.26488035]\n",
            " [-2.32685181 -0.79094218]\n",
            " [ 0.03168927 -2.88578599]]\n",
            "\n",
            "\n",
            "===== マハラノビス距離による異常検知 =====\n",
            "異常値: [[ 3. -3.]]\n",
            "\n",
            "\n",
            "===== ホテリングのT^2法による異常検知 ====\n",
            "異常値: [[ 1.60702013 -0.8210482 ]\n",
            " [ 3.          3.        ]\n",
            " [-3.         -3.        ]\n",
            " [ 1.61764653 -0.84035206]\n",
            " [ 3.         -3.        ]\n",
            " [ 2.32858487  0.49744657]\n",
            " [ 1.87311897  2.15242828]\n",
            " [ 2.32936571  1.22094773]\n",
            " [-1.1885142  -1.98038375]\n",
            " [ 0.12111887 -2.26488035]\n",
            " [ 0.89669023  2.0717062 ]\n",
            " [-2.32685181 -0.79094218]\n",
            " [-1.75253046 -1.68453723]\n",
            " [-2.00156022 -1.20235715]\n",
            " [-0.80805454  1.48752757]\n",
            " [ 0.03168927 -2.88578599]\n",
            " [ 0.46614451  1.99703035]]\n",
            "\n",
            "\n",
            "======== 密度比推定による異常検知 ========\n",
            "異常値: [[ 3.          3.        ]\n",
            " [-3.         -3.        ]\n",
            " [ 3.         -3.        ]\n",
            " [ 2.32858487  0.49744657]\n",
            " [ 0.03168927 -2.88578599]]\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 非正規データの異常検知\n",
        "### 例\n",
        "ウェブサイトへのアクセス数の異常検知\n",
        "<br>\n",
        "<br>\n",
        "### 手法\n",
        "\n",
        "#### ガンマ分布の当てはめ\n",
        "データがガンマ分布に従うと仮定し、その分布から大きく外れた値を異常値とみなす方法。データがガンマ分布に従う場合、高い精度で異常を検出できるが、従わない場合、精度が低下する。\n",
        "<br>\n",
        "\n",
        "#### カイ二乗分布への当てはめ\n",
        "データがカイ二乗分布に従うと仮定し、その分布から大きく外れた値を異常値とみなす方法。主に、特徴量の値が正の値を取り、歪んだ分布をしている場合に有効。<br>\n",
        "<br>\n",
        "\n",
        "#### $k$近傍法\n",
        "各データポイントからk番目に近いデータポイントまでの距離を計算し、距離が大きいデータポイントを異常値と判定するアルゴリズム。$k$はデータセットのサイズの平方根よりも小さい奇数にすると良い。\n",
        "<br>\n",
        "\n",
        "#### $k$ means法\n",
        "データを複数のクラスタに分割し、どのクラスタにも属さないデータポイントを異常値と判定するアルゴリズム。$k$を推定する手法としてエルボー法やシルエット分析などがある。\n",
        "<br>\n",
        "\n",
        "#### 混合ガウス分布モデル（Gaussian Mixture Model, GMM）\n",
        "データが複数のガウス分布（正規分布）の混合で表現できると仮定し、低確率なデータ点を異常値とみなす方法。\n",
        "<br>\n",
        "\n",
        "#### One-Class SVM\n",
        "正常データのみを用いて、正常データの領域を学習するアルゴリズム。 学習した領域から外れたデータは異常値と判定する。\n",
        "<br>\n",
        "\n",
        "#### [密度比推定](#scrollTo=1dE3Gampih0k&line=1&uniqifier=1)\n",
        "<br>\n",
        "\n",
        "#### 孤立フォレスト（Isolation Forest）\n",
        "データポイントをランダムに分割していくことで、異常値を孤立させるアルゴリズム。\n",
        "<br>\n",
        "\n",
        "#### Local Outlier Factor (LOF)\n",
        "データポイントの局所的な密度を計算し、密度が低いデータポイントを異常値と判定するアルゴリズム。\n",
        "<br>\n",
        "\n"
      ],
      "metadata": {
        "id": "FMOqmjNLiot5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "\n",
        "\n",
        "def fitting_gamma_distribution(data):\n",
        "  # データにガンマ分布を当てはめる\n",
        "  shape, loc, scale = stats.gamma.fit(data)\n",
        "\n",
        "  # 異常度を計算する\n",
        "  anomaly_scores = 1 / stats.gamma.pdf(data, shape, loc, scale)\n",
        "\n",
        "  # 閾値を設定する\n",
        "  threshold = np.mean(anomaly_scores) + 3 * np.std(anomaly_scores)\n",
        "\n",
        "  # 異常値を検出する\n",
        "  anomalies = data[anomaly_scores > threshold]\n",
        "\n",
        "  return anomalies\n",
        "\n",
        "\n",
        "def fitting_chi_square_distribution(data):\n",
        "  # データにカイ二乗分布を当てはめる\n",
        "  df, loc, scale = stats.chi2.fit(data)\n",
        "\n",
        "  # 異常度を計算する (確率密度関数の逆数を使用)\n",
        "  anomaly_scores = 1 / stats.chi2.pdf(data, df, loc, scale)\n",
        "\n",
        "  # 閾値を設定する (上位1%点を閾値にする)\n",
        "  threshold = stats.chi2.ppf(0.99, df, loc, scale)\n",
        "\n",
        "  # 異常値を検出する\n",
        "  anomalies = data[anomaly_scores > threshold]\n",
        "\n",
        "  return anomalies\n",
        "\n",
        "\n",
        "def k_nearest_neighbors(data, k):\n",
        "  data = data.reshape(-1, 1)\n",
        "\n",
        "  # k近傍の計算\n",
        "  knn = NearestNeighbors(n_neighbors=k)\n",
        "  knn.fit(data)\n",
        "  distances, indices = knn.kneighbors(data)\n",
        "\n",
        "  # 異常度の算出\n",
        "  anomaly_score = distances[:, -1]\n",
        "  threshold = np.percentile(anomaly_score, 99)\n",
        "\n",
        "  # 異常値の判定\n",
        "  anomaly_index = np.where(anomaly_score > threshold)\n",
        "\n",
        "  # 異常値を検出する\n",
        "  anomalies = data[anomaly_index]\n",
        "\n",
        "  return anomalies\n",
        "\n",
        "\n",
        "def k_means(data, k):\n",
        "  data = data.reshape(-1, 1)\n",
        "\n",
        "  # クラスタリングの実行\n",
        "  kmeans = KMeans(n_clusters=k)\n",
        "  kmeans.fit(data)\n",
        "  labels = kmeans.labels_\n",
        "\n",
        "  # 異常値の判定\n",
        "  anomaly_index = np.where(labels == -1)\n",
        "\n",
        "  # 異常値を検出する\n",
        "  anomalies = data[anomaly_index]\n",
        "\n",
        "  return anomalies\n",
        "\n",
        "\n",
        "def gaussian_mixture_model(data):\n",
        "  data = data.reshape(-1, 1)\n",
        "\n",
        "  # GMMモデルの構築 (コンポーネント数は3)\n",
        "  gmm = GaussianMixture(n_components=3)\n",
        "\n",
        "  # データを用いてモデルを学習\n",
        "  gmm.fit(data)\n",
        "\n",
        "  # 各データ点の確率密度を計算\n",
        "  densities = gmm.score_samples(data)\n",
        "  # 異常度は確率密度の負の対数で表されることが多い\n",
        "  anomaly_scores = -densities\n",
        "\n",
        "  # 閾値を設定 (例：確率密度の下位5%点を閾値にする)\n",
        "  threshold = np.percentile(anomaly_scores, 5)\n",
        "\n",
        "  # 異常値を検出\n",
        "  anomalies = data[anomaly_scores > threshold]\n",
        "\n",
        "  return anomalies\n",
        "\n",
        "\n",
        "def one_class_svm(data):\n",
        "  data = data.reshape(-1, 1)\n",
        "\n",
        "  # モデルの学習\n",
        "  model = OneClassSVM()\n",
        "  model.fit(data)\n",
        "\n",
        "  # 異常度の算出\n",
        "  anomaly_score = model.decision_function(data)\n",
        "\n",
        "  # 異常値の判定\n",
        "  anomaly_index = np.where(anomaly_score < -850)\n",
        "\n",
        "  # 異常値を検出する\n",
        "  anomalies = data[anomaly_index]\n",
        "\n",
        "  return anomalies\n",
        "\n",
        "\n",
        "def isolation_forest(data):\n",
        "  data = data.reshape(-1, 1)\n",
        "\n",
        "  # モデルの学習\n",
        "  model = IsolationForest()\n",
        "  model.fit(data)\n",
        "\n",
        "  # 異常度の算出\n",
        "  anomaly_score = model.decision_function(data)\n",
        "\n",
        "  # 異常値の判定\n",
        "  anomaly_index = np.where(anomaly_score < -0.35)\n",
        "\n",
        "  # 異常値を検出する\n",
        "  anomalies = data[anomaly_index]\n",
        "\n",
        "  return anomalies\n",
        "\n",
        "\n",
        "def local_outlier_factor(data):\n",
        "  data = data.reshape(-1, 1)\n",
        "\n",
        "  # モデルの学習\n",
        "  model = LocalOutlierFactor()\n",
        "  anomaly_score = model.fit_predict(data)\n",
        "\n",
        "  # 異常値の判定\n",
        "  anomaly_index = np.where(anomaly_score == -1)\n",
        "\n",
        "  # 異常値を検出する\n",
        "  anomalies = data[anomaly_index]\n",
        "\n",
        "  return anomalies\n",
        "\n",
        "\n",
        "def power_law_distribution(alpha, xmin, xmax, size):\n",
        "    \"\"\"べき乗則に従う乱数を生成する関数\n",
        "\n",
        "    Args:\n",
        "        alpha (float): べき乗則の指数\n",
        "        xmin (float): 最小値\n",
        "        xmax (float): 最大値\n",
        "        size (int): 生成する乱数の数\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: べき乗則に従う乱数\n",
        "    \"\"\"\n",
        "\n",
        "    u = np.random.uniform(size=size)\n",
        "    return (xmin ** (1 - alpha) + (xmax ** (1 - alpha) - xmin ** (1 - alpha)) * u) ** (1 / (1 - alpha))\n",
        "\n",
        "\n",
        "# 正常データの生成\n",
        "normal_data = power_law_distribution(alpha=2, xmin=1, xmax=100, size=1000)\n",
        "\n",
        "# 異常データの生成\n",
        "outliers = np.random.uniform(low=2*normal_data.max(), high=3*normal_data.max(), size=10)\n",
        "anomaly_data = np.concatenate([normal_data, outliers])\n",
        "\n",
        "all_data = np.concatenate([normal_data, anomaly_data])\n",
        "\n",
        "#### ガンマ分布の当てはめ\n",
        "anomalies = fitting_gamma_distribution(all_data)\n",
        "print(\"============ ガンマ分布の当てはめ ==========\")\n",
        "print(\"異常値:\", anomalies)\n",
        "print(\"\\n\")\n",
        "\n",
        "#### カイ二乗分布への当てはめ\n",
        "anomalies = fitting_chi_square_distribution(all_data)\n",
        "print(\"========= カイ二乗分布への当てはめ =========\")\n",
        "print(\"異常値:\", anomalies)\n",
        "print(\"\\n\")\n",
        "\n",
        "#### k近傍法\n",
        "anomalies = k_nearest_neighbors(all_data, 5)\n",
        "print(\"================== k近傍法 =================\")\n",
        "print(\"異常値:\", anomalies)\n",
        "print(\"\\n\")\n",
        "\n",
        "#### k means法\n",
        "anomalies = k_means(all_data, 3)\n",
        "print(\"================= k means法 ================\")\n",
        "print(\"異常値:\", anomalies)\n",
        "print(\"\\n\")\n",
        "\n",
        "#### 混合ガウス分布モデル（Gaussian Mixture Model, GMM）\n",
        "anomalies = gaussian_mixture_model(all_data)\n",
        "print(\"============ 混合ガウス分布モデル ==========\")\n",
        "print(\"異常値:\", anomalies)\n",
        "print(\"\\n\")\n",
        "\n",
        "#### One-Class SVM\n",
        "anomalies = one_class_svm(all_data)\n",
        "print(\"=============== One-Class SVM ==============\")\n",
        "print(\"異常値:\", anomalies)\n",
        "print(\"\\n\")\n",
        "\n",
        "#### 密度比推定\n",
        "anomalies = kliep(normal_data, anomaly_data)\n",
        "print(\"================ 密度比推定 ================\")\n",
        "print(\"異常値:\", anomalies)\n",
        "print(\"\\n\")\n",
        "\n",
        "#### 孤立フォレスト (Isolation Forest)\n",
        "anomalies = isolation_forest(all_data)\n",
        "print(\"=============== 孤立フォレスト ==============\")\n",
        "print(\"異常値:\", anomalies)\n",
        "print(\"\\n\")\n",
        "\n",
        "#### Local Outlier Factor (LOF)\n",
        "anomalies = local_outlier_factor(all_data)\n",
        "print(\"============ Local Outlier Factor ===========\")\n",
        "print(\"異常値:\", anomalies)\n",
        "print(\"\\n\")\n"
      ],
      "metadata": {
        "id": "aDeywrg2lfYP",
        "outputId": "caef074e-3acc-456c-f7c1-c825a148aef7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============ ガンマ分布の当てはめ ==========\n",
            "異常値: [252.6739005  265.24192644 264.72299405 261.70242498 252.20111675\n",
            " 252.8578622  233.36639385]\n",
            "\n",
            "\n",
            "========= カイ二乗分布への当てはめ =========\n",
            "異常値: [ 18.04223324  85.45820386  17.88613053  19.04766031  81.27267206\n",
            "  27.69276714  36.93090836  29.89528865  85.48952257  29.88776521\n",
            "  23.72334909  88.91261248  41.25528155  33.89099858  19.08745492\n",
            "  40.01586574  68.7023943   18.48283269  28.44140521  42.99152305\n",
            "  39.55338805  34.63524389  28.71018742  27.07949404  33.77646093\n",
            "  33.5378116   88.46437426  46.30375979  18.41907818  35.57286511\n",
            "  33.66937523  46.16750412  47.71117289  31.92887339  31.76977109\n",
            "  18.75554805  46.71739721  18.1821352   20.70647844  86.5325949\n",
            "  22.06609245  68.37066397  39.08098898  23.82472231  18.70462098\n",
            "  20.08284469  32.43337712  28.03608279  25.05409646  53.43934297\n",
            "  76.24829077  67.08249628  28.70085922  20.52962857  18.04223324\n",
            "  85.45820386  17.88613053  19.04766031  81.27267206  27.69276714\n",
            "  36.93090836  29.89528865  85.48952257  29.88776521  23.72334909\n",
            "  88.91261248  41.25528155  33.89099858  19.08745492  40.01586574\n",
            "  68.7023943   18.48283269  28.44140521  42.99152305  39.55338805\n",
            "  34.63524389  28.71018742  27.07949404  33.77646093  33.5378116\n",
            "  88.46437426  46.30375979  18.41907818  35.57286511  33.66937523\n",
            "  46.16750412  47.71117289  31.92887339  31.76977109  18.75554805\n",
            "  46.71739721  18.1821352   20.70647844  86.5325949   22.06609245\n",
            "  68.37066397  39.08098898  23.82472231  18.70462098  20.08284469\n",
            "  32.43337712  28.03608279  25.05409646  53.43934297  76.24829077\n",
            "  67.08249628  28.70085922  20.52962857 252.6739005  265.24192644\n",
            " 264.72299405 208.04168272 261.70242498 214.14511469 252.20111675\n",
            " 252.8578622  213.65400282 233.36639385]\n",
            "\n",
            "\n",
            "================== k近傍法 =================\n",
            "異常値: [[ 81.27267206]\n",
            " [ 88.91261248]\n",
            " [ 42.99152305]\n",
            " [ 53.43934297]\n",
            " [ 76.24829077]\n",
            " [ 81.27267206]\n",
            " [ 88.91261248]\n",
            " [ 42.99152305]\n",
            " [ 53.43934297]\n",
            " [ 76.24829077]\n",
            " [252.6739005 ]\n",
            " [265.24192644]\n",
            " [264.72299405]\n",
            " [208.04168272]\n",
            " [261.70242498]\n",
            " [214.14511469]\n",
            " [252.20111675]\n",
            " [252.8578622 ]\n",
            " [213.65400282]\n",
            " [233.36639385]]\n",
            "\n",
            "\n",
            "================= k means法 ================\n",
            "異常値: []\n",
            "\n",
            "\n",
            "============ 混合ガウス分布モデル ==========\n",
            "異常値: [[  1.08287371]\n",
            " [  1.78245718]\n",
            " [  1.40095024]\n",
            " ...\n",
            " [252.8578622 ]\n",
            " [213.65400282]\n",
            " [233.36639385]]\n",
            "\n",
            "\n",
            "=============== One-Class SVM ==============\n",
            "異常値: [[252.6739005 ]\n",
            " [265.24192644]\n",
            " [264.72299405]\n",
            " [208.04168272]\n",
            " [261.70242498]\n",
            " [214.14511469]\n",
            " [252.20111675]\n",
            " [252.8578622 ]\n",
            " [213.65400282]\n",
            " [233.36639385]]\n",
            "\n",
            "\n",
            "================ 密度比推定 ================\n",
            "異常値: [[  1.75658465]\n",
            " [  1.19604552]\n",
            " [  1.96042269]\n",
            " [  1.73851489]\n",
            " [  1.19714164]\n",
            " [  1.30516316]\n",
            " [  1.48683324]\n",
            " [  1.22684672]\n",
            " [  1.03086006]\n",
            " [  1.52144107]\n",
            " [  1.43492066]\n",
            " [  1.44209848]\n",
            " [  1.39518659]\n",
            " [  1.70797133]\n",
            " [  1.19951878]\n",
            " [  1.40817236]\n",
            " [  1.8915806 ]\n",
            " [  1.50272382]\n",
            " [  1.56467876]\n",
            " [  1.34988373]\n",
            " [  1.14332889]\n",
            " [  1.45815137]\n",
            " [  1.22743721]\n",
            " [  1.62173145]\n",
            " [  1.89234089]\n",
            " [  1.76610826]\n",
            " [  1.00988483]\n",
            " [  4.33517242]\n",
            " [  1.36684254]\n",
            " [  1.84598777]\n",
            " [  1.05987711]\n",
            " [  1.33123259]\n",
            " [  1.22993405]\n",
            " [  1.13799091]\n",
            " [252.6739005 ]\n",
            " [265.24192644]\n",
            " [264.72299405]\n",
            " [208.04168272]\n",
            " [261.70242498]\n",
            " [214.14511469]\n",
            " [252.20111675]\n",
            " [252.8578622 ]\n",
            " [213.65400282]\n",
            " [233.36639385]]\n",
            "\n",
            "\n",
            "=============== 孤立フォレスト ==============\n",
            "異常値: [[252.6739005 ]\n",
            " [265.24192644]\n",
            " [264.72299405]\n",
            " [208.04168272]\n",
            " [261.70242498]\n",
            " [214.14511469]\n",
            " [252.20111675]\n",
            " [252.8578622 ]\n",
            " [213.65400282]\n",
            " [233.36639385]]\n",
            "\n",
            "\n",
            "============ Local Outlier Factor ===========\n",
            "異常値: [[ 20.70647844]\n",
            " [ 22.06609245]\n",
            " [ 53.43934297]\n",
            " [  1.00133729]\n",
            " [ 20.70647844]\n",
            " [ 22.06609245]\n",
            " [ 53.43934297]\n",
            " [  1.00133729]\n",
            " [252.6739005 ]\n",
            " [265.24192644]\n",
            " [264.72299405]\n",
            " [208.04168272]\n",
            " [261.70242498]\n",
            " [214.14511469]\n",
            " [252.20111675]\n",
            " [252.8578622 ]\n",
            " [213.65400282]\n",
            " [233.36639385]]\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 不要次元のある次元データの異常検知\n",
        "### 例\n",
        "顧客の購買データを用いた異常検知\n",
        "<br>\n",
        "<br>\n",
        "### 次元削減を用いる手法\n",
        "\n",
        "#### 主成分分析（PCA）\n",
        "データの分散を最大化するように新しい軸を定義し、その軸にデータを射影することで次元を削減する手法。異常なデータ点は、主成分空間において、正常なデータ点から離れた位置に配置される傾向がある。\n",
        "<br>\n",
        "\n",
        "#### 確率的主成分分析（Probabilistic PCA）\n",
        "PCAを確率モデルとして拡張した手法。データにノイズが含まれる場合に、よりロバストな結果が得られる。（＝データにノイズや外れ値が含まれていても、分析結果が大きく影響を受けにくい）\n",
        "<br>\n",
        "\n",
        "#### カーネル主成分分析（Kernel PCA）\n",
        "非線形な関係を持つデータに対して、カーネル関数を使用して高次元空間に写像し、その空間でPCAを行うことで次元を削減する手法。\n",
        "<br>\n",
        "\n",
        "#### 因子分析（Factor Analysis）\n",
        "観測変数の背後にある潜在変数を推定し、それらを用いて次元を削減する手法。\n",
        "<br>\n",
        "\n",
        "#### 独立成分分析（ICA）\n",
        "観測変数を、統計的に独立な成分に分解することで次元を削減する手法。\n",
        "<br>\n",
        "\n",
        "#### t-distributed Stochastic Neighbor Embedding (t-SNE)\n",
        "高次元データを低次元空間に埋め込む際に、データの局所的な構造を保持する様に設計された手法。異常検知では、正常なデータ点と異常なデータ点が、低次元空間において明確に分離されることが期待する。\n",
        "<br>\n",
        "\n",
        "#### Uniform Manifold Approximation and Projection (UMAP)\n",
        "高次元データを低次元空間に埋め込む際に、データのトポロジカルな構造を保持する様に設計された手法です。t-SNEと同様に、異常検知では、正常なデータ点と異常なデータ点が、低次元空間において明確に分離されることを期待する。\n",
        "<br>\n",
        "\n",
        "### 特徴量選択を用いる手法\n",
        "#### フィルター法\n",
        "各特徴量と目的変数の間の相関や相互情報量などを用いて、重要度の低い特徴量を除去する手法。\n",
        "<br>\n",
        "\n",
        "#### ラッパー法\n",
        "特徴量のサブセットを選択し、そのサブセットを用いて学習したモデルの性能を評価することで、最適な特徴量を選択する手法。\n",
        "<br>\n",
        "\n",
        "#### 埋め込み法\n",
        "モデルの学習過程で特徴量選択を行う手法。LASSO回帰や決定木などが用いられる。\n",
        "<br>\n",
        "\n",
        "### その他の手法\n",
        "#### [One-Class SVM](#scrollTo=FMOqmjNLiot5&line=33&uniqifier=1)\n",
        "<br>\n",
        "\n",
        "#### [孤立フォレスト（Isolation Forest）](#scrollTo=FMOqmjNLiot5&line=33&uniqifier=1)\n",
        "<br>\n",
        "\n",
        "#### [Local Outlier Factor (LOF)](#scrollTo=FMOqmjNLiot5&line=33&uniqifier=1)\n",
        "<br>\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "V4fWrW7RivcU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install umap-learn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.decomposition import KernelPCA\n",
        "from sklearn.decomposition import FactorAnalysis\n",
        "from sklearn.decomposition import FastICA\n",
        "from sklearn.manifold import TSNE\n",
        "import umap\n",
        "from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_classif, mutual_info_classif, RFE\n",
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "\n",
        "#### 正常データの生成\n",
        "# 顧客ID、年齢、性別、購入金額、購入頻度、不要な次元（ランダムな数値）を生成\n",
        "num_customers = 100  # 顧客数\n",
        "customer_ids = np.arange(1, num_customers + 1)\n",
        "ages = np.random.randint(20, 60, size=num_customers)\n",
        "genders = np.random.choice(['Male', 'Female'], size=num_customers)\n",
        "purchase_amounts = np.random.randint(100, 10000, size=num_customers)\n",
        "purchase_frequencies = np.random.randint(1, 10, size=num_customers)\n",
        "unnecessary_dimension = np.random.rand(num_customers)\n",
        "\n",
        "# データフレームを作成\n",
        "normal_data = pd.DataFrame({\n",
        "  'CustomerID': customer_ids,\n",
        "  'Age': ages,\n",
        "  'Gender': genders,\n",
        "  'PurchaseAmount': purchase_amounts,\n",
        "  'PurchaseFrequency': purchase_frequencies,\n",
        "  'UnnecessaryDimension': unnecessary_dimension  # 不要な次元\n",
        "})\n",
        "\n",
        "#### 異常データの生成\n",
        "anomaly_data = normal_data.copy()\n",
        "\n",
        "# 異常値を挿入するインデックスをランダムに選択\n",
        "# 例：顧客の10%を異常値とする\n",
        "anomaly_indices = np.random.choice(anomaly_data.index, size=int(num_customers * 0.1), replace=False)\n",
        "\n",
        "# 選択したインデックスのデータに異常値を代入\n",
        "# 例：PurchaseAmountを極端に大きくする\n",
        "anomaly_data.loc[anomaly_indices, 'PurchaseAmount'] = anomaly_data.loc[anomaly_indices, 'PurchaseAmount'] * np.random.uniform(100, 200, size=len(anomaly_indices))\n",
        "\n",
        "\n",
        "def pca(normal_data, anomaly_data):\n",
        "  # 不要な次元を除外\n",
        "  features = ['Age', 'PurchaseAmount', 'PurchaseFrequency', 'UnnecessaryDimension']  # Genderはカテゴリカルデータなので除外\n",
        "  X_normal = normal_data[features].values\n",
        "  X_anomaly = anomaly_data[features].values\n",
        "\n",
        "  # データを標準化\n",
        "  scaler = StandardScaler()\n",
        "  X_normal_scaled = scaler.fit_transform(X_normal)\n",
        "  X_anomaly_scaled = scaler.transform(X_anomaly)\n",
        "\n",
        "  # PCAモデルを学習\n",
        "  pca = PCA(n_components=2)  # 次元数を2に削減\n",
        "  pca.fit(X_normal_scaled)  # 正常データでモデルを学習\n",
        "\n",
        "  # 異常度を計算\n",
        "  normal_scores = pca.transform(X_normal_scaled)\n",
        "  anomaly_scores = pca.transform(X_anomaly_scaled)\n",
        "\n",
        "  # 再構成誤差を異常度として使用\n",
        "  reconstruction_error_normal = np.sum(np.square(X_normal_scaled - pca.inverse_transform(normal_scores)), axis=1)\n",
        "  reconstruction_error_anomaly = np.sum(np.square(X_anomaly_scaled - pca.inverse_transform(anomaly_scores)), axis=1)\n",
        "\n",
        "  # 閾値を設定 (例: 再構成誤差の95%点)\n",
        "  threshold = np.percentile(reconstruction_error_normal, 95)\n",
        "\n",
        "  # 異常を検出\n",
        "  anomalies = anomaly_data[reconstruction_error_anomaly > threshold]\n",
        "\n",
        "  # 整数で表示\n",
        "  anomalies['PurchaseAmount'] = anomalies['PurchaseAmount'].astype(int)\n",
        "\n",
        "  return anomalies\n",
        "\n",
        "\n",
        "def probabilistic_pca(normal_data, anomaly_data):\n",
        "  # 不要な次元を除外\n",
        "  features = ['Age', 'PurchaseAmount', 'PurchaseFrequency', 'UnnecessaryDimension']  # Genderはカテゴリカルデータなので除外\n",
        "  X_normal = normal_data[features].values\n",
        "  X_anomaly = anomaly_data[features].values\n",
        "\n",
        "  # データを標準化\n",
        "  scaler = StandardScaler()\n",
        "  X_normal_scaled = scaler.fit_transform(X_normal)\n",
        "  X_anomaly_scaled = scaler.transform(X_anomaly)\n",
        "\n",
        "  # 確率的PCAモデルを学習\n",
        "  ppca = PCA(n_components=2, svd_solver='full')  # 次元数を2に削減, svd_solver='full'で確率的PCAを指定\n",
        "  ppca.fit(X_normal_scaled)  # 正常データでモデルを学習\n",
        "\n",
        "  # 正常データの平均対数尤度を計算\n",
        "  normal_scores = -ppca.score_samples(X_normal_scaled)\n",
        "\n",
        "  # 異常度を計算 (平均対数尤度)\n",
        "  anomaly_scores = -ppca.score_samples(X_anomaly_scaled)\n",
        "\n",
        "  # 閾値を設定 (例: 平均対数尤度の5%点)\n",
        "  threshold = np.percentile(normal_scores, 95)\n",
        "\n",
        "  # 異常を検出\n",
        "  anomalies = anomaly_data[anomaly_scores > threshold]  # 平均対数尤度が低いデータが異常\n",
        "\n",
        "  # 整数で表示\n",
        "  anomalies['PurchaseAmount'] = anomalies['PurchaseAmount'].astype(int)\n",
        "\n",
        "  return anomalies\n",
        "\n",
        "\n",
        "def kernel_pca(normal_data, anomaly_data):\n",
        "  # 不要な次元を除外\n",
        "  features = ['Age', 'PurchaseAmount', 'PurchaseFrequency', 'UnnecessaryDimension']  # Genderはカテゴリカルデータなので除外\n",
        "  X_normal = normal_data[features].values\n",
        "  X_anomaly = anomaly_data[features].values\n",
        "\n",
        "  # データを標準化\n",
        "  scaler = StandardScaler()\n",
        "  X_normal_scaled = scaler.fit_transform(X_normal)\n",
        "  X_anomaly_scaled = scaler.transform(X_anomaly)\n",
        "\n",
        "  # Kernel PCAモデルを学習\n",
        "  kpca = KernelPCA(n_components=2, kernel='rbf', fit_inverse_transform=True)  # 次元数を2に削減, kernel='rbf'でRBFカーネルを指定, fit_inverse_transform=Trueを追加\n",
        "  kpca.fit(X_normal_scaled)  # 正常データでモデルを学習\n",
        "\n",
        "  # 異常度を計算 (再構成誤差)\n",
        "  normal_scores = kpca.transform(X_normal_scaled)\n",
        "  anomaly_scores = kpca.transform(X_anomaly_scaled)\n",
        "\n",
        "  # 再構成誤差を計算 (近似)\n",
        "  reconstruction_error_normal = np.sum(np.square(X_normal_scaled - kpca.inverse_transform(normal_scores)), axis=1)\n",
        "  reconstruction_error_anomaly = np.sum(np.square(X_anomaly_scaled - kpca.inverse_transform(anomaly_scores)), axis=1)\n",
        "\n",
        "  # 閾値を設定 (例: 再構成誤差の95%点)\n",
        "  threshold = np.percentile(reconstruction_error_normal, 95)\n",
        "\n",
        "  # 異常を検出\n",
        "  anomalies = anomaly_data[reconstruction_error_anomaly > threshold]  # 再構成誤差が閾値を超えるデータが異常\n",
        "\n",
        "  # 整数で表示\n",
        "  anomalies['PurchaseAmount'] = anomalies['PurchaseAmount'].astype(int)\n",
        "\n",
        "  return anomalies\n",
        "\n",
        "\n",
        "def factor_analysis(normal_data, anomaly_data):\n",
        "  # 不要な次元を除外\n",
        "  features = ['Age', 'PurchaseAmount', 'PurchaseFrequency', 'UnnecessaryDimension']  # Genderはカテゴリカルデータなので除外\n",
        "  X_normal = normal_data[features].values\n",
        "  X_anomaly = anomaly_data[features].values\n",
        "\n",
        "  # データの標準化\n",
        "  scaler = StandardScaler()\n",
        "  X_normal_scaled = scaler.fit_transform(X_normal)\n",
        "  X_anomaly_scaled = scaler.transform(X_anomaly)\n",
        "\n",
        "  # 因子分析モデルの学習\n",
        "  fa = FactorAnalysis(n_components=2)  # 次元数を2に削減\n",
        "  fa.fit(X_normal_scaled)  # 正常データでモデルを学習\n",
        "\n",
        "  # 異常度の計算 (再構成誤差)\n",
        "  normal_scores = fa.transform(X_normal_scaled)\n",
        "  anomaly_scores = fa.transform(X_anomaly_scaled)\n",
        "  reconstruction_normal = np.dot(normal_scores, fa.components_) + np.mean(X_normal_scaled, axis=0)\n",
        "  reconstruction_anomaly = np.dot(anomaly_scores, fa.components_) + np.mean(X_anomaly_scaled, axis=0)\n",
        "  reconstruction_error_normal = np.sum(np.square(X_normal_scaled - reconstruction_normal), axis=1)\n",
        "  reconstruction_error_anomaly = np.sum(np.square(X_anomaly_scaled - reconstruction_anomaly), axis=1)\n",
        "\n",
        "  # 閾値を設定\n",
        "  threshold = np.percentile(reconstruction_error_normal, 95)\n",
        "\n",
        "  # 異常を検出\n",
        "  anomalies = anomaly_data[reconstruction_error_anomaly > threshold]\n",
        "\n",
        "  # 整数で表示\n",
        "  anomalies['PurchaseAmount'] = anomalies['PurchaseAmount'].astype(int)\n",
        "\n",
        "  return anomalies\n",
        "\n",
        "\n",
        "def ica(normal_data, anomaly_data):\n",
        "  # 不要な次元を除外\n",
        "  features = ['Age', 'PurchaseAmount', 'PurchaseFrequency', 'UnnecessaryDimension']  # Genderはカテゴリカルデータなので除外\n",
        "  X_normal = normal_data[features].values\n",
        "  X_anomaly = anomaly_data[features].values\n",
        "\n",
        "  # データの標準化\n",
        "  scaler = StandardScaler()\n",
        "  normal_data_scaled = scaler.fit_transform(X_normal)\n",
        "  anomaly_data_scaled = scaler.transform(X_anomaly)\n",
        "\n",
        "  # ICAモデルを学習\n",
        "  ica = FastICA(n_components=2)  # 次元数を2に削減\n",
        "  ica.fit(normal_data_scaled)  # 正常データでモデルを学習\n",
        "\n",
        "  # 異常度の計算\n",
        "  normal_scores = ica.transform(normal_data_scaled)\n",
        "  anomaly_scores = ica.transform(anomaly_data_scaled)\n",
        "\n",
        "  # 再構成誤差を計算\n",
        "  reconstruction_error_normal = np.sum(np.square(normal_data_scaled - ica.inverse_transform(normal_scores)), axis=1)\n",
        "  reconstruction_error_anomaly = np.sum(np.square(anomaly_data_scaled - ica.inverse_transform(anomaly_scores)), axis=1)\n",
        "\n",
        "  # 閾値を設定\n",
        "  threshold = np.percentile(reconstruction_error_normal, 95)\n",
        "\n",
        "  # 異常を検出\n",
        "  anomalies = anomaly_data[reconstruction_error_anomaly > threshold]\n",
        "\n",
        "  # 整数で表示\n",
        "  anomalies['PurchaseAmount'] = anomalies['PurchaseAmount'].astype(int)\n",
        "\n",
        "  return anomalies\n",
        "\n",
        "\n",
        "def t_sne(normal_data, anomaly_data):\n",
        "  # 不要な次元を除外\n",
        "  features = ['Age', 'PurchaseAmount', 'PurchaseFrequency', 'UnnecessaryDimension']  # Genderはカテゴリカルデータなので除外\n",
        "  X_normal = normal_data[features].values\n",
        "  X_anomaly = anomaly_data[features].values\n",
        "\n",
        "  # データの標準化\n",
        "  scaler = StandardScaler()\n",
        "  normal_data_scaled = scaler.fit_transform(X_normal)\n",
        "  anomaly_data_scaled = scaler.transform(X_anomaly)\n",
        "\n",
        "  # t-SNEモデルを学習\n",
        "  tsne = TSNE(n_components=2)  # 次元数を2に削減\n",
        "  normal_data_embedded = tsne.fit_transform(normal_data_scaled)  # 正常データでモデルを学習し、埋め込み\n",
        "\n",
        "  # テストデータを埋め込む\n",
        "  anomaly_data_embedded = tsne.fit_transform(anomaly_data_scaled)\n",
        "\n",
        "  # 異常度を計算 (例: k近傍法)\n",
        "  anomalies = k_nearest_neighbors(anomaly_data_embedded, 5)\n",
        "\n",
        "  return anomalies\n",
        "\n",
        "\n",
        "def umap_(normal_data, anomaly_data):\n",
        "  # 不要な次元を除外\n",
        "  features = ['Age', 'PurchaseAmount', 'PurchaseFrequency', 'UnnecessaryDimension']  # Genderはカテゴリカルデータなので除外\n",
        "  X_normal = normal_data[features].values\n",
        "  X_anomaly = anomaly_data[features].values\n",
        "\n",
        "  # データの標準化\n",
        "  scaler = StandardScaler()\n",
        "  normal_data_scaled = scaler.fit_transform(X_normal)\n",
        "  anomaly_data_scaled = scaler.transform(X_anomaly)\n",
        "\n",
        "  # UMAPモデルを学習\n",
        "  reducer = umap.UMAP(n_components=2)  # 次元数を2に削減\n",
        "  normal_data_embedded = reducer.fit_transform(normal_data_scaled)  # 正常データでモデルを学習し、埋め込み\n",
        "\n",
        "  # テストデータを埋め込む\n",
        "  anomaly_data_embedded = reducer.transform(anomaly_data_scaled)\n",
        "\n",
        "  # 異常度を計算 (例: k近傍法)\n",
        "  anomalies = k_nearest_neighbors(anomaly_data_embedded, 5)\n",
        "\n",
        "  return anomalies\n",
        "\n",
        "\n",
        "def filter_method(normal_data, anomaly_data):\n",
        "  # データを結合し、ターゲット変数を追加\n",
        "  X = pd.concat([normal_data, anomaly_data])\n",
        "  y = np.concatenate([np.zeros(len(normal_data)), np.ones(len(anomaly_data))])\n",
        "\n",
        "  # 不要な次元を除外\n",
        "  features = ['Age', 'PurchaseAmount', 'PurchaseFrequency', 'UnnecessaryDimension']\n",
        "  X = X[features]\n",
        "\n",
        "  # 特徴量選択 (分散による選択)\n",
        "  selector = VarianceThreshold(threshold=0.1)\n",
        "  X_selected = selector.fit_transform(X)\n",
        "\n",
        "  # 異常検知 (One-Class SVM)\n",
        "  model = OneClassSVM()\n",
        "  model.fit(X_selected[y == 0])  # 正常データでモデルを学習\n",
        "  anomaly_scores = model.decision_function(X_selected)  # 異常度を計算\n",
        "\n",
        "  # 異常値の判定\n",
        "  # 閾値を調整して異常値を検出できるようにする\n",
        "  threshold = np.percentile(anomaly_scores, 5)  # 下位5%を異常値とする\n",
        "  anomaly_index = np.where(anomaly_scores < threshold)\n",
        "\n",
        "  # 異常値を検出する\n",
        "  anomalies = X.iloc[anomaly_index[0]]\n",
        "\n",
        "  # 整数で表示\n",
        "  anomalies['PurchaseAmount'] = anomalies['PurchaseAmount'].astype(int)\n",
        "\n",
        "  return anomalies\n",
        "\n",
        "\n",
        "def wrapper_method(normal_data, anomaly_data):\n",
        "  # データを結合し、ターゲット変数を追加\n",
        "  X = pd.concat([normal_data, anomaly_data])\n",
        "  y = np.concatenate([np.zeros(len(normal_data)), np.ones(len(anomaly_data))])\n",
        "\n",
        "  # 不要な次元を除外\n",
        "  features = ['Age', 'PurchaseAmount', 'PurchaseFrequency', 'UnnecessaryDimension']\n",
        "  X = X[features]\n",
        "\n",
        "  # 特徴量選択 (RFE)\n",
        "  selector = RFE(estimator=OneClassSVM(), n_features_to_select=5)  # 5個の特徴量を選択\n",
        "  X_selected = selector.fit_transform(X, y)\n",
        "\n",
        "  # 異常検知 (One-Class SVM)\n",
        "  model = OneClassSVM()\n",
        "  model.fit(X_selected[y == 0])  # 正常データでモデルを学習\n",
        "  anomaly_scores = model.decision_function(X_selected)  # 異常度を計算\n",
        "\n",
        "  # 異常値の判定\n",
        "  # 閾値を調整して異常値を検出できるようにする\n",
        "  threshold = np.percentile(anomaly_scores, 5)  # 下位5%を異常値とする\n",
        "  anomaly_index = np.where(anomaly_scores < threshold)\n",
        "\n",
        "  # 異常値を検出する\n",
        "  anomalies = X.iloc[anomaly_index[0]]\n",
        "\n",
        "  # 整数で表示\n",
        "  anomalies['PurchaseAmount'] = anomalies['PurchaseAmount'].astype(int)\n",
        "\n",
        "  return anomalies\n",
        "\n",
        "\n",
        "def embedding_method(normal_data, anomaly_data):\n",
        "  # データを結合し、ターゲット変数を追加\n",
        "  X = pd.concat([normal_data, anomaly_data])\n",
        "\n",
        "  # 不要な次元を除外\n",
        "  features = ['Age', 'PurchaseAmount', 'PurchaseFrequency', 'UnnecessaryDimension']\n",
        "  X = X[features]\n",
        "\n",
        "  # OneClassSVMではターゲット変数は不要なため、ここでは正常データのみを使用します。\n",
        "  X_normal = normal_data\n",
        "\n",
        "  # 異常検知 (One-Class SVM)\n",
        "  model = OneClassSVM(nu=0.1)  # nuは異常値の割合を指定\n",
        "  model.fit(X_normal)  # 正常データでモデルを学習\n",
        "  anomaly_scores = model.decision_function(X)  # 異常度を計算\n",
        "\n",
        "  # 異常値の判定 (例：異常度が負の値を異常値とする)\n",
        "  anomalies = X[anomaly_scores < 0]\n",
        "\n",
        "  # 整数で表示\n",
        "  anomalies['PurchaseAmount'] = anomalies['PurchaseAmount'].astype(int)\n",
        "\n",
        "  return anomalies\n",
        "\n",
        "\n",
        "#### 主成分分析（PCA）\n",
        "anomalies = pca(normal_data, anomaly_data)\n",
        "print(\"============ 主成分分析 ===========\")\n",
        "print(\"異常値:\", anomalies)\n",
        "print(\"\\n\")\n",
        "\n",
        "#### 確率的主成分分析（Probabilistic PCA）\n",
        "anomalies = probabilistic_pca(normal_data, anomaly_data)\n",
        "print(\"========= 確率的主成分分析 ========\")\n",
        "print(\"異常値:\", anomalies)\n",
        "print(\"\\n\")\n",
        "\n",
        "#### カーネル主成分分析（Kernel PCA）\n",
        "anomalies = kernel_pca(normal_data, anomaly_data)\n",
        "print(\"======== カーネル主成分分析 ========\")\n",
        "print(\"異常値:\", anomalies)\n",
        "print(\"\\n\")\n",
        "\n",
        "#### 因子分析（Factor Analysis）\n",
        "anomalies = factor_analysis(normal_data, anomaly_data)\n",
        "print(\"============= 因子分析 =============\")\n",
        "print(\"異常値:\", anomalies)\n",
        "print(\"\\n\")\n",
        "\n",
        "#### 独立成分分析（ICA）\n",
        "anomalies = ica(normal_data, anomaly_data)\n",
        "print(\"=========== 独立成分分析 ===========\")\n",
        "print(\"異常値:\", anomalies)\n",
        "print(\"\\n\")\n",
        "\n",
        "#### t-distributed Stochastic Neighbor Embedding (t-SNE)\n",
        "anomalies = t_sne(normal_data, anomaly_data)\n",
        "print(\"=============== t-SNE ==============\")\n",
        "print(\"異常値:\", anomalies)\n",
        "print(\"\\n\")\n",
        "\n",
        "#### Uniform Manifold Approximation and Projection (UMAP)\n",
        "anomalies = umap_(normal_data, anomaly_data)\n",
        "print(\"=============== UMAP ===============\")\n",
        "print(\"異常値:\", anomalies)\n",
        "print(\"\\n\")\n",
        "\n",
        "#### フィルター法\n",
        "anomalies = filter_method(normal_data, anomaly_data)\n",
        "print(\"=========== フィルター法 ===========\")\n",
        "print(\"異常値:\", anomalies)\n",
        "print(\"\\n\")\n",
        "\n",
        "#### ラッパー法\n",
        "anomalies = wrapper_method(normal_data, anomaly_data)\n",
        "print(\"============ ラッパー法 ============\")\n",
        "print(\"異常値:\", anomalies)\n",
        "print(\"\\n\")\n",
        "\n",
        "#### 埋め込み法\n",
        "\n",
        "#### One-Class SVM\n",
        "anomalies = one_class_svm(all_data)\n",
        "print(\"=============== One-Class SVM ==============\")\n",
        "print(\"異常値:\", anomalies)\n",
        "print(\"\\n\")\n",
        "\n",
        "#### 孤立フォレスト（Isolation Forest）\n",
        "anomalies = isolation_forest(all_data)\n",
        "print(\"=============== 孤立フォレスト ==============\")\n",
        "print(\"異常値:\", anomalies)\n",
        "print(\"\\n\")\n",
        "\n",
        "#### Local Outlier Factor (LOF)\n",
        "anomalies = local_outlier_factor(all_data)\n",
        "print(\"============ Local Outlier Factor ===========\")\n",
        "print(\"異常値:\", anomalies)\n",
        "print(\"\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "8QWDCFA6aqkS",
        "outputId": "67429143-5401-4103-8e9d-dfceddaaf86e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: umap-learn in /usr/local/lib/python3.11/dist-packages (0.5.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (1.14.1)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (1.6.1)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (0.60.0)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (0.5.13)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from umap-learn) (4.67.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.2->umap-learn) (0.43.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.11/dist-packages (from pynndescent>=0.5->umap-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22->umap-learn) (3.6.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-30-ada558343a3d>:48: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[ 230268.37000049  441599.36721387  327180.61327209 1487766.32264944\n",
            "  924782.39497688  886820.68050755 1408597.64831599  173616.32346264\n",
            "  267841.14256087  522590.39353928]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
            "  anomaly_data.loc[anomaly_indices, 'PurchaseAmount'] = anomaly_data.loc[anomaly_indices, 'PurchaseAmount'] * np.random.uniform(100, 200, size=len(anomaly_indices))\n",
            "<ipython-input-30-ada558343a3d>:81: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  anomalies['PurchaseAmount'] = anomalies['PurchaseAmount'].astype(int)\n",
            "<ipython-input-30-ada558343a3d>:114: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  anomalies['PurchaseAmount'] = anomalies['PurchaseAmount'].astype(int)\n",
            "<ipython-input-30-ada558343a3d>:149: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  anomalies['PurchaseAmount'] = anomalies['PurchaseAmount'].astype(int)\n",
            "<ipython-input-30-ada558343a3d>:219: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  anomalies['PurchaseAmount'] = anomalies['PurchaseAmount'].astype(int)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============ 主成分分析 ===========\n",
            "異常値:     CustomerID  Age  Gender  PurchaseAmount  PurchaseFrequency  \\\n",
            "7            8   55  Female             282                  2   \n",
            "8            9   38    Male          441599                  8   \n",
            "23          24   34  Female         1408597                  3   \n",
            "25          26   42    Male          173616                  6   \n",
            "26          27   24    Male            9512                  7   \n",
            "38          39   29  Female          886820                  4   \n",
            "45          46   26    Male          924782                  5   \n",
            "46          47   52  Female            7439                  9   \n",
            "55          56   22  Female          230268                  9   \n",
            "56          57   44  Female          327180                  9   \n",
            "64          65   55  Female          267841                  5   \n",
            "82          83   32  Female          522590                  4   \n",
            "86          87   50    Male             278                  3   \n",
            "88          89   24  Female         1487766                  9   \n",
            "\n",
            "    UnnecessaryDimension  \n",
            "7               0.351480  \n",
            "8               0.490280  \n",
            "23              0.593143  \n",
            "25              0.495040  \n",
            "26              0.916220  \n",
            "38              0.939107  \n",
            "45              0.896860  \n",
            "46              0.081423  \n",
            "55              0.235812  \n",
            "56              0.656868  \n",
            "64              0.280319  \n",
            "82              0.770038  \n",
            "86              0.156135  \n",
            "88              0.487952  \n",
            "\n",
            "\n",
            "========= 確率的主成分分析 ========\n",
            "異常値:     CustomerID  Age  Gender  PurchaseAmount  PurchaseFrequency  \\\n",
            "7            8   55  Female             282                  2   \n",
            "8            9   38    Male          441599                  8   \n",
            "23          24   34  Female         1408597                  3   \n",
            "25          26   42    Male          173616                  6   \n",
            "26          27   24    Male            9512                  7   \n",
            "38          39   29  Female          886820                  4   \n",
            "42          43   56    Male            1008                  9   \n",
            "45          46   26    Male          924782                  5   \n",
            "46          47   52  Female            7439                  9   \n",
            "50          51   58  Female            9366                  1   \n",
            "55          56   22  Female          230268                  9   \n",
            "56          57   44  Female          327180                  9   \n",
            "64          65   55  Female          267841                  5   \n",
            "82          83   32  Female          522590                  4   \n",
            "88          89   24  Female         1487766                  9   \n",
            "\n",
            "    UnnecessaryDimension  \n",
            "7               0.351480  \n",
            "8               0.490280  \n",
            "23              0.593143  \n",
            "25              0.495040  \n",
            "26              0.916220  \n",
            "38              0.939107  \n",
            "42              0.081529  \n",
            "45              0.896860  \n",
            "46              0.081423  \n",
            "50              0.873194  \n",
            "55              0.235812  \n",
            "56              0.656868  \n",
            "64              0.280319  \n",
            "82              0.770038  \n",
            "88              0.487952  \n",
            "\n",
            "\n",
            "======== カーネル主成分分析 ========\n",
            "異常値:     CustomerID  Age  Gender  PurchaseAmount  PurchaseFrequency  \\\n",
            "3            4   31    Male            8925                  8   \n",
            "7            8   55  Female             282                  2   \n",
            "8            9   38    Male          441599                  8   \n",
            "23          24   34  Female         1408597                  3   \n",
            "25          26   42    Male          173616                  6   \n",
            "26          27   24    Male            9512                  7   \n",
            "38          39   29  Female          886820                  4   \n",
            "42          43   56    Male            1008                  9   \n",
            "45          46   26    Male          924782                  5   \n",
            "46          47   52  Female            7439                  9   \n",
            "55          56   22  Female          230268                  9   \n",
            "56          57   44  Female          327180                  9   \n",
            "64          65   55  Female          267841                  5   \n",
            "82          83   32  Female          522590                  4   \n",
            "88          89   24  Female         1487766                  9   \n",
            "\n",
            "    UnnecessaryDimension  \n",
            "3               0.979667  \n",
            "7               0.351480  \n",
            "8               0.490280  \n",
            "23              0.593143  \n",
            "25              0.495040  \n",
            "26              0.916220  \n",
            "38              0.939107  \n",
            "42              0.081529  \n",
            "45              0.896860  \n",
            "46              0.081423  \n",
            "55              0.235812  \n",
            "56              0.656868  \n",
            "64              0.280319  \n",
            "82              0.770038  \n",
            "88              0.487952  \n",
            "\n",
            "\n",
            "============= 因子分析 =============\n",
            "異常値:     CustomerID  Age  Gender  PurchaseAmount  PurchaseFrequency  \\\n",
            "0            1   40    Male            3644                  4   \n",
            "1            2   25  Female            2584                  1   \n",
            "2            3   38    Male            4795                  6   \n",
            "3            4   31    Male            8925                  8   \n",
            "4            5   48    Male             101                  3   \n",
            "..         ...  ...     ...             ...                ...   \n",
            "95          96   46  Female            6397                  1   \n",
            "96          97   23    Male            6843                  7   \n",
            "97          98   50  Female            6099                  9   \n",
            "98          99   26  Female            6932                  5   \n",
            "99         100   22  Female            3270                  8   \n",
            "\n",
            "    UnnecessaryDimension  \n",
            "0               0.426278  \n",
            "1               0.707705  \n",
            "2               0.221672  \n",
            "3               0.979667  \n",
            "4               0.830749  \n",
            "..                   ...  \n",
            "95              0.418922  \n",
            "96              0.248666  \n",
            "97              0.140489  \n",
            "98              0.534913  \n",
            "99              0.181604  \n",
            "\n",
            "[100 rows x 6 columns]\n",
            "\n",
            "\n",
            "=========== 独立成分分析 ===========\n",
            "異常値:     CustomerID  Age  Gender  PurchaseAmount  PurchaseFrequency  \\\n",
            "7            8   55  Female             282                  2   \n",
            "8            9   38    Male          441599                  8   \n",
            "23          24   34  Female         1408597                  3   \n",
            "25          26   42    Male          173616                  6   \n",
            "26          27   24    Male            9512                  7   \n",
            "38          39   29  Female          886820                  4   \n",
            "45          46   26    Male          924782                  5   \n",
            "46          47   52  Female            7439                  9   \n",
            "55          56   22  Female          230268                  9   \n",
            "56          57   44  Female          327180                  9   \n",
            "64          65   55  Female          267841                  5   \n",
            "82          83   32  Female          522590                  4   \n",
            "86          87   50    Male             278                  3   \n",
            "88          89   24  Female         1487766                  9   \n",
            "\n",
            "    UnnecessaryDimension  \n",
            "7               0.351480  \n",
            "8               0.490280  \n",
            "23              0.593143  \n",
            "25              0.495040  \n",
            "26              0.916220  \n",
            "38              0.939107  \n",
            "45              0.896860  \n",
            "46              0.081423  \n",
            "55              0.235812  \n",
            "56              0.656868  \n",
            "64              0.280319  \n",
            "82              0.770038  \n",
            "86              0.156135  \n",
            "88              0.487952  \n",
            "\n",
            "\n",
            "=============== t-SNE ==============\n",
            "異常値: [[-9.114491]\n",
            " [-9.13366 ]]\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=============== UMAP ===============\n",
            "異常値: [[10.586141]\n",
            " [ 9.409656]]\n",
            "\n",
            "\n",
            "=========== フィルター法 ===========\n",
            "異常値:     Age  PurchaseAmount  PurchaseFrequency  UnnecessaryDimension\n",
            "8    38          441599                  8              0.490280\n",
            "23   34         1408597                  3              0.593143\n",
            "25   42          173616                  6              0.495040\n",
            "38   29          886820                  4              0.939107\n",
            "45   26          924782                  5              0.896860\n",
            "55   22          230268                  9              0.235812\n",
            "56   44          327180                  9              0.656868\n",
            "64   55          267841                  5              0.280319\n",
            "82   32          522590                  4              0.770038\n",
            "88   24         1487766                  9              0.487952\n",
            "\n",
            "\n",
            "============ ラッパー法 ============\n",
            "異常値:     Age  PurchaseAmount  PurchaseFrequency  UnnecessaryDimension\n",
            "8    38          441599                  8              0.490280\n",
            "23   34         1408597                  3              0.593143\n",
            "25   42          173616                  6              0.495040\n",
            "38   29          886820                  4              0.939107\n",
            "45   26          924782                  5              0.896860\n",
            "55   22          230268                  9              0.235812\n",
            "56   44          327180                  9              0.656868\n",
            "64   55          267841                  5              0.280319\n",
            "82   32          522590                  4              0.770038\n",
            "88   24         1487766                  9              0.487952\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-30-ada558343a3d>:299: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  anomalies['PurchaseAmount'] = anomalies['PurchaseAmount'].astype(int)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/feature_selection/_rfe.py:300: UserWarning: Found n_features_to_select=5 > n_features=4. There will be no feature selection and all features will be kept.\n",
            "  warnings.warn(\n",
            "<ipython-input-30-ada558343a3d>:331: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  anomalies['PurchaseAmount'] = anomalies['PurchaseAmount'].astype(int)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=============== One-Class SVM ==============\n",
            "異常値: [[252.6739005 ]\n",
            " [265.24192644]\n",
            " [264.72299405]\n",
            " [208.04168272]\n",
            " [261.70242498]\n",
            " [214.14511469]\n",
            " [252.20111675]\n",
            " [252.8578622 ]\n",
            " [213.65400282]\n",
            " [233.36639385]]\n",
            "\n",
            "\n",
            "=============== 孤立フォレスト ==============\n",
            "異常値: [[252.6739005 ]\n",
            " [265.24192644]\n",
            " [264.72299405]\n",
            " [208.04168272]\n",
            " [261.70242498]\n",
            " [214.14511469]\n",
            " [252.20111675]\n",
            " [252.8578622 ]\n",
            " [213.65400282]\n",
            " [233.36639385]]\n",
            "\n",
            "\n",
            "============ Local Outlier Factor ===========\n",
            "異常値: [[ 20.70647844]\n",
            " [ 22.06609245]\n",
            " [ 53.43934297]\n",
            " [  1.00133729]\n",
            " [ 20.70647844]\n",
            " [ 22.06609245]\n",
            " [ 53.43934297]\n",
            " [  1.00133729]\n",
            " [252.6739005 ]\n",
            " [265.24192644]\n",
            " [264.72299405]\n",
            " [208.04168272]\n",
            " [261.70242498]\n",
            " [214.14511469]\n",
            " [252.20111675]\n",
            " [252.8578622 ]\n",
            " [213.65400282]\n",
            " [233.36639385]]\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 入出力関係のあるデータの異常検知\n",
        "### 例\n",
        "製造工程における品質管理\n",
        "<br>\n",
        "\n",
        "### 手法\n",
        "\n",
        "#### 線形回帰モデル\n",
        "正常データを用いて、入力と出力の関係を回帰モデルで学習し、新しいデータが入力された際に、回帰モデルの予測値と実際の出力値の差が大きい場合、異常と判定する。\n",
        "<br>\n",
        "\n",
        "#### リッジ回帰\n",
        "線形回帰モデルの一種で、過学習を防ぐために正則化項を導入したもの。\n",
        "<br>\n",
        "\n",
        "#### ガウス過程回帰\n",
        "非線形な回帰問題を解くための機械学習手法。観測データから入力と出力の関係を確率分布として学習する。この確率分布はガウス過程（任意の有限個の点における確率変数の集合が、常に多変量正規分布に従うような確率過程）で表現され、予測値だけでなく、予測値の不確実性も推定することができる。\n",
        "<br>\n",
        "\n",
        "#### 状態空間モデル\n",
        "入出力関係を状態空間モデルで表現し、観測値と予測値の差を異常度として用いる方法。\n",
        "<br>\n",
        "\n",
        "#### [One-Class SVM](#scrollTo=FMOqmjNLiot5&line=33&uniqifier=1)"
      ],
      "metadata": {
        "id": "NWg4xZztixiP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 時系列データの異常検知\n",
        "### 例\n",
        "サーバーのCPU使用率の異常検知\n",
        "\n",
        "### 手法\n",
        "\n",
        "#### 自己回帰モデル\n",
        "時系列データの過去の値を用いて、未来の値を予測するモデル。予測値と実測値の差が大きい場合に異常と判定する。\n",
        "<br>\n",
        "\n",
        "#### 移動平均法\n",
        "一定期間のデータの平均値を計算し、その平均値から大きく外れた値を異常とみなす手法。\n",
        "<br>\n",
        "\n",
        "#### 指数平滑法\n",
        "直近のデータに大きな重みを置くことで、移動平均法よりも長期的な変動に対応できる手法。\n",
        "<br>\n",
        "\n",
        "#### ARIMAモデル\n",
        "時系列データの自己相関を考慮したモデルで、予測値と実測値の差が大きい場合、異常とみなす。\n",
        "<br>\n",
        "\n",
        "#### [$k$近傍法](#scrollTo=FMOqmjNLiot5&line=7&uniqifier=1)\n",
        "<br>\n",
        "\n",
        "#### サポートベクターマシン (SVM)\n",
        "正常データと異常データを分離する超平面を学習する手法。[One-Class SVM](#scrollTo=FMOqmjNLiot5&line=33&uniqifier=1)は正常データのみを用いる。\n",
        "<br>\n",
        "\n",
        "#### [孤立フォレスト (Isolation Forest)](#scrollTo=FMOqmjNLiot5&line=7&uniqifier=1)\n",
        "<br>\n",
        "\n",
        "#### LSTM (Long Short-Term Memory)\n",
        "<br>\n",
        "\n",
        "#### 特異スペクトル変換法\n",
        "時系列データの長期的な依存関係を学習できるニューラルネットワーク。正常な時系列データを学習し、学習データと大きく異なるパターンを持つデータを異常とみなす。\n",
        "<br>\n",
        "\n",
        "#### [状態空間モデル](#scrollTo=NWg4xZztixiP&line=23&uniqifier=1)\n",
        "\n",
        "#### Autoencoder\n",
        "データを低次元空間に圧縮し、復元するニューラルネットワーク。正常な時系列データを学習し、復元誤差が大きいデータを異常とみなす。\n",
        "<br>\n",
        "\n",
        "#### Variational Autoencoder (VAE)\n",
        "Autoencoderを確率モデルとして拡張した手法。\n",
        "<br>\n",
        "\n",
        "#### Generative Adversarial Networks (GANs)\n",
        " 正常な時系列データを生成する生成器と、生成されたデータが正常か異常かを判定する識別器を競合的に学習させる手法。識別器が正常と判定できないデータを異常とみなす。\n",
        " <br>"
      ],
      "metadata": {
        "id": "n1WuKOxxizq4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 変数間に関係があるデータの異常検知\n",
        "### 例\n",
        "クレジットカードの不正利用検知\n",
        "\n",
        "### 手法\n",
        "#### 疎構造学習\n",
        "高次元データにおいて、変数間の関係が疎であることを仮定して、データの構造を学習する手法。正常なデータにおける変数間の関係を疎構造として学習し、その構造から外れるデータを異常とみなす。\n",
        "<br>\n",
        "\n",
        "#### ベイジアンネットワーク\n",
        "変数間の因果関係を表現したグラフィカルモデル。変数間の条件付き確率を学習し、異常な条件付き確率を持つデータを異常とみなす。\n",
        "<br>\n",
        "\n",
        "#### マルコフ確率場\n",
        "変数間の相互作用を表現したグラフィカルモデル。変数間のポテンシャル関数を学習し、異常なポテンシャル関数を持つデータを異常とみなす。\n",
        "<br>\n",
        "\n",
        "#### [k-means法](#scrollTo=FMOqmjNLiot5&line=7&uniqifier=1)\n",
        "<br>\n",
        "\n",
        "#### DBSCAN\n",
        "密度ベースのクラスタリングアルゴリズム。データポイントを、高密度領域にあるデータポイントと低密度領域にあるデータポイント（ノイズ）に分類し、低密度領域にあるデータポイントを異常とみなす。\n",
        "<br>\n",
        "\n",
        "#### [One-Class SVM](#scrollTo=FMOqmjNLiot5&line=33&uniqifier=1)\n",
        "<br>\n",
        "\n",
        "#### [孤立フォレスト (Isolation Forest)](#scrollTo=FMOqmjNLiot5&line=7&uniqifier=1)\n",
        "<br>\n",
        "\n",
        "#### [Local Outlier Factor (LOF)](#scrollTo=FMOqmjNLiot5&line=33&uniqifier=1)\n",
        "<br>"
      ],
      "metadata": {
        "id": "42XygjKgi1od"
      }
    }
  ]
}