{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNYhC0ffyeVGK71cICdepzg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hsswkwk/turbo-chainsaw/blob/feature-add-anomaly-detection/notebooks/anomaly_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 異常検知（Anomaly detection）\n",
        "標準的な状態や想定から逸脱しているデータや事象を特定する技術\n"
      ],
      "metadata": {
        "id": "PaP90VRh7wqm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "6SBjNkeLS_is"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 正規分布に従うデータの異常検知\n",
        "### 例\n",
        "製品の温度をセンサーで監視している場合の異常検知\n",
        "<br>\n",
        "<br>\n",
        "### 手法\n",
        "\n",
        "#### 3$\\sigma$法\n",
        "データが正規分布に従うと仮定し、平均値から標準偏差の3倍以上離れた値を異常値とみなす方法\n",
        "<br>\n",
        "#### マハラノビス・タグチ法\n",
        "データの各次元間の相関を考慮した距離尺度を用いて、平均値から離れた値を異常値とみなす方法\n",
        "<br>\n",
        "#### ホテリングの$T^2$法\n",
        "マハラノビス距離を拡張した手法で、データの平均値からのずれを検定統計量として用いて異常値を検出する方法\n",
        "<br>\n",
        "#### 密度比推定\n",
        "正常データと異常データの確率密度比を推定することで異常を検知する手法\n",
        "<br>"
      ],
      "metadata": {
        "id": "1dE3Gampih0k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.spatial.distance import mahalanobis\n",
        "from scipy.stats import f\n",
        "from sklearn.neighbors import KernelDensity\n",
        "\n",
        "\n",
        "### 3σ法\n",
        "# threshold: データの平均値から標準偏差の何倍離れたら異常値とみなすか\n",
        "def three_sigma_method(data, threshold=3):\n",
        "  mean = np.mean(data)\n",
        "  std = np.std(data)\n",
        "  lower_bound = mean - threshold * std\n",
        "  upper_bound = mean + threshold * std\n",
        "  if data.ndim == 1:\n",
        "    anomaly_indices = np.where((data < lower_bound) | (data > upper_bound))[0]\n",
        "  elif data.ndim >= 2:\n",
        "    anomaly_indices = np.where(np.any((data < lower_bound) | (data > upper_bound), axis=1))[0]\n",
        "  return data[anomaly_indices]\n",
        "\n",
        "### マハラノビス距離\n",
        "# マハラノビス距離: データの相関を考慮した距離尺度\n",
        "# threshold_percentile: マハラノビス距離の分布におけるパーセンタイル値を超える\n",
        "#                       距離を持つデータを異常値とみなす\n",
        "def mahalanobis_distance_method(data, threshold_percentile=99):\n",
        "  mean = np.mean(data, axis=0)\n",
        "  if data.ndim == 1:\n",
        "    std = np.std(data)\n",
        "\n",
        "    # 標準偏差が0の場合、ゼロ除算を避けるために微小な値を加算\n",
        "    if std == 0:\n",
        "      std = 1e-6\n",
        "\n",
        "    distances = np.abs((data - mean) / std)\n",
        "\n",
        "    # 異常値とみなす閾値を設定\n",
        "    threshold = np.percentile(distances, threshold_percentile)\n",
        "\n",
        "    # 閾値を超えるデータを異常値として検出\n",
        "    anomalies = data[distances > threshold]\n",
        "  elif data.ndim >= 2:\n",
        "    cov = np.cov(data, rowvar=False)\n",
        "    inv_cov = np.linalg.inv(cov)\n",
        "\n",
        "    distances = [mahalanobis(x, mean, inv_cov) for x in data]\n",
        "\n",
        "    # 異常値とみなす閾値を設定\n",
        "    threshold = np.percentile(distances, threshold_percentile)\n",
        "\n",
        "    # 閾値を超えるデータを異常値として検出\n",
        "    anomaly_indices = np.where(np.array(distances) > threshold)[0]\n",
        "    anomalies = data[anomaly_indices]\n",
        "  return anomalies\n",
        "\n",
        "### ホテリングのT^2法\n",
        "def hotelling_t2_method(data):\n",
        "  mean = np.mean(data, axis=0)\n",
        "  if data.ndim == 1:\n",
        "    var = np.var(data)\n",
        "    t2_values = [(x - mean)**2 / var for x in data]\n",
        "    # f.ppf: F分布のパーセント点関数\n",
        "    threshold = f.ppf(0.95, 1, len(data) - 1)  # 異常値とみなす閾値を設定\n",
        "    anomalies = [data[i] for i, t2 in enumerate(t2_values) if t2 > threshold]\n",
        "  elif data.ndim >= 2:\n",
        "    mean = np.mean(data, axis=0)\n",
        "    cov = np.cov(data, rowvar=False)\n",
        "    inv_cov = np.linalg.inv(cov)\n",
        "    t2_values = [mahalanobis(x, mean, inv_cov)**2 for x in data]\n",
        "    threshold = f.ppf(0.95, data.shape[1], data.shape[0] - data.shape[1] - 1)  # 異常値とみなす閾値を設定\n",
        "    anomaly_indices = np.where(np.array(t2_values) > threshold)[0]\n",
        "    anomalies = data[anomaly_indices]\n",
        "  return anomalies\n",
        "\n",
        "\n",
        "# 正常データ生成\n",
        "normal_data = np.random.normal(loc=25, scale=2, size=100)\n",
        "\n",
        "# 異常データ生成\n",
        "anomaly_data = np.random.normal(loc=25, scale=2, size=100)\n",
        "# 異常値の追加\n",
        "anomaly_indices = [10, 20, 30]  # 異常値のインデックス\n",
        "anomaly_values = [35, 15, 32]  # 異常値\n",
        "anomaly_data[anomaly_indices] = anomaly_values\n",
        "\n",
        "# 結果の出力\n",
        "anomalies = three_sigma_method(anomaly_data)\n",
        "print(\"================== 3σ法 =================\")\n",
        "print(\"異常値:\", anomalies)\n",
        "print(\"\\n\")\n",
        "\n",
        "anomalies = mahalanobis_distance_method(anomaly_data)\n",
        "print(\"===== マハラノビス距離による異常検知 =====\")\n",
        "print(\"異常値:\", anomalies)\n",
        "print(\"\\n\")\n",
        "\n",
        "anomalies = hotelling_t2_method(anomaly_data)\n",
        "print(\"===== ホテリングのT^2法による異常検知 ====\")\n",
        "print(\"異常値:\", anomalies)\n",
        "print(\"\\n\")\n"
      ],
      "metadata": {
        "id": "VydvSi0Ejy_Z",
        "outputId": "a5e9e470-e2b8-4720-937e-302eb808731c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================== 3σ法 =================\n",
            "異常値: [35. 15.]\n",
            "\n",
            "\n",
            "===== マハラノビス距離による異常検知 =====\n",
            "異常値: [35.]\n",
            "\n",
            "\n",
            "===== ホテリングのT^2法による異常検知 ====\n",
            "異常値: [np.float64(35.0), np.float64(15.0), np.float64(32.0), np.float64(30.345462268832556)]\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 非正規データの異常検知\n",
        "### 例\n",
        "ウェブサイトへのアクセス数の異常検知\n",
        "<br>\n",
        "<br>\n",
        "### 手法\n",
        "\n",
        "#### ガンマ分布の当てはめ\n",
        "データがガンマ分布に従うと仮定し、その分布から大きく外れた値を異常値とみなす方法。データがガンマ分布に従う場合、高い精度で異常を検出できるが、従わない場合、精度が低下する。\n",
        "<br>\n",
        "\n",
        "#### カイ二乗分布への当てはめ\n",
        "データがカイ二乗分布に従うと仮定し、その分布から大きく外れた値を異常値とみなす方法。主に、特徴量の値が正の値を取り、歪んだ分布をしている場合に有効。<br>\n",
        "<br>\n",
        "\n",
        "#### $k$近傍法\n",
        "各データポイントからk番目に近いデータポイントまでの距離を計算し、距離が大きいデータポイントを異常値と判定するアルゴリズム。$k$はデータセットのサイズの平方根よりも小さい奇数にすると良い。\n",
        "<br>\n",
        "\n",
        "#### $k$ means法\n",
        "データを複数のクラスタに分割し、どのクラスタにも属さないデータポイントを異常値と判定するアルゴリズム。$k$を推定する手法としてエルボー法やシルエット分析などがある。\n",
        "<br>\n",
        "\n",
        "#### 混合ガウス分布モデル（Gaussian Mixture Model, GMM）\n",
        "データが複数のガウス分布（正規分布）の混合で表現できると仮定し、低確率なデータ点を異常値とみなす方法。\n",
        "<br>\n",
        "\n",
        "#### One-Class SVM\n",
        "正常データのみを用いて、正常データの領域を学習するアルゴリズム。 学習した領域から外れたデータは異常値と判定する。\n",
        "<br>\n",
        "\n",
        "#### [密度比推定](#scrollTo=1dE3Gampih0k&line=1&uniqifier=1)\n",
        "<br>\n",
        "\n",
        "#### 孤立フォレスト（Isolation Forest）\n",
        "データポイントをランダムに分割していくことで、異常値を孤立させるアルゴリズム。\n",
        "<br>\n",
        "\n",
        "#### Local Outlier Factor (LOF)\n",
        "データポイントの局所的な密度を計算し、密度が低いデータポイントを異常値と判定するアルゴリズム。\n",
        "<br>\n",
        "\n"
      ],
      "metadata": {
        "id": "FMOqmjNLiot5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.stats as stats\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "\n",
        "### ガンマ分布の当てはめ\n",
        "def fitting_gamma_distribution(data):\n",
        "  # データにガンマ分布を当てはめる\n",
        "  shape, loc, scale = stats.gamma.fit(data)\n",
        "\n",
        "  # 異常度を計算する\n",
        "  anomaly_scores = 1 / stats.gamma.pdf(data, shape, loc, scale)\n",
        "\n",
        "  # 閾値を設定する\n",
        "  threshold = np.mean(anomaly_scores) + 3 * np.std(anomaly_scores)\n",
        "\n",
        "  # 異常値を検出する\n",
        "  anomalies = data[anomaly_scores > threshold]\n",
        "\n",
        "  return anomalies\n",
        "\n",
        "\n",
        "### k近傍法\n",
        "def k_nearest_neighbors(data, k):\n",
        "  data = data.reshape(-1, 1)\n",
        "\n",
        "  # k近傍の計算\n",
        "  knn = NearestNeighbors(n_neighbors=k)\n",
        "  knn.fit(data)\n",
        "  distances, indices = knn.kneighbors(data)\n",
        "\n",
        "  # 異常度の算出\n",
        "  anomaly_score = distances[:, -1]\n",
        "  threshold = np.percentile(anomaly_score, 99)\n",
        "\n",
        "  # 異常値の判定\n",
        "  anomaly_index = np.where(anomaly_score > threshold)\n",
        "\n",
        "  # 異常値を検出する\n",
        "  anomalies = data[anomaly_index]\n",
        "\n",
        "  return anomalies\n",
        "\n",
        "\n",
        "### 孤立フォレスト\n",
        "def isolation_forest(data):\n",
        "  data = data.reshape(-1, 1)\n",
        "\n",
        "  # モデルの学習\n",
        "  model = IsolationForest()\n",
        "  model.fit(data)\n",
        "\n",
        "  # 異常度の算出\n",
        "  anomaly_score = model.decision_function(data)\n",
        "\n",
        "  # 異常値の判定\n",
        "  anomaly_index = np.where(anomaly_score < -0.35)\n",
        "\n",
        "  # 異常値を検出する\n",
        "  anomalies = data[anomaly_index]\n",
        "\n",
        "  return anomalies\n",
        "\n",
        "\n",
        "def power_law_distribution(alpha, xmin, xmax, size):\n",
        "    \"\"\"べき乗則に従う乱数を生成する関数\n",
        "\n",
        "    Args:\n",
        "        alpha (float): べき乗則の指数\n",
        "        xmin (float): 最小値\n",
        "        xmax (float): 最大値\n",
        "        size (int): 生成する乱数の数\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: べき乗則に従う乱数\n",
        "    \"\"\"\n",
        "\n",
        "    u = np.random.uniform(size=size)\n",
        "    return (xmin ** (1 - alpha) + (xmax ** (1 - alpha) - xmin ** (1 - alpha)) * u) ** (1 / (1 - alpha))\n",
        "\n",
        "\n",
        "# 正常データの生成\n",
        "normal_data = power_law_distribution(alpha=2, xmin=1, xmax=100, size=1000)\n",
        "\n",
        "# 異常データの生成\n",
        "outliers = np.random.uniform(low=2*normal_data.max(), high=3*normal_data.max(), size=10)\n",
        "anomaly_data = np.concatenate([normal_data, outliers])\n",
        "\n",
        "all_data = np.concatenate([normal_data, anomaly_data])\n",
        "\n",
        "#### ガンマ分布の当てはめ\n",
        "anomalies = fitting_gamma_distribution(all_data)\n",
        "print(\"============ ガンマ分布の当てはめ ==========\")\n",
        "print(\"異常値:\", anomalies)\n",
        "print(\"\\n\")\n",
        "\n",
        "#### k近傍法\n",
        "anomalies = k_nearest_neighbors(all_data, 5)\n",
        "print(\"================== k近傍法 =================\")\n",
        "print(\"異常値:\", anomalies)\n",
        "print(\"\\n\")\n",
        "\n",
        "#### 孤立フォレスト (Isolation Forest)\n",
        "anomalies = isolation_forest(all_data)\n",
        "print(\"=============== 孤立フォレスト ==============\")\n",
        "print(\"異常値:\", anomalies)\n",
        "print(\"\\n\")\n"
      ],
      "metadata": {
        "id": "aDeywrg2lfYP",
        "outputId": "65f776e2-09f8-4e6c-a0ad-abf7b1b413e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============ ガンマ分布の当てはめ ==========\n",
            "異常値: [268.93160448 290.18407073 284.94866004 276.45041134 289.92953617\n",
            " 273.38715601 273.10970905]\n",
            "\n",
            "\n",
            "================== k近傍法 =================\n",
            "異常値: [[ 96.74146167]\n",
            " [ 70.27543975]\n",
            " [ 66.48558792]\n",
            " [ 78.56197391]\n",
            " [ 88.67405235]\n",
            " [ 96.74146167]\n",
            " [ 70.27543975]\n",
            " [ 66.48558792]\n",
            " [ 78.56197391]\n",
            " [ 88.67405235]\n",
            " [268.93160448]\n",
            " [290.18407073]\n",
            " [242.3174822 ]\n",
            " [284.94866004]\n",
            " [276.45041134]\n",
            " [226.79897513]\n",
            " [249.05686843]\n",
            " [289.92953617]\n",
            " [273.38715601]\n",
            " [273.10970905]]\n",
            "\n",
            "\n",
            "=============== 孤立フォレスト ==============\n",
            "異常値: [[268.93160448]\n",
            " [290.18407073]\n",
            " [242.3174822 ]\n",
            " [284.94866004]\n",
            " [276.45041134]\n",
            " [226.79897513]\n",
            " [249.05686843]\n",
            " [289.92953617]\n",
            " [273.38715601]\n",
            " [273.10970905]]\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 不要次元のある次元データの異常検知\n",
        "### 例\n",
        "顧客の購買データを用いた異常検知\n",
        "<br>\n",
        "<br>\n",
        "### 次元削減を用いる手法\n",
        "\n",
        "#### 主成分分析（PCA）\n",
        "データの分散を最大化するように新しい軸を定義し、その軸にデータを射影することで次元を削減する手法。異常なデータ点は、主成分空間において、正常なデータ点から離れた位置に配置される傾向がある。\n",
        "<br>\n",
        "\n",
        "#### 確率的主成分分析（Probabilistic PCA）\n",
        "PCAを確率モデルとして拡張した手法。データにノイズが含まれる場合に、よりロバストな結果が得られる。（＝データにノイズや外れ値が含まれていても、分析結果が大きく影響を受けにくい）\n",
        "<br>\n",
        "\n",
        "#### カーネル主成分分析（Kernel PCA）\n",
        "非線形な関係を持つデータに対して、カーネル関数を使用して高次元空間に写像し、その空間でPCAを行うことで次元を削減する手法。\n",
        "<br>\n",
        "\n",
        "#### 因子分析（Factor Analysis）\n",
        "観測変数の背後にある潜在変数を推定し、それらを用いて次元を削減する手法。\n",
        "<br>\n",
        "\n",
        "#### 独立成分分析（ICA）\n",
        "観測変数を、統計的に独立な成分に分解することで次元を削減する手法。\n",
        "<br>\n",
        "\n",
        "#### t-distributed Stochastic Neighbor Embedding (t-SNE)\n",
        "高次元データを低次元空間に埋め込む際に、データの局所的な構造を保持する様に設計された手法。異常検知では、正常なデータ点と異常なデータ点が、低次元空間において明確に分離されることが期待する。\n",
        "<br>\n",
        "\n",
        "#### Uniform Manifold Approximation and Projection (UMAP)\n",
        "高次元データを低次元空間に埋め込む際に、データのトポロジカルな構造を保持する様に設計された手法です。t-SNEと同様に、異常検知では、正常なデータ点と異常なデータ点が、低次元空間において明確に分離されることを期待する。\n",
        "<br>\n",
        "\n",
        "### 特徴量選択を用いる手法\n",
        "#### フィルター法\n",
        "各特徴量と目的変数の間の相関や相互情報量などを用いて、重要度の低い特徴量を除去する手法。\n",
        "<br>\n",
        "\n",
        "#### ラッパー法\n",
        "特徴量のサブセットを選択し、そのサブセットを用いて学習したモデルの性能を評価することで、最適な特徴量を選択する手法。\n",
        "<br>\n",
        "\n",
        "#### 埋め込み法\n",
        "モデルの学習過程で特徴量選択を行う手法。LASSO回帰や決定木などが用いられる。\n",
        "<br>\n",
        "\n",
        "### その他の手法\n",
        "#### [One-Class SVM](#scrollTo=FMOqmjNLiot5&line=33&uniqifier=1)\n",
        "<br>\n",
        "\n",
        "#### [孤立フォレスト（Isolation Forest）](#scrollTo=FMOqmjNLiot5&line=33&uniqifier=1)\n",
        "<br>\n",
        "\n",
        "#### [Local Outlier Factor (LOF)](#scrollTo=FMOqmjNLiot5&line=33&uniqifier=1)\n",
        "<br>\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "V4fWrW7RivcU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "### One-Class SVM\n",
        "def one_class_svm(data):\n",
        "  data = data.reshape(-1, 1)\n",
        "\n",
        "  # モデルの学習\n",
        "  model = OneClassSVM()\n",
        "\n",
        "  # 閾値の候補を指定\n",
        "  param_grid = {'nu': [0.1, 0.2, 0.3], 'gamma': ['scale', 'auto']}\n",
        "\n",
        "  # 交差検定を実行\n",
        "  grid_search = GridSearchCV(model, param_grid, scoring='f1')\n",
        "  grid_search.fit(data)\n",
        "\n",
        "  # 最適なモデルを取得\n",
        "  model = grid_search.best_estimator_\n",
        "\n",
        "  # 最適な閾値を取得\n",
        "  best_threshold = grid_search.best_estimator_.decision_function(data).min()\n",
        "\n",
        "  # 異常度の算出\n",
        "  anomaly_score = model.decision_function(data)\n",
        "\n",
        "  # 異常値の判定\n",
        "  anomaly_index = np.where(anomaly_score < -850)\n",
        "\n",
        "  # 異常値を検出する\n",
        "  anomalies = data[anomaly_index]\n",
        "\n",
        "  return anomalies\n",
        "\n",
        "\n",
        "### Local Outlier Factor (LOF)\n",
        "def local_outlier_factor(data):\n",
        "  data = data.reshape(-1, 1)\n",
        "\n",
        "  # モデルの学習\n",
        "  model = LocalOutlierFactor()\n",
        "  anomaly_score = model.fit_predict(data)\n",
        "\n",
        "  # 異常値の判定\n",
        "  anomaly_index = np.where(anomaly_score == -1)\n",
        "\n",
        "  # 異常値を検出する\n",
        "  anomalies = data[anomaly_index]\n",
        "\n",
        "  return anomalies\n",
        "\n",
        "\n",
        "#### 正常データの生成\n",
        "# 顧客ID、年齢、性別、購入金額、購入頻度、不要な次元（ランダムな数値）を生成\n",
        "num_customers = 100  # 顧客数\n",
        "customer_ids = np.arange(1, num_customers + 1)\n",
        "ages = np.random.randint(20, 60, size=num_customers)\n",
        "genders = np.random.choice(['Male', 'Female'], size=num_customers)\n",
        "purchase_amounts = np.random.randint(100, 10000, size=num_customers)\n",
        "purchase_frequencies = np.random.randint(1, 10, size=num_customers)\n",
        "unnecessary_dimension = np.random.rand(num_customers)\n",
        "\n",
        "# データフレームを作成\n",
        "normal_data = pd.DataFrame({\n",
        "  'CustomerID': customer_ids,\n",
        "  'Age': ages,\n",
        "  'Gender': genders,\n",
        "  'PurchaseAmount': purchase_amounts,\n",
        "  'PurchaseFrequency': purchase_frequencies,\n",
        "  'UnnecessaryDimension': unnecessary_dimension  # 不要な次元\n",
        "})\n",
        "\n",
        "#### 異常データの生成\n",
        "anomaly_data = normal_data.copy()\n",
        "\n",
        "# 異常値を挿入するインデックスをランダムに選択\n",
        "# 例：顧客の10%を異常値とする\n",
        "anomaly_indices = np.random.choice(anomaly_data.index, size=int(num_customers * 0.1), replace=False)\n",
        "\n",
        "# 選択したインデックスのデータに異常値を代入\n",
        "# 例：PurchaseAmountを極端に大きくする\n",
        "anomaly_data.loc[anomaly_indices, 'PurchaseAmount'] = anomaly_data.loc[anomaly_indices, 'PurchaseAmount'] * np.random.uniform(100, 200, size=len(anomaly_indices))\n",
        "\n",
        "\n",
        "#### One-Class SVM\n",
        "anomalies = one_class_svm(all_data)\n",
        "print(\"=============== One-Class SVM ==============\")\n",
        "print(\"異常値:\", anomalies)\n",
        "print(\"\\n\")\n",
        "\n",
        "# #### 孤立フォレスト（Isolation Forest）\n",
        "# anomalies = isolation_forest(all_data)\n",
        "# print(\"=============== 孤立フォレスト ==============\")\n",
        "# print(\"異常値:\", anomalies)\n",
        "# print(\"\\n\")\n",
        "\n",
        "# #### Local Outlier Factor (LOF)\n",
        "# anomalies = local_outlier_factor(all_data)\n",
        "# print(\"============ Local Outlier Factor ===========\")\n",
        "# print(\"異常値:\", anomalies)\n",
        "# print(\"\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "8QWDCFA6aqkS",
        "outputId": "9fd102c5-4bd9-4d31-d593-1609a1efe94e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-d4ec15b9e6dd>:84: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[ 924781.17190045 1253254.68864096  836564.92746946  430832.86975697\n",
            "  766079.46257722  460052.14903901  750831.57450826  423639.83483709\n",
            "  899874.63539881  220424.29516352]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
            "  anomaly_data.loc[anomaly_indices, 'PurchaseAmount'] = anomaly_data.loc[anomaly_indices, 'PurchaseAmount'] * np.random.uniform(100, 200, size=len(anomaly_indices))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:960: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 947, in _score\n",
            "    scores = scorer(estimator, X_test, **score_params)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: _BaseScorer.__call__() missing 1 required positional argument: 'y_true'\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:960: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 947, in _score\n",
            "    scores = scorer(estimator, X_test, **score_params)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: _BaseScorer.__call__() missing 1 required positional argument: 'y_true'\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:960: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 947, in _score\n",
            "    scores = scorer(estimator, X_test, **score_params)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: _BaseScorer.__call__() missing 1 required positional argument: 'y_true'\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:960: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 947, in _score\n",
            "    scores = scorer(estimator, X_test, **score_params)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: _BaseScorer.__call__() missing 1 required positional argument: 'y_true'\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:960: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 947, in _score\n",
            "    scores = scorer(estimator, X_test, **score_params)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: _BaseScorer.__call__() missing 1 required positional argument: 'y_true'\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:960: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 947, in _score\n",
            "    scores = scorer(estimator, X_test, **score_params)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: _BaseScorer.__call__() missing 1 required positional argument: 'y_true'\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:960: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 947, in _score\n",
            "    scores = scorer(estimator, X_test, **score_params)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: _BaseScorer.__call__() missing 1 required positional argument: 'y_true'\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:960: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 947, in _score\n",
            "    scores = scorer(estimator, X_test, **score_params)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: _BaseScorer.__call__() missing 1 required positional argument: 'y_true'\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:960: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 947, in _score\n",
            "    scores = scorer(estimator, X_test, **score_params)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: _BaseScorer.__call__() missing 1 required positional argument: 'y_true'\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:960: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 947, in _score\n",
            "    scores = scorer(estimator, X_test, **score_params)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: _BaseScorer.__call__() missing 1 required positional argument: 'y_true'\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:960: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 947, in _score\n",
            "    scores = scorer(estimator, X_test, **score_params)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: _BaseScorer.__call__() missing 1 required positional argument: 'y_true'\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:960: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 947, in _score\n",
            "    scores = scorer(estimator, X_test, **score_params)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: _BaseScorer.__call__() missing 1 required positional argument: 'y_true'\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:960: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 947, in _score\n",
            "    scores = scorer(estimator, X_test, **score_params)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: _BaseScorer.__call__() missing 1 required positional argument: 'y_true'\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:960: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 947, in _score\n",
            "    scores = scorer(estimator, X_test, **score_params)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: _BaseScorer.__call__() missing 1 required positional argument: 'y_true'\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:960: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 947, in _score\n",
            "    scores = scorer(estimator, X_test, **score_params)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: _BaseScorer.__call__() missing 1 required positional argument: 'y_true'\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:960: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 947, in _score\n",
            "    scores = scorer(estimator, X_test, **score_params)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: _BaseScorer.__call__() missing 1 required positional argument: 'y_true'\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:960: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 947, in _score\n",
            "    scores = scorer(estimator, X_test, **score_params)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: _BaseScorer.__call__() missing 1 required positional argument: 'y_true'\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:960: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 947, in _score\n",
            "    scores = scorer(estimator, X_test, **score_params)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: _BaseScorer.__call__() missing 1 required positional argument: 'y_true'\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:960: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 947, in _score\n",
            "    scores = scorer(estimator, X_test, **score_params)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: _BaseScorer.__call__() missing 1 required positional argument: 'y_true'\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:960: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 947, in _score\n",
            "    scores = scorer(estimator, X_test, **score_params)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: _BaseScorer.__call__() missing 1 required positional argument: 'y_true'\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:960: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 947, in _score\n",
            "    scores = scorer(estimator, X_test, **score_params)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: _BaseScorer.__call__() missing 1 required positional argument: 'y_true'\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:960: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 947, in _score\n",
            "    scores = scorer(estimator, X_test, **score_params)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: _BaseScorer.__call__() missing 1 required positional argument: 'y_true'\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:960: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 947, in _score\n",
            "    scores = scorer(estimator, X_test, **score_params)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: _BaseScorer.__call__() missing 1 required positional argument: 'y_true'\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:960: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 947, in _score\n",
            "    scores = scorer(estimator, X_test, **score_params)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: _BaseScorer.__call__() missing 1 required positional argument: 'y_true'\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:960: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 947, in _score\n",
            "    scores = scorer(estimator, X_test, **score_params)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: _BaseScorer.__call__() missing 1 required positional argument: 'y_true'\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:960: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 947, in _score\n",
            "    scores = scorer(estimator, X_test, **score_params)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: _BaseScorer.__call__() missing 1 required positional argument: 'y_true'\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:960: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 947, in _score\n",
            "    scores = scorer(estimator, X_test, **score_params)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: _BaseScorer.__call__() missing 1 required positional argument: 'y_true'\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:960: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 947, in _score\n",
            "    scores = scorer(estimator, X_test, **score_params)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: _BaseScorer.__call__() missing 1 required positional argument: 'y_true'\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:960: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 947, in _score\n",
            "    scores = scorer(estimator, X_test, **score_params)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: _BaseScorer.__call__() missing 1 required positional argument: 'y_true'\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:960: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 947, in _score\n",
            "    scores = scorer(estimator, X_test, **score_params)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: _BaseScorer.__call__() missing 1 required positional argument: 'y_true'\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py:1108: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=============== One-Class SVM ==============\n",
            "異常値: []\n",
            "\n",
            "\n",
            "=============== 孤立フォレスト ==============\n",
            "異常値: []\n",
            "\n",
            "\n",
            "============ Local Outlier Factor ===========\n",
            "異常値: []\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 入出力関係のあるデータの異常検知\n",
        "### 例\n",
        "製造工程における品質管理\n",
        "<br>\n",
        "\n",
        "### 手法\n",
        "\n",
        "#### 線形回帰モデル\n",
        "正常データを用いて、入力と出力の関係を回帰モデルで学習し、新しいデータが入力された際に、回帰モデルの予測値と実際の出力値の差が大きい場合、異常と判定する。\n",
        "<br>\n",
        "\n",
        "#### リッジ回帰\n",
        "線形回帰モデルの一種で、過学習を防ぐために正則化項を導入したもの。\n",
        "<br>\n",
        "\n",
        "#### ガウス過程回帰\n",
        "非線形な回帰問題を解くための機械学習手法。観測データから入力と出力の関係を確率分布として学習する。この確率分布はガウス過程（任意の有限個の点における確率変数の集合が、常に多変量正規分布に従うような確率過程）で表現され、予測値だけでなく、予測値の不確実性も推定することができる。\n",
        "<br>\n",
        "\n",
        "#### 状態空間モデル\n",
        "入出力関係を状態空間モデルで表現し、観測値と予測値の差を異常度として用いる方法。\n",
        "<br>\n",
        "\n",
        "#### [One-Class SVM](#scrollTo=FMOqmjNLiot5&line=33&uniqifier=1)"
      ],
      "metadata": {
        "id": "NWg4xZztixiP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "### 線形回帰\n",
        "def linear_regression(input_data, output_data):\n",
        "  # 線形回帰モデルを学習\n",
        "  model = LinearRegression()\n",
        "  model.fit(input_data, output_data)\n",
        "\n",
        "  # 予測値と実際の出力値の差を計算\n",
        "  predicted_output = model.predict(input_data)\n",
        "  residuals = output_data - predicted_output\n",
        "\n",
        "  # 異常値を判定 (例：残差が大きい値を異常値とする)\n",
        "  threshold = np.mean(residuals) + 2 * np.std(residuals)  # 閾値は調整が必要\n",
        "  anomalies = input_data[np.abs(residuals) > threshold]\n",
        "\n",
        "  return anomalies\n",
        "\n",
        "\n",
        "### リッジ回帰\n",
        "def ridge_regression(input_data, output_data):\n",
        "  # リッジ回帰モデルを学習\n",
        "  model = Ridge(alpha=1.0)  # alphaは正則化の強さを指定\n",
        "  model.fit(input_data, output_data)\n",
        "\n",
        "  # 予測値と実際の出力値の差を計算\n",
        "  predicted_output = model.predict(input_data)\n",
        "  residuals = output_data - predicted_output\n",
        "\n",
        "  # 異常値を判定 (例：残差が大きい値を異常値とする)\n",
        "  threshold = np.mean(residuals) + 2 * np.std(residuals)  # 閾値は調整が必要\n",
        "  anomalies = input_data[np.abs(residuals) > threshold]\n",
        "\n",
        "  return anomalies\n",
        "\n",
        "\n",
        "# from sklearn.svm import OneClassSVM\n",
        "\n",
        "# # One-Class SVM\n",
        "#    # 入力データと出力データを結合\n",
        "#    data = np.concatenate([input_data, output_data.reshape(-1, 1)], axis=1)\n",
        "\n",
        "#    # One-Class SVMモデルを学習\n",
        "#    model = OneClassSVM(nu=0.1)  # nuは異常値の割合を指定\n",
        "#    model.fit(data)\n",
        "\n",
        "#    # 異常度を算出\n",
        "#    anomaly_scores = model.decision_function(data)\n",
        "\n",
        "#    # 異常値を判定 (例：異常度が負の値を異常値とする)\n",
        "#    anomalies = data[anomaly_scores < 0]\n",
        "\n",
        "\n",
        "# 入力データと出力データの例\n",
        "input_data = np.array([[10, 20], [12, 22], [13, 23], [11, 21], [12, 23],\n",
        "                       [11, 22], [13, 24], [10, 21], [5, 10], [12, 22]])  # [5, 10] が異常値\n",
        "output_data = np.array([30, 34, 36, 32, 35, 33, 37, 31, 15, 34])\n",
        "all_data = np.concatenate([input_data, output_data.reshape(-1, 1)], axis=1)\n",
        "\n",
        "#### 線形回帰\n",
        "anomalies = linear_regression(input_data, output_data)\n",
        "print(\"================== 線形回帰 ================\")\n",
        "print(\"異常値:\", anomalies)\n",
        "print(\"\\n\")\n",
        "\n",
        "#### リッジ回帰\n",
        "anomalies = ridge_regression(input_data, output_data)\n",
        "print(\"================= リッジ回帰 ================\")\n",
        "print(\"異常値:\", anomalies)\n",
        "print(\"\\n\")\n",
        "\n",
        "#### One-Class SVM\n",
        "anomalies = one_class_svm(all_data)\n",
        "print(\"=============== One-Class SVM ==============\")\n",
        "print(\"異常値:\", anomalies)\n",
        "print(\"\\n\")"
      ],
      "metadata": {
        "id": "l6EqJ9RmO4q6",
        "outputId": "a2d4438b-7f29-44f4-ad93-c24bb6463730",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================== 線形回帰 ================\n",
            "異常値: []\n",
            "\n",
            "\n",
            "================= リッジ回帰 ================\n",
            "異常値: []\n",
            "\n",
            "\n",
            "=============== One-Class SVM ==============\n",
            "異常値: []\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 時系列データの異常検知\n",
        "### 例\n",
        "サーバーのCPU使用率の異常検知\n",
        "\n",
        "### 手法\n",
        "\n",
        "#### 自己回帰モデル\n",
        "時系列データの過去の値を用いて、未来の値を予測するモデル。予測値と実測値の差が大きい場合に異常と判定する。\n",
        "<br>\n",
        "\n",
        "#### 移動平均法\n",
        "一定期間のデータの平均値を計算し、その平均値から大きく外れた値を異常とみなす手法。\n",
        "<br>\n",
        "\n",
        "#### 指数平滑法\n",
        "直近のデータに大きな重みを置くことで、移動平均法よりも長期的な変動に対応できる手法。\n",
        "<br>\n",
        "\n",
        "#### ARIMAモデル\n",
        "時系列データの自己相関を考慮したモデルで、予測値と実測値の差が大きい場合、異常とみなす。\n",
        "<br>\n",
        "\n",
        "#### [$k$近傍法](#scrollTo=FMOqmjNLiot5&line=7&uniqifier=1)\n",
        "<br>\n",
        "\n",
        "#### サポートベクターマシン (SVM)\n",
        "正常データと異常データを分離する超平面を学習する手法。[One-Class SVM](#scrollTo=FMOqmjNLiot5&line=33&uniqifier=1)は正常データのみを用いる。\n",
        "<br>\n",
        "\n",
        "#### [孤立フォレスト (Isolation Forest)](#scrollTo=FMOqmjNLiot5&line=7&uniqifier=1)\n",
        "<br>\n",
        "\n",
        "#### LSTM (Long Short-Term Memory)\n",
        "<br>\n",
        "\n",
        "#### 特異スペクトル変換法\n",
        "時系列データの長期的な依存関係を学習できるニューラルネットワーク。正常な時系列データを学習し、学習データと大きく異なるパターンを持つデータを異常とみなす。\n",
        "<br>\n",
        "\n",
        "#### [状態空間モデル](#scrollTo=NWg4xZztixiP&line=23&uniqifier=1)\n",
        "\n",
        "#### Autoencoder\n",
        "データを低次元空間に圧縮し、復元するニューラルネットワーク。正常な時系列データを学習し、復元誤差が大きいデータを異常とみなす。\n",
        "<br>\n",
        "\n",
        "#### Variational Autoencoder (VAE)\n",
        "Autoencoderを確率モデルとして拡張した手法。\n",
        "<br>\n",
        "\n",
        "#### Generative Adversarial Networks (GANs)\n",
        " 正常な時系列データを生成する生成器と、生成されたデータが正常か異常かを判定する識別器を競合的に学習させる手法。識別器が正常と判定できないデータを異常とみなす。\n",
        " <br>"
      ],
      "metadata": {
        "id": "n1WuKOxxizq4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 移動平均法\n",
        "import numpy as np\n",
        "\n",
        "def moving_average_method(data, window_size):\n",
        "    # 移動平均を計算\n",
        "    moving_average = np.convolve(data, np.ones(window_size) / window_size, mode='valid')\n",
        "\n",
        "    # 異常値を判定 (例：移動平均から3σ以上外れた値を異常値とする)\n",
        "    threshold = 3 * np.std(data - moving_average)  # 閾値は調整が必要\n",
        "    anomalies = data[np.abs(data - moving_average) > threshold]\n",
        "\n",
        "    return anomalies\n",
        "\n",
        "\n",
        "# CPU使用率データの例\n",
        "cpu_usage = np.array([20, 22, 25, 23, 21, 24, 26, 28, 80, 25])  # 80 が異常値\n",
        "\n",
        "# 異常値を検出\n",
        "anomalies = moving_average_method(cpu_usage, window_size=3)\n",
        "\n",
        "# ARIMAモデル\n",
        "import numpy as np\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "\n",
        "def arima_model(data, order):\n",
        "    # ARIMAモデルを学習\n",
        "    model = ARIMA(data, order=order)\n",
        "    model_fit = model.fit()\n",
        "\n",
        "    # 予測値と実測値の差を計算\n",
        "    residuals = model_fit.resid\n",
        "\n",
        "    # 異常値を判定 (例：残差が大きい値を異常値とする)\n",
        "    threshold = 3 * np.std(residuals)  # 閾値は調整が必要\n",
        "    anomalies = data[np.abs(residuals) > threshold]\n",
        "\n",
        "    return anomalies\n",
        "\n",
        "# CPU使用率データの例\n",
        "cpu_usage = np.array([20, 22, 25, 23, 21, 24, 26, 28, 80, 25])  # 80 が異常値\n",
        "\n",
        "# 異常値を検出\n",
        "anomalies = arima_model(cpu_usage, order=(5, 1, 0))  # orderはモデルの次数を指定\n",
        "\n",
        "\n",
        "# LSTM (Long Short-Term Memory)\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "\n",
        "def lstm_model(data, timesteps):\n",
        "    # データをLSTMモデルの入力形式に変換\n",
        "    X = []\n",
        "    y = []\n",
        "    for i in range(timesteps, len(data)):\n",
        "        X.append(data[i - timesteps:i])\n",
        "        y.append(data[i])\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "\n",
        "    # LSTMモデルを構築\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(units=50, return_sequences=True, input_shape=(timesteps, 1)))\n",
        "    model.add(LSTM(units=50))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "    # LSTMモデルを学習\n",
        "    model.fit(X, y, epochs=100, batch_size=32)\n",
        "\n",
        "    # 予測値と実測値の差を計算\n",
        "    predictions = model.predict(X)\n",
        "    residuals = y - predictions.flatten()\n",
        "\n",
        "    # 異常値を判定 (例：残差が大きい値を異常値とする)\n",
        "    threshold = 3 * np.std(residuals)  # 閾値は調整が必要\n",
        "    anomalies = data[np.abs(residuals) > threshold]\n",
        "\n",
        "    return anomalies\n",
        "\n",
        "# CPU使用率データの例\n",
        "cpu_usage = np.array([20, 22, 25, 23, 21, 24, 26, 28, 80, 25])  # 80 が異常値\n",
        "\n",
        "# 異常値を検出\n",
        "anomalies = lstm_model(cpu_usage, timesteps=3)  # timestepsは過去のデータを使用する期間を指定\n"
      ],
      "metadata": {
        "id": "Sp81D4inPF0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 変数間に関係があるデータの異常検知\n",
        "### 例\n",
        "クレジットカードの不正利用検知\n",
        "\n",
        "### 手法\n",
        "#### 疎構造学習\n",
        "高次元データにおいて、変数間の関係が疎であることを仮定して、データの構造を学習する手法。正常なデータにおける変数間の関係を疎構造として学習し、その構造から外れるデータを異常とみなす。\n",
        "<br>\n",
        "\n",
        "#### ベイジアンネットワーク\n",
        "変数間の因果関係を表現したグラフィカルモデル。変数間の条件付き確率を学習し、異常な条件付き確率を持つデータを異常とみなす。\n",
        "<br>\n",
        "\n",
        "#### マルコフ確率場\n",
        "変数間の相互作用を表現したグラフィカルモデル。変数間のポテンシャル関数を学習し、異常なポテンシャル関数を持つデータを異常とみなす。\n",
        "<br>\n",
        "\n",
        "#### [k-means法](#scrollTo=FMOqmjNLiot5&line=7&uniqifier=1)\n",
        "<br>\n",
        "\n",
        "#### DBSCAN\n",
        "密度ベースのクラスタリングアルゴリズム。データポイントを、高密度領域にあるデータポイントと低密度領域にあるデータポイント（ノイズ）に分類し、低密度領域にあるデータポイントを異常とみなす。\n",
        "<br>\n",
        "\n",
        "#### [One-Class SVM](#scrollTo=FMOqmjNLiot5&line=33&uniqifier=1)\n",
        "<br>\n",
        "\n",
        "#### [孤立フォレスト (Isolation Forest)](#scrollTo=FMOqmjNLiot5&line=7&uniqifier=1)\n",
        "<br>\n",
        "\n",
        "#### [Local Outlier Factor (LOF)](#scrollTo=FMOqmjNLiot5&line=33&uniqifier=1)\n",
        "<br>"
      ],
      "metadata": {
        "id": "42XygjKgi1od"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# One-Class SVM\n",
        "from sklearn.svm import OneClassSVM\n",
        "\n",
        "# 取引データの例\n",
        "transaction_data = [[100, 10, 1], [120, 12, 2], [130, 13, 1], [110, 11, 3], [125, 12, 2],\n",
        "                    [115, 11, 1], [135, 13, 3], [105, 10, 2], [1000, 20, 5], [120, 12, 1]]  # [1000, 20, 5] が不正利用\n",
        "\n",
        "# One-Class SVMモデルを学習\n",
        "model = OneClassSVM(nu=0.1)  # nuは異常値の割合を指定\n",
        "model.fit(transaction_data)\n",
        "\n",
        "# 異常度を算出\n",
        "anomaly_scores = model.decision_function(transaction_data)\n",
        "\n",
        "# 異常値を判定 (例：異常度が負の値を不正利用と判定)\n",
        "fraudulent_transactions = [transaction_data[i] for i, score in enumerate(anomaly_scores) if score < 0]\n",
        "\n",
        "# 孤立フォレスト (Isolation Forest)\n",
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "# 取引データの例\n",
        "transaction_data = [[100, 10, 1], [120, 12, 2], [130, 13, 1], [110, 11, 3], [125, 12, 2],\n",
        "                    [115, 11, 1], [135, 13, 3], [105, 10, 2], [1000, 20, 5], [120, 12, 1]]  # [1000, 20, 5] が不正利用\n",
        "\n",
        "# Isolation Forestモデルを学習\n",
        "model = IsolationForest()\n",
        "model.fit(transaction_data)\n",
        "\n",
        "# 異常度を算出\n",
        "anomaly_scores = model.decision_function(transaction_data)\n",
        "\n",
        "# 異常値を判定 (例：異常度が低い値を不正利用と判定)\n",
        "fraudulent_transactions = [transaction_data[i] for i, score in enumerate(anomaly_scores) if score < -0.35]  # 閾値は調整が必要\n",
        "\n",
        "# Local Outlier Factor (LOF)\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "\n",
        "# 取引データの例\n",
        "transaction_data = [[100, 10, 1], [120, 12, 2], [130, 13, 1], [110, 11, 3], [125, 12, 2],\n",
        "                    [115, 11, 1], [135, 13, 3], [105, 10, 2], [1000, 20, 5], [120, 12, 1]]  # [1000, 20, 5] が不正利用\n",
        "\n",
        "# Local Outlier Factorモデルを学習\n",
        "model = LocalOutlierFactor(n_neighbors=20)  # n_neighborsはkの値を指定\n",
        "anomaly_scores = model.fit_predict(transaction_data)\n",
        "\n",
        "# 異常値を判定 (例：異常度が-1の値を不正利用と判定)\n",
        "fraudulent_transactions = [transaction_data[i] for i, score in enumerate(anomaly_scores) if score == -1]\n"
      ],
      "metadata": {
        "id": "UbHIYYDNPGap"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}