{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP4I5Lza3fZQaOt2GrGbAiB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hsswkwk/turbo-chainsaw/blob/feature-add-nlp/notebooks/nlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 自然言語処理（Natural language processing, NLP）\n",
        "人間の言語をコンピュータに理解させ、様々な処理を行わせるための技術。\n",
        "<br>\n",
        "\n",
        "## 単語分割\n",
        "テキストを単語や文節といった意味を持つ最小単位に分けるプロセス。\n",
        "\n",
        "### 形態素解析\n",
        "テキストを単語や文節などの意味を持つ最小単位（形態素）に分割する。JanomeやMeCabといったライブラリが使われる。\n",
        "\n",
        "### N-gram解析\n",
        "テキストをn個連続する文字や単語の並びとして分割する手法。<br>\n",
        "例）「自然言語処理」というテキストを2-gramで分割すると、「自然」「然言」「言語」「語処」「処理」\n",
        "\n",
        "<br>\n",
        "\n",
        "## 構文解析\n",
        "単語分割によって分けられた単語や文節が、文中でどのような文法的な関係にあるかを解析し、文全体の構造を明らかにするプロセス。CaboChaやGinza、Stanzaといったライブラリが使われる。\n",
        "\n",
        "### 係り受け解析\n",
        "文中の単語や文節間の修飾・被修飾の関係（どの単語がどの単語にかかっているか）を明らかにする。\n",
        "\n",
        "### 句構造解析\n",
        "文を、名詞句、動詞句、形容詞句などの句に分解し、それぞれの句がどのように階層的に構成されているかを解析する。\n",
        "\n",
        "<br>\n",
        "\n",
        "## 意味解析\n",
        "構文解析によって明らかになった文の構造や、単語そのものが持つ意味に基づいて、文全体の意味を理解するプロセス。\n",
        "\n",
        "### 語彙意味論\n",
        "個々の単語が持つ意味や、単語間の意味的な関係性（同義語、類義語、上位語、下位語など）を扱う。WordNetのような意味辞書や、Word2Vec、fastTextのような単語の分散表現（単語の意味をベクトルで表現する手法）が用いられる。\n",
        "\n",
        "### 語義曖昧性解消\n",
        "文脈の中で複数の意味を持ちうる単語（多義語）が、具体的にどの意味で使用されているかを解析する。\n",
        "\n",
        "### 述語項構造解析\n",
        "文中の動詞（述語）に対して、その動詞が要求する要素（誰が、何を、いつ、どこで、どのようになど）を特定し、それぞれの要素が文中のどの単語や句に対応するのかを解析する。\n",
        "\n",
        "### BERT\n",
        "Googleが開発した自然言語処理のためのTransformerベースの強力な事前学習済みモデル。\n",
        "BERTはテキストを双方向に処理していて単語の周囲の文脈全体をより深く理解できる。また、大量のテキストデータを用いて、Masked Language Model (MLM)、Next Sentence Prediction (NSP)を学習する。\n",
        "\n",
        "#### Masked Language Model (MLM)\n",
        "入力文の一部の単語をマスク（隠す）し、マスクされた単語を予測するように学習する。\n",
        "\n",
        "#### Next Sentence Prediction (NSP)\n",
        "2つの文の関係性（次の文かどうか）を予測するように学習する。\n",
        "\n",
        "<br>\n",
        "\n",
        "## 文脈解析\n",
        "文や単語が置かれている文脈、つまり前後の文章や周囲の単語、あるいは会話や文書全体といったより広範な情報を考慮して、その意味や意図を理解するプロセス。spaCy、Hugging Face Transformers、AllenNLP、Flairといったライブラリが使われる。\n",
        "<br>\n",
        "文脈解析は以下のような場面で必要となる。\n",
        "#### 多義語の解消\n",
        "文脈によって意味が変わる単語の正しい意味を特定する。\n",
        "\n",
        "#### 代名詞の参照解決\n",
        "「彼」「彼女」「それ」といった代名詞が、文中のどの名詞を指しているのかを特定する。\n",
        "\n",
        "#### 省略の補完\n",
        "会話や文章で省略されている情報を、文脈から推測して補完する。\n",
        "\n",
        "#### 発話意図の理解\n",
        "単に発言内容だけでなく、その発言の裏にある意図（質問、指示、皮肉など）を理解する。\n",
        "\n",
        "<br>\n",
        "\n",
        "## 感情分析\n",
        "テキストデータに含まれる感情、意見、態度などの主観的な情報を抽出・分析するプロセス。TextBlob、VaderSentiment、Flairといったライブラリが使われる。\n",
        "\n",
        "#### ルールベースのアプローチ\n",
        "感情を示す単語やフレーズの辞書を作成し、それに基づいて感情を判断する。\n",
        "\n",
        "#### 機械学習ベースのアプローチ\n",
        "大量の感情ラベル付きデータを用いて、感情分類モデルを学習させる。\n",
        "\n",
        "#### ハイブリッドアプローチ\n",
        "ルールベースと機械学習ベースのアプローチを組み合わせる。\n",
        "\n",
        "<br>\n",
        "\n",
        "## トピック分析\n",
        "大量のテキストデータから、そのデータに含まれる主要なトピック（話題）を抽出するプロセス。Gensim、sklearnといったライブラリが使われる。\n",
        "\n",
        "### 潜在ディリクレ配分法 (Latent Dirichlet Allocation, LDA)\n",
        "文書群の中に隠れたテーマ構造を発見するために用いられる確率的トピックモデリング手法。\n",
        "\n",
        "<br>\n",
        "\n",
        "## 応用\n",
        "* テキストの要約\n",
        "* 機械翻訳\n",
        "* 文章生成\n",
        "* 質問応答\n",
        "* 情報抽出\n",
        "* 文字変換予測\n"
      ],
      "metadata": {
        "id": "zDdsCj1f1MG4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "cI5Ph4XNzV4x",
        "outputId": "61f2967c-67a5-4bec-cbba-1d737ee67fa4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "これ これ PRON nsubj\n",
            "は は ADP case\n",
            "テスト テスト NOUN nmod\n",
            "の の ADP case\n",
            "文章 文章 NOUN ROOT\n",
            "です です AUX cop\n",
            "。 。 PUNCT punct\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-5775060a7a7a>:34: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
            "  similarity_matrix[i][j] = nlp(sent1).similarity(nlp(sent2))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "要約:\n",
            "このように、日本の自然は四季折々の魅力にあふれています。 秋になると紅葉が山々を彩り、冬には雪景色を楽しむことができます。 春には桜が咲き、人々は花見を楽しみます。\n"
          ]
        }
      ],
      "source": [
        "# spaCy\n",
        "# https://spacy.io\n",
        "#\n",
        "# 効率的な自然言語処理（NLP）パイプラインを構築するために設計されたライブラリ\n",
        "#\n",
        "# * 75以上の言語をサポート\n",
        "# * 25言語に対応する84の学習済みパイプライン\n",
        "# * BERTなどの事前学習済みトランスフォーマーを使用したマルチタスク学習\n",
        "# * 事前学習済み単語ベクトル\n",
        "# * 最先端の速度\n",
        "# * 本番環境で使用可能な学習システム\n",
        "# * 言語学的に動機付けられたトークン化\n",
        "# * 固有表現抽出、品詞タグ付け、依存関係解析、文分割、テキスト分類、見出し語化、形態素解析、エンティティリンクなどのコンポーネント\n",
        "# * カスタムコンポーネントと属性で簡単に拡張可能\n",
        "# * PyTorch、TensorFlow、その他のフレームワークのカスタムモデルをサポート\n",
        "# * 構文とNER用のビジュアライザーを内蔵\n",
        "# * モデルのパッケージ化、デプロイ、ワークフロー管理が容易\n",
        "# * 堅牢で厳密に評価された精度\n",
        "#\n",
        "\n",
        "\n",
        "# spaCyを使用した文章の要約\n",
        "\n",
        "!pip install -U spacy -q\n",
        "!pip install spacy[ja] -q\n",
        "!pip install ja-ginza -q\n",
        "\n",
        "import collections\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import networkx as nx\n",
        "\n",
        "# モデルのロード\n",
        "nlp = spacy.load(\"ja_ginza\")\n",
        "\n",
        "# spaCyを使った処理の例\n",
        "text = \"これはテストの文章です。\"\n",
        "doc = nlp(text)\n",
        "\n",
        "for token in doc:\n",
        "    print(token.text, token.lemma_, token.pos_, token.dep_)\n",
        "\n",
        "\n",
        "# 参考: https://qiita.com/automation2025/items/4cf2d5bf7276f05ed665\n",
        "def textrank_summarize(text, num_sentences=3):\n",
        "    nlp = spacy.load(\"ja_ginza\")\n",
        "    doc = nlp(text)\n",
        "    sentences = [sent.text.strip() for sent in doc.sents]\n",
        "\n",
        "    # 文同士の類似度を計算\n",
        "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
        "    for i, sent1 in enumerate(sentences):\n",
        "        for j, sent2 in enumerate(sentences):\n",
        "            if i != j:\n",
        "                similarity_matrix[i][j] = nlp(sent1).similarity(nlp(sent2))\n",
        "\n",
        "    # グラフを作成\n",
        "    nx_graph = nx.from_numpy_array(similarity_matrix)\n",
        "    scores = nx.pagerank(nx_graph)\n",
        "\n",
        "    # スコアの高い文を抽出\n",
        "    ranked_sentences = sorted(((scores[i], s) for i, s in enumerate(sentences)), reverse=True)\n",
        "    summary = \" \".join([ranked_sentences[i][1] for i in range(min(num_sentences, len(ranked_sentences)))])\n",
        "\n",
        "    return summary\n",
        "\n",
        "text = \"\"\"\n",
        "日本の四季は美しいです。春には桜が咲き、人々は花見を楽しみます。\n",
        "夏は蝉の声が聞こえ、花火大会が各地で開催されます。\n",
        "秋になると紅葉が山々を彩り、冬には雪景色を楽しむことができます。\n",
        "このように、日本の自然は四季折々の魅力にあふれています。\n",
        "\"\"\"\n",
        "\n",
        "summary = textrank_summarize(text)\n",
        "print(\"要約:\")\n",
        "print(summary)"
      ]
    }
  ]
}