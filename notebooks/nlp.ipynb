{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM92vSB1jz+iAAze/lZPlXu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hsswkwk/turbo-chainsaw/blob/feature-add-nlp/notebooks/nlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 自然言語処理（Natural language processing, NLP）\n",
        "人間の言語をコンピュータに理解させ、様々な処理を行わせるための技術。\n",
        "<br>\n",
        "\n",
        "## 単語分割\n",
        "テキストを単語や文節といった意味を持つ最小単位に分けるプロセス。\n",
        "\n",
        "### 形態素解析\n",
        "テキストを単語や文節などの意味を持つ最小単位（形態素）に分割する。JanomeやMeCabといったライブラリが使われる。\n",
        "\n",
        "### N-gram解析\n",
        "テキストをn個連続する文字や単語の並びとして分割する手法。<br>\n",
        "例）「自然言語処理」というテキストを2-gramで分割すると、「自然」「然言」「言語」「語処」「処理」\n",
        "\n",
        "<br>\n",
        "\n",
        "## 構文解析\n",
        "単語分割によって分けられた単語や文節が、文中でどのような文法的な関係にあるかを解析し、文全体の構造を明らかにするプロセス。CaboChaやGinza、Stanzaといったライブラリが使われる。\n",
        "\n",
        "### 係り受け解析\n",
        "文中の単語や文節間の修飾・被修飾の関係（どの単語がどの単語にかかっているか）を明らかにする。\n",
        "\n",
        "### 句構造解析\n",
        "文を、名詞句、動詞句、形容詞句などの句に分解し、それぞれの句がどのように階層的に構成されているかを解析する。\n",
        "\n",
        "<br>\n",
        "\n",
        "## 意味解析\n",
        "構文解析によって明らかになった文の構造や、単語そのものが持つ意味に基づいて、文全体の意味を理解するプロセス。\n",
        "\n",
        "### 語彙意味論\n",
        "個々の単語が持つ意味や、単語間の意味的な関係性（同義語、類義語、上位語、下位語など）を扱う。WordNetのような意味辞書や、Word2Vec、fastTextのような単語の分散表現（単語の意味をベクトルで表現する手法）が用いられる。\n",
        "\n",
        "### 語義曖昧性解消\n",
        "文脈の中で複数の意味を持ちうる単語（多義語）が、具体的にどの意味で使用されているかを解析する。\n",
        "\n",
        "### 述語項構造解析\n",
        "文中の動詞（述語）に対して、その動詞が要求する要素（誰が、何を、いつ、どこで、どのようになど）を特定し、それぞれの要素が文中のどの単語や句に対応するのかを解析する。\n",
        "\n",
        "### BERT\n",
        "Googleが開発した自然言語処理のためのTransformerベースの強力な事前学習済みモデル。\n",
        "BERTはテキストを双方向に処理していて単語の周囲の文脈全体をより深く理解できる。また、大量のテキストデータを用いて、Masked Language Model (MLM)、Next Sentence Prediction (NSP)を学習する。\n",
        "\n",
        "#### Masked Language Model (MLM)\n",
        "入力文の一部の単語をマスク（隠す）し、マスクされた単語を予測するように学習する。\n",
        "\n",
        "#### Next Sentence Prediction (NSP)\n",
        "2つの文の関係性（次の文かどうか）を予測するように学習する。\n",
        "\n",
        "<br>\n",
        "\n",
        "## 文脈解析\n",
        "文や単語が置かれている文脈、つまり前後の文章や周囲の単語、あるいは会話や文書全体といったより広範な情報を考慮して、その意味や意図を理解するプロセス。spaCy、Hugging Face Transformers、AllenNLP、Flairといったライブラリが使われる。\n",
        "<br>\n",
        "文脈解析は以下のような場面で必要となる。\n",
        "#### 多義語の解消\n",
        "文脈によって意味が変わる単語の正しい意味を特定する。\n",
        "\n",
        "#### 代名詞の参照解決\n",
        "「彼」「彼女」「それ」といった代名詞が、文中のどの名詞を指しているのかを特定する。\n",
        "\n",
        "#### 省略の補完\n",
        "会話や文章で省略されている情報を、文脈から推測して補完する。\n",
        "\n",
        "#### 発話意図の理解\n",
        "単に発言内容だけでなく、その発言の裏にある意図（質問、指示、皮肉など）を理解する。\n",
        "\n",
        "<br>\n",
        "\n",
        "## 感情分析\n",
        "テキストデータに含まれる感情、意見、態度などの主観的な情報を抽出・分析するプロセス。TextBlob、VaderSentiment、Flairといったライブラリが使われる。\n",
        "\n",
        "#### ルールベースのアプローチ\n",
        "感情を示す単語やフレーズの辞書を作成し、それに基づいて感情を判断する。\n",
        "\n",
        "#### 機械学習ベースのアプローチ\n",
        "大量の感情ラベル付きデータを用いて、感情分類モデルを学習させる。\n",
        "\n",
        "#### ハイブリッドアプローチ\n",
        "ルールベースと機械学習ベースのアプローチを組み合わせる。\n",
        "\n",
        "<br>\n",
        "\n",
        "## トピック分析\n",
        "大量のテキストデータから、そのデータに含まれる主要なトピック（話題）を抽出するプロセス。Gensim、sklearnといったライブラリが使われる。\n",
        "\n",
        "### 潜在ディリクレ配分法 (Latent Dirichlet Allocation, LDA)\n",
        "文書群の中に隠れたテーマ構造を発見するために用いられる確率的トピックモデリング手法。\n",
        "\n",
        "<br>\n",
        "\n",
        "## 応用\n",
        "* テキストの要約\n",
        "* 機械翻訳\n",
        "* 文章生成\n",
        "* 質問応答\n",
        "* 情報抽出\n",
        "* 文字変換予測\n",
        "など"
      ],
      "metadata": {
        "id": "zDdsCj1f1MG4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# spaCy\n",
        "# https://spacy.io\n",
        "#\n",
        "# 効率的な自然言語処理（NLP）パイプラインを構築するために設計されたライブラリ\n",
        "#\n",
        "# * 75以上の言語をサポート\n",
        "# * 25言語に対応する84の学習済みパイプライン\n",
        "# * BERTなどの事前学習済みトランスフォーマーを使用したマルチタスク学習\n",
        "# * 事前学習済み単語ベクトル\n",
        "# * 最先端の速度\n",
        "# * 本番環境で使用可能な学習システム\n",
        "# * 言語学的に動機付けられたトークン化\n",
        "# * 固有表現抽出、品詞タグ付け、依存関係解析、文分割、テキスト分類、見出し語化、形態素解析、エンティティリンクなどのコンポーネント\n",
        "# * カスタムコンポーネントと属性で簡単に拡張可能\n",
        "# * PyTorch、TensorFlow、その他のフレームワークのカスタムモデルをサポート\n",
        "# * 構文とNER用のビジュアライザーを内蔵\n",
        "# * モデルのパッケージ化、デプロイ、ワークフロー管理が容易\n",
        "# * 堅牢で厳密に評価された精度\n",
        "#\n",
        "!pip install -U spacy -q\n",
        "!pip install spacy[ja] -q\n",
        "!pip install ja-ginza -q\n",
        "\n",
        "import collections\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import spacy\n",
        "from spacy.tokens import Doc\n",
        "\n",
        "\n",
        "nlp = spacy.load(\"ja_ginza\")\n",
        "\n",
        "\n",
        "# https://news.yahoo.co.jp/articles/a4642672635a37b14522e345769da9e9bad89c1e\n",
        "text = \"\"\"\n",
        "人気キャラクター「ちいかわ」のおもちゃが付くマクドナルドのハッピーセットを転売目的で購入する人が多発し、キャンペーンが早期終了となった。\n",
        "桜美林大学准教授の西山守さんは「最近のハッピーセットの特典は大人ウケを狙ったものや、ハンバーガーの価値を超えたものが多く見受けられる。\n",
        "『お子さまセット』としての原点に立ち返るべきではないか」という――。\n",
        "■ハッピーセットが前倒しで終了\n",
        "日本マクドナルドは5月24日、ハッピーセットの「マインクラフト ザ・ムービー」「ちいかわ」第2弾の早期終了を発表した。同時に5月30日から開始予定の第3弾の中止も発表している。\n",
        "第1弾は5月16日に販売を開始したが、顧客が殺到し、わずか3日後の19日に販売終了した。転売が相次いだり、セットメニューと思しき商品が大量廃棄されたりする問題も起こっていた。\n",
        "第2弾においても、フリマサイトで大量の転売が起きるに至っている。\n",
        "なぜ、マクドナルドは第1弾のトラブルを知りながら、十分な対応を取ることができなかったのだろうか？　今後、こうしたことが起こらないためには、どのような対策を講じるべきなのだろうか？\n",
        "■過去の教訓は活かせなかったのか？\n",
        "先述の通り、第2弾で起きた買い占めや転売の問題は、すでに第1弾でも起きている。\n",
        "第2弾開始の際は、“1人4セット”という制限を設け、「転売または再販売、その他営利を目的としたご購入はお控えください」といった周知を行ったが、効果は限定的だったようだ。\n",
        "この対応は、実は第1弾と同じであり、それを第2弾で改めて広く周知したに過ぎない。購入数を1人あたり1セットなり、2セットに減らすなり、販売対象を子供に限定するなりの対応が取れなかったのだろうか？\n",
        "そうすれば、買い占めや転売にも手間がかかるので、一定の効果を上げることはできたのではないかと思う。\n",
        "第2弾開始までの期間が短いため、購入条件に変更を加えることが難しかったのかもしれない。マクドナルドの店舗数は、日本全国で約3000店もある。全店舗津々浦々に変更を周知し、\n",
        "十全な対応をするのは容易なことではなかったのではないだろうか。\n",
        "ただ、マクドナルド側で何らかの対策を講じることはできなかっただろうかという疑問は残される。\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "OeUqR6XaQSr_"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# spaCyを使用したキーワード抽出\n",
        "# 参考: https://qiita.com/automation2025/items/4cf2d5bf7276f05ed665#%E7%AC%AC12%E7%AB%A0-%E3%82%AD%E3%83%BC%E3%83%AF%E3%83%BC%E3%83%89%E6%8A%BD%E5%87%BA\n",
        "\n",
        "def preprocess(nlp, text):\n",
        "    doc = nlp(text)\n",
        "    return \" \".join([token.lemma_ for token in doc if not token.is_stop and token.pos_ != \"PUNCT\"])\n",
        "\n",
        "preprocessed_text = preprocess(nlp, text)\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform([preprocessed_text])\n",
        "\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "feature_index = tfidf_matrix[0,:].nonzero()[1]\n",
        "tfidf_scores = zip(feature_index, [tfidf_matrix[0, x] for x in feature_index])\n",
        "for word, score in [(feature_names[i], s) for (i, s) in tfidf_scores]:\n",
        "  print(f\"\\t{word}: {score:.3f}\")\n"
      ],
      "metadata": {
        "id": "91JvCk84QeDa",
        "outputId": "93591db5-a487-4529-e409-9258fa14500b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t人気: 0.050\n",
            "\tキャラクター: 0.050\n",
            "\tちい: 0.100\n",
            "\tおもちゃ: 0.050\n",
            "\t付く: 0.050\n",
            "\tマクドナルド: 0.249\n",
            "\tハッピー: 0.199\n",
            "\tセット: 0.448\n",
            "\t転売: 0.299\n",
            "\t目的: 0.100\n",
            "\t購入: 0.199\n",
            "\t多発: 0.050\n",
            "\tキャンペーン: 0.050\n",
            "\t早期: 0.100\n",
            "\t終了: 0.199\n",
            "\t桜美林大学: 0.050\n",
            "\t准教授: 0.050\n",
            "\t西山: 0.050\n",
            "\t最近: 0.050\n",
            "\t特典: 0.050\n",
            "\t大人: 0.050\n",
            "\tウケ: 0.050\n",
            "\t狙う: 0.050\n",
            "\tハンバーガー: 0.050\n",
            "\t価値: 0.050\n",
            "\t超える: 0.050\n",
            "\t多い: 0.050\n",
            "\t見受ける: 0.050\n",
            "\tお子さま: 0.050\n",
            "\t原点: 0.050\n",
            "\t立ち返る: 0.050\n",
            "\t前倒し: 0.050\n",
            "\t日本: 0.100\n",
            "\t24: 0.050\n",
            "\tマイン: 0.050\n",
            "\tクラフト: 0.050\n",
            "\tムービー: 0.050\n",
            "\t発表: 0.100\n",
            "\t同時: 0.050\n",
            "\t30: 0.050\n",
            "\t開始: 0.199\n",
            "\t予定: 0.050\n",
            "\t中止: 0.050\n",
            "\t16: 0.050\n",
            "\t販売: 0.149\n",
            "\t顧客: 0.050\n",
            "\t殺到: 0.050\n",
            "\tわずか: 0.050\n",
            "\t19: 0.050\n",
            "\t次ぐ: 0.050\n",
            "\tだり: 0.050\n",
            "\tメニュー: 0.050\n",
            "\t思し: 0.050\n",
            "\t商品: 0.050\n",
            "\t大量: 0.100\n",
            "\t廃棄: 0.050\n",
            "\t問題: 0.100\n",
            "\t起こる: 0.100\n",
            "\tフリマ: 0.050\n",
            "\tサイト: 0.050\n",
            "\t起きる: 0.149\n",
            "\t至る: 0.050\n",
            "\tなぜ: 0.050\n",
            "\tトラブル: 0.050\n",
            "\t知る: 0.050\n",
            "\t十分: 0.050\n",
            "\t対応: 0.199\n",
            "\t取る: 0.050\n",
            "\t今後: 0.050\n",
            "\tどのような: 0.050\n",
            "\t対策: 0.100\n",
            "\t講じる: 0.100\n",
            "\t過去: 0.050\n",
            "\t教訓: 0.050\n",
            "\t活かせる: 0.050\n",
            "\t先述: 0.050\n",
            "\t通り: 0.050\n",
            "\t買い占め: 0.100\n",
            "\tすでに: 0.050\n",
            "\t1人: 0.100\n",
            "\t制限: 0.050\n",
            "\t設ける: 0.050\n",
            "\t再販売: 0.050\n",
            "\t営利: 0.050\n",
            "\t控える: 0.050\n",
            "\tくださる: 0.050\n",
            "\t周知: 0.149\n",
            "\t行う: 0.050\n",
            "\t効果: 0.100\n",
            "\t限定的: 0.050\n",
            "\t同じ: 0.050\n",
            "\t改めて: 0.050\n",
            "\t広い: 0.050\n",
            "\t過ぎる: 0.050\n",
            "\tあたり: 0.050\n",
            "\t減らす: 0.050\n",
            "\t対象: 0.050\n",
            "\t子供: 0.050\n",
            "\t限定: 0.050\n",
            "\t取れる: 0.050\n",
            "\tする: 0.050\n",
            "\t手間: 0.050\n",
            "\tかかる: 0.050\n",
            "\t一定: 0.050\n",
            "\t上げる: 0.050\n",
            "\t思う: 0.050\n",
            "\t期間: 0.050\n",
            "\t短い: 0.050\n",
            "\t条件: 0.050\n",
            "\t変更: 0.100\n",
            "\t加える: 0.050\n",
            "\t難しい: 0.050\n",
            "\tしれる: 0.050\n",
            "\t店舗: 0.050\n",
            "\t全国: 0.050\n",
            "\t3000: 0.050\n",
            "\t全店舗: 0.050\n",
            "\t津々: 0.050\n",
            "\t浦々: 0.050\n",
            "\t十全: 0.050\n",
            "\t容易: 0.050\n",
            "\tただ: 0.050\n",
            "\t疑問: 0.050\n",
            "\t残す: 0.050\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "cI5Ph4XNzV4x",
        "outputId": "07389925-5792-4d79-e35c-f03c2c9da3e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-1fd148c8e29d>:12: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
            "  similarity_matrix[i][j] = nlp(sent1).similarity(nlp(sent2))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "要約:\n",
            "なぜ、マクドナルドは第1弾のトラブルを知りながら、十分な対応を取ることができなかったのだろうか？ 第2弾開始の際は、“1人4セット”という制限を設け、「転売または再販売、その他営利を目的としたご購入はお控えください」といった周知を行ったが、効果は限定的だったようだ。 第2弾開始までの期間が短いため、購入条件に変更を加えることが難しかったのかもしれない。\n"
          ]
        }
      ],
      "source": [
        "# spaCyを使用した文章の要約\n",
        "# 参考: https://qiita.com/automation2025/items/4cf2d5bf7276f05ed665\n",
        "def textrank_summarize(nlp, text, num_sentences=3):\n",
        "    doc = nlp(text)\n",
        "    sentences = [sent.text.strip() for sent in doc.sents]\n",
        "\n",
        "    # 文同士の類似度を計算\n",
        "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
        "    for i, sent1 in enumerate(sentences):\n",
        "        for j, sent2 in enumerate(sentences):\n",
        "            if i != j:\n",
        "                similarity_matrix[i][j] = nlp(sent1).similarity(nlp(sent2))\n",
        "\n",
        "    # グラフを作成\n",
        "    nx_graph = nx.from_numpy_array(similarity_matrix)\n",
        "    scores = nx.pagerank(nx_graph)\n",
        "\n",
        "    # スコアの高い文を抽出\n",
        "    ranked_sentences = sorted(((scores[i], s) for i, s in enumerate(sentences)), reverse=True)\n",
        "    summary = \" \".join([ranked_sentences[i][1] for i in range(min(num_sentences, len(ranked_sentences)))])\n",
        "\n",
        "    return summary\n",
        "\n",
        "\n",
        "summary = textrank_summarize(nlp, text)\n",
        "print(\"要約:\")\n",
        "print(summary)"
      ]
    }
  ]
}