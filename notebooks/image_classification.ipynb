{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hsswkwk/turbo-chainsaw/blob/feature-add-image-classification/notebooks/image_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 画像分類（Image Classification）\n",
        "入力した画像情報を「形」や「模様」、「色」など特徴量に分解し、入力情報から対象物を判定できるように対応付けを行う\n",
        "\n",
        "#### 畳み込み層\n",
        "特徴を抽出する層\n",
        "\n",
        "#### プーリング層\n",
        "画像サイズを小さくする層\n",
        "\n",
        "<br>\n",
        "\n",
        "## 画像分類のアーキテクチャ\n",
        "\n",
        "#### CNN（Convolutional Neural Network）\n",
        "畳み込みニューラルネットワーク（CNNまたはConvNet）は、畳み込みを使用しているニューラルネットワークの総称\n",
        "\n",
        "#### ResNet\n",
        "残差ニューラルネットワーク（ResNet）は、ウェイト層が層入力を参照して残差関数を学習する深層学習モデル\n",
        "\n",
        "#### VGG16\n",
        "ImageNetと呼ばれる大規模画像データセットで学習された16層(畳み込み13層、フル結合3層)からなる畳み込みニューラルネットワーク(CNN)\n",
        "\n",
        "#### Efficient Net\n",
        "複合スケーリング手法を用いて、ネットワークの幅、深さ、解像度を同時に最適化し、より少ないパラメータ数で高い精度を持つCNN\n"
      ],
      "metadata": {
        "id": "wm_anjAZ6g32"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models, applications\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# CIFAR-10データセットの読み込み\n",
        "(x_train, y_train), (x_test, y_test) = datasets.cifar10.load_data()\n",
        "# CIFAR-10のクラスラベル\n",
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "# データの前処理（画素値を0-1の範囲に正規化）\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes=10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes=10)\n",
        "\n",
        "# データ拡張\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=30,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "\n",
        "@tf.keras.utils.register_keras_serializable()\n",
        "class ResizeImage(tf.keras.layers.Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "\n",
        "  def call(self, image):\n",
        "    image = tf.image.resize(image, (224, 224))\n",
        "    return image\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super().get_config()\n",
        "    return config\n",
        "\n",
        "  @classmethod\n",
        "  def from_config(cls, config):\n",
        "    return cls(**config)\n",
        "\n",
        "\n",
        "class ImageClassificationModelBase:\n",
        "  def __init__(self, x_train, y_train, x_test, y_test, filepath):\n",
        "    self.filename = filepath.split('/')[-1]\n",
        "    self.name = self.filename.split('.')[0]\n",
        "    if os.path.exists(filepath):\n",
        "      self.model = tf.keras.models.load_model(filepath)\n",
        "    else:\n",
        "      self._build()\n",
        "      # optimizer: 最適化アルゴリズム, loss: 損失関数, metrics: 評価指標\n",
        "      self._compile()\n",
        "      history = self._fit(x_train, y_train, x_test, y_test)\n",
        "      self.model.save(self.filename)\n",
        "\n",
        "  def evaluate(self, x_test,  y_test):\n",
        "    test_loss, test_acc = self.model.evaluate(x_test,  y_test, verbose=2)\n",
        "    print(f'\\n{self.name}: Test accuracy: {test_acc}')\n",
        "\n",
        "  def predict(self, x):\n",
        "    return self.model.predict(np.expand_dims(x, axis=0))\n",
        "\n",
        "  def _build(self):\n",
        "    pass\n",
        "\n",
        "  def _compile(self):\n",
        "    self.model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy'])\n",
        "\n",
        "  def _fit(self, x_train, y_train, x_test, y_test):\n",
        "    history = self.model.fit(\n",
        "      datagen.flow(x_train, y_train, batch_size=32),\n",
        "      batch_size=32,\n",
        "      epochs=10,\n",
        "      validation_data=(x_test, y_test))\n",
        "    return history\n",
        "\n",
        "\n",
        "\n",
        "class CnnModel(ImageClassificationModelBase):\n",
        "  def _build(self):\n",
        "    self.model = models.Sequential()\n",
        "    self.model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
        "    self.model.add(layers.MaxPooling2D((2, 2)))\n",
        "    self.model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "    self.model.add(layers.MaxPooling2D((2, 2)))\n",
        "    self.model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "    self.model.add(layers.Flatten())\n",
        "    self.model.add(layers.Dense(64, activation='relu'))\n",
        "    self.model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "class ResNetModel(ImageClassificationModelBase):\n",
        "  def _build(self):\n",
        "    # ResNet50モデルの読み込み（ImageNetで事前学習済み）\n",
        "    # ImageNet:\n",
        "    # 物体認識ソフトウェアの研究において、ソフトウェアの最適化や性能の評価等に\n",
        "    # 用いるために設計された、大規模な画像データセット\n",
        "    resnet_model = applications.ResNet50(weights='imagenet', include_top=False, input_shape=(224,224, 3))\n",
        "    resnet_model.trainable = True\n",
        "    inputs = layers.Input(shape=(32, 32, 3))\n",
        "\n",
        "    # ResNet50の出力層をCIFAR-10用に変更\n",
        "    # 事前学習済みのResNet50モデルに、CIFAR-10のクラス数に対応する全結合層を追加\n",
        "    x = ResizeImage()(inputs) # 入力画像の大きさをモデルに合わせる\n",
        "    x = resnet_model(x)\n",
        "    x = layers.Flatten()(x)\n",
        "    outputs = layers.Dense(10, activation=\"softmax\")(x)\n",
        "    self.model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "\n",
        "class Vgg16Model(ImageClassificationModelBase):\n",
        "  def _build(self):\n",
        "    vgg16_model = applications.VGG16(weights='imagenet', include_top=False, input_shape=(224,224, 3))\n",
        "    vgg16_model.trainable = True\n",
        "    inputs = layers.Input(shape=(32, 32, 3))\n",
        "\n",
        "    x = ResizeImage()(inputs) # 入力画像の大きさをモデルに合わせる\n",
        "    x = vgg16_model(x)\n",
        "    x = layers.Flatten()(x)\n",
        "    outputs = layers.Dense(10, activation=\"softmax\")(x)\n",
        "    self.model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "  def _compile(self):\n",
        "    self.model.compile(\n",
        "        optimizer=tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.09),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy'])\n",
        "\n",
        "\n",
        "class EfficientNetModel(ImageClassificationModelBase):\n",
        "  def _build(self):\n",
        "    efficientnet_model = applications.EfficientNetB5(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
        "    efficientnet_model.trainable = True\n",
        "    inputs = layers.Input(shape=(32, 32, 3))\n",
        "\n",
        "    x = ResizeImage()(inputs) # 入力画像の大きさをモデルに合わせる\n",
        "    x = efficientnet_model(x)\n",
        "    x = layers.GlobalAveragePooling2D()(x) # GlobalAveragePooling2Dを追加\n",
        "    x = layers.Dropout(0.5)(x) # 学習済みモデルの出力付近のレイヤー(トップレイヤー)は不要なので取り除く\n",
        "    outputs = layers.Dense(10, activation=\"softmax\")(x)\n",
        "    self.model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "\n",
        "  def _compile(self):\n",
        "    self.model.compile(\n",
        "        optimizer=tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.025),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# CNN\n",
        "cnn_model = CnnModel(x_train, y_train, x_test, y_test, '/content/cnn.keras')\n",
        "cnn_model.evaluate(x_test, y_test)\n",
        "\n",
        "# ResNet\n",
        "resnet_model = ResNetModel(x_train, y_train, x_test, y_test, '/content/resnet.keras')\n",
        "resnet_model.evaluate(x_test, y_test)\n",
        "\n",
        "# VGG16\n",
        "vgg16_model = Vgg16Model(x_train, y_train, x_test, y_test, '/content/vgg16.keras')\n",
        "vgg16_model.evaluate(x_test, y_test)\n",
        "\n",
        "Efficient Net\n",
        "effnet_model = EfficientNetModel(x_train, y_train, x_test, y_test, '/content/efficientnet.keras')\n",
        "effnet_model.evaluate(x_test, y_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGnPN0oe6ynR",
        "outputId": "d6558f81-a623-4790-ac01-5da41f600229"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 0us/step\n",
            "313/313 - 3s - 11ms/step - accuracy: 0.6333 - loss: 1.0415\n",
            "\n",
            "cnn: Test accuracy: 0.6333000063896179\n",
            "313/313 - 35s - 111ms/step - accuracy: 0.8153 - loss: 0.6274\n",
            "\n",
            "resnet: Test accuracy: 0.8152999877929688\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m58889256/58889256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m931s\u001b[0m 572ms/step - accuracy: 0.3783 - loss: 1.6938 - val_accuracy: 0.7768 - val_loss: 0.6344\n",
            "Epoch 2/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m872s\u001b[0m 558ms/step - accuracy: 0.7164 - loss: 0.8137 - val_accuracy: 0.8297 - val_loss: 0.4976\n",
            "Epoch 3/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m872s\u001b[0m 558ms/step - accuracy: 0.7717 - loss: 0.6533 - val_accuracy: 0.8420 - val_loss: 0.4618\n",
            "Epoch 4/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m872s\u001b[0m 558ms/step - accuracy: 0.8040 - loss: 0.5635 - val_accuracy: 0.8721 - val_loss: 0.3719\n",
            "Epoch 5/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m850s\u001b[0m 544ms/step - accuracy: 0.8228 - loss: 0.5157 - val_accuracy: 0.8842 - val_loss: 0.3345\n",
            "Epoch 6/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m872s\u001b[0m 558ms/step - accuracy: 0.8357 - loss: 0.4776 - val_accuracy: 0.8845 - val_loss: 0.3504\n",
            "Epoch 7/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m872s\u001b[0m 558ms/step - accuracy: 0.8478 - loss: 0.4380 - val_accuracy: 0.8938 - val_loss: 0.3091\n",
            "Epoch 8/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m872s\u001b[0m 558ms/step - accuracy: 0.8613 - loss: 0.4030 - val_accuracy: 0.8970 - val_loss: 0.3074\n",
            "Epoch 9/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m922s\u001b[0m 558ms/step - accuracy: 0.8628 - loss: 0.3937 - val_accuracy: 0.9118 - val_loss: 0.2635\n",
            "Epoch 10/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m871s\u001b[0m 557ms/step - accuracy: 0.8711 - loss: 0.3726 - val_accuracy: 0.9093 - val_loss: 0.2724\n",
            "313/313 - 61s - 195ms/step - accuracy: 0.9093 - loss: 0.2724\n",
            "\n",
            "vgg16: Test accuracy: 0.9093000292778015\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index = random.randint(0, len(x_test) - 1)\n",
        "\n",
        "plt.imshow(x_test[index])\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "def print_checking_answer(model, x, y):\n",
        "  predicted_label = np.argmax(model.predict(x))\n",
        "  actual_label = np.argmax(y)\n",
        "  print(f'\\n{model.name}: Prediction: {class_names[predicted_label]}, Actual: {class_names[actual_label]}')\n",
        "\n",
        "for model in [cnn_model, resnet_model, vgg16_model, effnet_model]:\n",
        "  print_checking_answer(model, x_test[index], y_test[index])\n"
      ],
      "metadata": {
        "id": "NnLvLQDA-6E6",
        "outputId": "6a39c221-a893-46d0-d4a9-a203543e0033",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
            "\n",
            "cnn: Prediction: airplane, Actual: airplane\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "\n",
            "resnet: Prediction: airplane, Actual: airplane\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\n",
            "vgg16: Prediction: airplane, Actual: airplane\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\n",
            "efficientnet: Prediction: ship, Actual: airplane\n"
          ]
        }
      ]
    }
  ]
}